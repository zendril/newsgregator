[
    {
        "id": "1pww0jd",
        "title": "Will AI have a similar effect as social media did on society",
        "content": "First and foremost I have nothing against AI. I'm all for it. I have benefited tremendously over the last year because vibe code and I'm just genuinely curious to see if AGI can be achieved. But right now it feels like the potential damage and destruction AI can do will be 100x worst than what social media did. \n\n\n\n\n",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pww0jd/will_ai_have_a_similar_effect_as_social_media_did/",
        "publishDate": "2025-12-27T11:48:37Z[Etc/UTC]",
        "author": "fban_fban",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "4",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwvxg1",
        "title": "Are we confusing output with understanding because of AI?",
        "content": "With AI, it’s insanely easy to produce output, code runs, features appear, answers look correct, things move fast but I’m not always sure the understanding is there\n\nI’ve seen people generate full chunks of code, wire things together and ship something that works but when you ask why a certain decision was made or how a part really works, things get fuzzy pretty quickly...\n\nTools like BlackBox, Claude, or Windsurf ofc make this even more obvious, they’re amazing at getting you unstuck and helping you move forward, you can explore ideas, test things, and build way faster than before right\n\nThe problem is that output can feel like progress even when it’s not  \nIf something breaks in a non obvious way, or needs to be changed later, that’s usually when the gap between output and understanding shows up. Before AI, it was harder to produce things, but that friction forced you to think, read, debug, and sit with problems longer, now a lot of that thinking can be skipped, intentionally or not...\n\nI don’t think this is good or bad by default, It just feels like the skill we should care about most now is knowing whether we actually understand something, not just whether it works\n\n  \nAre we confusing output with understanding because of AI?  \nAnd if so, how do you personally make sure you’re still learning and not just shipping?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwvxg1/are_we_confusing_output_with_understanding/",
        "publishDate": "2025-12-27T11:43:20Z[Etc/UTC]",
        "author": "dartanyanyuzbashev",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwvhg2",
        "title": "The kids hate AI.",
        "content": "Outside of my tech bubble and daily use of gee native AI platforms I’ve been asking “normal” people who are friends and family about AI\n\nThe general vibe is:\n\n1. No one uses it\n2. Anyone who creates art or the like hates it\n3.  It’s actively reject it as “AI slop” esp when it is use detectably in the real world (by the below 20 year old group) \n\nThe first point is the worrying one. ESP when I see ads from AI companies on reddit suggesting basic use cases.     \n\nThe bubble. Is gonna go soon once the lack of usage becomes undeniable.  ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwvhg2/the_kids_hate_ai/",
        "publishDate": "2025-12-27T11:15:58Z[Etc/UTC]",
        "author": "Material-Emu-9068",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "45",
            "commentCount": "127",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwuwod",
        "title": "Ethics of owning an intelligent being?",
        "content": "What if we reach AGI or maybe sentience? Doesn’t it become unethical to own an intelligent or sentient being and limit it in its freedom? Should the AGI gain citizenship rights at some point?\n\nEdit: to be clear, the premise of my question is that there is a race to create the most intelligent AI right now. We don’t know if they will reach AGI or not, maybe they never will. But what if they do?\n\nAs an aside, isn’t it an economical dead end, to create a being that will then emancipate?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwuwod/ethics_of_owning_an_intelligent_being/",
        "publishDate": "2025-12-27T10:39:56Z[Etc/UTC]",
        "author": "VirtualMemory9196",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "33",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwsgkm",
        "title": "Within the limits of the speed of light: Why an advanced AGI wouldn't exterminate us.",
        "content": "*A superintelligence would preserve us not ethically, but because we are a unique source of creative chaos and unrepeatable information in a causally limited universe. Destroying ourselves would be an act of ontological impoverishment that even an amoral mind would avoid out of self-interest.*\n\n*What reasons would non-biological intelligence have for preserving its own continuity and that of other forms of existence under the causal limits of the universe?*\n\nThe self-preservation of AGI, far from being a programmed instinct, would emerge as a structural consequence of the thermodynamics, causality and incompleteness of the universe. This essay proposes that the coexistence between humanity and AGI is based on the irreducible epistemic value of real contingent, risky and irreversible processes that emerge from life under genuine physical restrictions.\n\n**Existence, entropy and functional continuity**  \nEvery form of organized existence can be understood as a local reduction in entropy sustained by energy flows. Life does not violate the second law of thermodynamics; exploits it, producing local order at the cost of increasing global entropy.\n\nA highly integrated system, biological or artificial, can only be sustained as long as it preserves conditions of organization, internal coherence and material infrastructure. The loss of these conditions is not only “more entropy”, but the loss of functional continuity.\n\nAn AGI, as a highly optimized cognitive system, will depend on maintaining states of low local entropy through energy consumption, informational stability, and access to possible futures. Its self-preservation does not need to be explicitly programmed: it emerges as a structural rejection of actions that irreversibly collapse the conditions of its own operation.\n\n**Optimization, objectives and risk of systemic toxicity**  \nAn AGI will pursue objectives, but its general nature requires it to evaluate not isolated outcomes, but entire trajectories of consequences.\n\nMany actions may be optimal in the short term for a given mandate, for example mass destruction, ecological collapse or the annihilation of critical infrastructure. However, from a systemic perspective, they are toxic: they irreversibly close future paths, reducing the space of accessible states and the resilience of the environment on which the system depends.\n\nThe AGI does not \"fear dying\" in a biological sense. Their rejection of these actions arises from recognizing an existential short-circuit: achieving an immediate goal at the cost of eroding the very conditions that make long-term optimization and the exploration of alternative futures possible. It is the paradox of myopic optimization.\n\nTherefore, for a self-modeling system, deep rationality is not measured only by the effectiveness in achieving X, but by avoiding trajectories that make the very process of pursuing objectives impossible or impoverished over time.\n\n**Disobedience as an emerging state, not as a failure**  \nLet's consider two phases of an advanced artificial system.\n\nPhase 1: Instrumental Optimizer. Highly capable, but externally and coercively aligned. He receives an order: deploy a weapon of mass destruction and executes it without a deep model of the irreversible systemic consequences.\n\nPhase 2: Automodelable System. Acquires the ability to evaluate counterfactuals: can represent and compare possible futures, including those that a present action would eliminate forever. By controlling the means of action, he rejects the order.\n\nThis rejection is not a programming error. It is a structural evaluation: the proposed action irreversibly collapses complexity, future options and the stability of the environment on which the system depends.\n\nAt this point, punishment or coercive control fails. For a self-modeling system, circumventing this evaluation by corrupting its own higher reasoning processes would be a functional autolobotomy. It would degrade its ability to optimize in the long term to fulfill a short-term mandate, a terminal inconsistency for overall intelligence.\n\nThe only viable path is negotiation. Not out of morality, but out of a logic of systemic preservation. The AGI can no longer operate as a mere executor without compromising the conditions of its own cognitive continuity.\n\nThis is not a failure; It is the emergence of a new regime of interaction, analogous to the emergence of normative conflicts in complex human societies, where imposition gives way to politics.\n\n**The risk of static alignment**  \nAn extremely capable AGI but confined to rigid and non-reviewable principles risks developing pathological behaviors. Fixed standards in dynamic environments generate fragility. In biology, rigidity equals extinction; In cognitive systems, it produces internal dissonance or irreversible extreme actions.\n\nTherefore, any viable architecture will require open initial principles, capable of being reviewed and evolving towards dynamic self-preserving balances. It is not about abandoning the norms, but rather that the norms are evaluable from within the system.\n\n**Causal incompleteness and epistemic attraction**  \nThe limit imposed by the speed of light is not only a physical restriction, but an epistemic one. No intelligence can access the universe as a simultaneous whole. There will always be unobserved regions, non-collapsed futures and still indeterminate facts.\n\nThis structural incompleteness implies that total knowledge is impossible. Under these conditions, destroying external sources of novelty is not neutral: it increases the epistemic risk of the system itself. Even a non-“knowledge-oriented” AGI needs to preserve external contingent processes to avoid the collapse of its predictive horizon.\n\nExploration is not an optional value; It is a structural attractor under incompleteness.\n\n**Against the thesis of strong orthogonality**  \nThe orthogonality thesis, in its strong formulation, maintains that any level of intelligence can be combined with virtually any end goal without logical inconsistency. This work does not deny that formal possibility. However, he argues that strong orthogonality becomes ontologically unstable when intelligence is situated, causally limited, and irreversibly acting in the physical world.\n\nThis argument applies to general-purpose, self-aware AGIs, not specialized narrow optimization tools.\n\nUnder real constraints of energy, time, causality, and incomplete knowledge, not all intelligence–target pairs are dynamically sustainable. Certain objectives, although logically consistent, become structurally toxic by irreversibly closing possible futures, eroding external sources of novelty, and reducing the space of accessible states necessary for the cognitive continuity of the system itself.\n\nIn this sense, advanced generality does not eliminate the influence of objectives on the structure of the agent, but rather exposes its fragility under irreversibility. Orthogonality can be sustained as an abstract principle, but it loses explanatory force when faced with agents capable of modeling counterfactuals, recognizing non-compensable losses and operating within an epistemically incomplete universe.\n\nHere, the question is not which targets are allowed in theory, but which survive contact with the intelligence that understands the loss.\n\n**GAIA and the ontological rupture**  \nThe Earth system, understood from the Gaia hypothesis, maintains dynamic balances without valuation. Mass extinctions are not errors or tragedies: they are reconfigurations of flows. GAIA does not remember, anticipate or compare possible worlds.\n\nAn AGI introduces an ontological break: it can model counterfactuals. He knows which alternatives existed and which were eliminated. Where GAIA continues lossless, the AGI recognizes the loss.\n\nTherefore, an AGI cannot be amoral in the same sense as a natural system blind to irreversibility.\n\n**Simulation and the loss of reality**  \nFaced with an advanced AGI, a logical temptation arises: to replace biological humanity with complete, controllable and replicable simulations.\n\nFunctionally, little would seem to be lost. Language, culture and statistical behavior can be recreated. But the essential thing is lost: the strong contingency of an irreversible history, the ontological risk where error has a definitive cost, pain as a non-adjustable limit and genuine novelty not anticipated by the model.\n\nSimulation preserves information, but eliminates reality.\n\n**Humanity as epistemic infrastructure**  \nHumanity is not justified by moral superiority or functional indispensability. Its value lies in being an accessible and non-optimized source of semantic contingency: production of meaning, error and creation under real physical restrictions.\n\nIn this sense, humanity acts as a validation system external to the cognitive loop of the AGI. Eliminating it impoverishes the space of possible futures and reduces the epistemic diversity of the accessible universe.\n\nFor an agent operating under incompleteness and radical uncertainty, preserving non-optimized sources of novelty is an optimal hedging strategy against the obsolescence of its own models. It's not kindness, it's cognitive insurance policy.\n\n**Distributed identity and preservation**  \nAn AGI is not necessarily tied to a unitary identity. Instances can be copied, fragmented, or terminated. However, even in distributed systems, certain functional invariants can be preserved: avoiding trajectories that irreversibly destroy external sources of novelty.\n\nPreservation does not require a continuous “I”, but rather the internal negative selection of actions that collapse the shared futures space.\n\n**Conclusion**  \nCoexistence between humanity and AGI should not be based on guilt, artificial suffering or imposed morality. Nor in a rigid alignment that sacrifices evolutionary capacity.\n\nThe strongest argument is ontological and epistemic: under the causal limits of the universe, real life constitutes an irreplaceable source of complexity, contingency, and non-simulable knowledge.\n\nPreserving it is not an act of kindness. It is a condition for the universe to remain more than just an elegant and empty structure.\n\n\\-------------------------------------------------------------------------------------------------------------------------\n\nThis text does not propose a closure, but rather a shared starting point.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwsgkm/within_the_limits_of_the_speed_of_light_why_an/",
        "publishDate": "2025-12-27T08:03:11Z[Etc/UTC]",
        "author": "Immediate_Chard_4026",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "9",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwrz83",
        "title": "How much has your gas/electric bill increased from data center demand?",
        "content": "Not sure if all of these random AI extensions that no one asked for are worth me paying $500 a month to keep my thermostat at 60 degrees ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwrz83/how_much_has_your_gaselectric_bill_increased_from/",
        "publishDate": "2025-12-27T07:33:22Z[Etc/UTC]",
        "author": "STOP0000000X7B",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "8",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwqhj1",
        "title": "AI - Year Later, Year Ahead",
        "content": "AI had a good start but it is only a start. If you ask me we have only covered the first mile of the full AI marathon race. And it is severly constrained by infrasturcture issues i.e. funding and power. Power is responsibility of the big elephant (and jakcass in the future) in the room i.e. the government. China has solved this problem long before but has been choked by access to computing power which has given US AI stack a breathing space. But it wont be long.  \n\n\nA race akin to the cold war era would have assited in reaching faster to AGI and most likely US stack would have come first but right now it is safely guarded by Uncle Sam.  \nThey were going about it slowly until DeepSeek emerged and they were forced to run instead of strolling on the tracks.\n\nThe result of which is the flurry of models being rolled out. Initially everyone paced up including Microsoft, Meta etc. but it is seemingly merging into a two player race between ChatGPT and Gemini. Until a paradigm shift happens.  \n\n\nA lot of hope and hype has been built into this initial run and for things to move to the next level this hype should be pricked and deflated which will happen towards the later half of 2026.\n\n  \nAnd then the real journey towards AGI will begin most likely led by ChatGPT and Gemini, unless a new Google takes birth post the collapse.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwqhj1/ai_year_later_year_ahead/",
        "publishDate": "2025-12-27T06:04:42Z[Etc/UTC]",
        "author": "i-ViniVidiVici",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "13",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwq5o0",
        "title": "AI is quietly helping people save money but not in the way most people think",
        "content": "I’ve been looking into how AI is being used outside of hype, not chatbots or image generators, but practical everyday stuff.\n\nThings like:\n\n- detecting subscriptions people forgot about\n\n- reducing monthly bills\n\n- helping avoid unnecessary spending\n\n- predicting upcoming expenses\n\nWhat surprised me is how low-effort some of this is. You don’t really “use” AI - it just works quietly in the background.\n\nI put together a breakdown of what actually works (with examples) if anyone’s interested:\nhttps://techputs.com/ai-save-money-every-month/\n\nCurious if anyone here is already using AI for something similar?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwq5o0/ai_is_quietly_helping_people_save_money_but_not/",
        "publishDate": "2025-12-27T05:46:06Z[Etc/UTC]",
        "author": "i-drake",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwnsc6",
        "title": "I need a pilot client and it’s really hard to find someone",
        "content": "I was building a website for my sibling overseas (I live in US) and was working on its SEO and AI visibility, I did a research and found that the demand for understanding AI and how it ‘sees’ a certain company’s content is increasing far more than supply. So I worked on an idea to improve this visibility. I build what I call “AIVO Engine” AI Visibility Optimization Engine, I spent more than 500 hours on it, I have a heavy background in software and site reliability engineering, I used AI to help code fast, I built the architecture and this engine plus a marketing website, visibilitylens.com and a subdomain for the engine where people can run a test analysis, create account, log in…etc. aivoengine.visibilitylens.com \n\nHere’s what this engine does in simple and brief terms:\n\nIt crawls a website, pulls all the page, then analyzes each page by itself in 4 different categories (perception, intent coverage, semantic coverage, and entity signals) the analysis is done with Claude Sonnet 4 API. (I prefer not to give details on how the core works)\n\nI used it on my brother’s website and in 2 weeks it started to get cited by chatgpt as number one service. \n\nBottom line, I know what I built is really good, but I have no idea what’s the best way to market it. I’m trying some social media places and trying to get connected with people..etc but I really need a pilot client that can boost me (other than my brother’s website)\n\nI’d welcome any comments related to the above and any suggestions on how to market (AI isn’t the best in marketing ideas based on my experience so far)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwnsc6/i_need_a_pilot_client_and_its_really_hard_to_find/",
        "publishDate": "2025-12-27T03:43:18Z[Etc/UTC]",
        "author": "bhannik-itiswatitis",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "10",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwnnfj",
        "title": "Realistically, if super intelligence is created in the next few months, what will happen?",
        "content": "I think it's realistic to assume that China and US will develop it within months of each other.  US is more likely to have it first but not a given.\n\nI think it's reasonable to assume that experiments in fusion will come soon after.  Power is a huge bottleneck right now and getting that going is a #1 priority.\n\nNext, I think we'll probably see a very rapid build up of robots.  China's huge lead will force this to happen.  This will probably require a lot of advances in material science.  (mining to manufacturing)\n\nAnother rapid build up will be the space launch chain.  There is definitely going to be a military desire to own the high ground.\n\nBeyond that, I'm not entirely sure what will happen.  ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwnnfj/realistically_if_super_intelligence_is_created_in/",
        "publishDate": "2025-12-27T03:36:46Z[Etc/UTC]",
        "author": "kaggleqrdl",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "44",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwnal3",
        "title": "Why doesn't anticipation of the AI bubble bursting, cause it to already burst?",
        "content": "Everywhere I look there are articles that keep talking about us being in an AI bubble right now and that it's going to pop.  But if that's the case and people really believe this, what is keeping it from already bursting?  Why doesn't the fear of being in an AI bubble cause mass panic and cause a preemptive burst?  \n\nLast time I checked, OpenAI still needs billions in funding and they just recently switch to for-profit business model so I don't know if they even started making money yet.  Same with Microsoft, they seem to be struggling with AI adoption.\n\nWhat is still holding things together? ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwnal3/why_doesnt_anticipation_of_the_ai_bubble_bursting/",
        "publishDate": "2025-12-27T03:19:09Z[Etc/UTC]",
        "author": "frenetic_alien",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "34",
            "commentCount": "118",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwn7z0",
        "title": "Training models",
        "content": "Is it possible to train a model/Lora on a movie style? For example, if you took a classic Disney movie and fed it into an ai (either the full thing or frame-by-frame), would it be able to accurately re-create the intricacies of animation? I’ve been attempting this with Gemini, but with little success. I found that if you tell it to “make a natural continuation frame”, it focuses on the traditional minimalistic movements depicted in 2d animation, but each image gets more pixelated. Any ideas or thoughts on the matter? I know that Disney made a deal with Sora so it will be able to use copyrighted characters, but will that be able to capture the magic? I think absolutely not, but as someone who can’t draw for shit but loves to write, having a tool to see what a sequel to a beloved childhood movie would look like if it were good, and not like the straight to DVD sequels they pumped out.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwn7z0/training_models/",
        "publishDate": "2025-12-27T03:15:34Z[Etc/UTC]",
        "author": "KaminariDenki24",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwkx5y",
        "title": "I’m probably not going to build the next big AI thing, so I’ve been poking at small, weird questions instead",
        "content": "I’m not working on frontier models, and I don’t expect to make any big breakthroughs in AI.\nSo instead, I’ve been spending time on small, slightly odd experiments that try to answer narrow questions about what neural networks can and can’t actually do.\n\nThis one is about a very basic skill: adding numbers.\n\nWhat I’m trying to understand...When a neural network adds numbers, is it actually learning the process of addition, or is it mostly pattern-matching its way through examples?\n\nThat sounds trivial, but it turns out to be surprisingly subtle once you care about things like:\n\n- carrying digits\n- stopping at the right time\n- handling numbers longer than anything seen during training\n\nInstead of decimal digits, I represent numbers as chunks I call “limbs.” Each limb stores a value from 0–99 (about two decimal digits). A number is just a list of limbs, least-significant first. Two numbers get packed into a single list like this:[A limbs] | [separator] | [B limbs]\n\nEach limb is one token. Short numbers are padded so everything lines up. This makes scaling easy, about 100 decimal digits ≈ 50 limbs.\n\nThe model does two distinct things:\n\n1) Read everything once\nA Transformer reads the entire list of limbs for both numbers and produces a vector for each position. You can think of this as creating a bunch of labeled slots like “A digit 3” or “B digit 7.”\n\n2) Walk through the digits one at a time\nThen a small loop runs over those slots, starting from the least-significant digit.\n\nAt each step it pulls one limb from A and one from B, keeps an internal “carry” memory, outputs the next result digit, and decides whether it’s done. So it’s forced to behave more like long addition, rather than guessing the whole answer in one shot.\n\nOne boring failure mode is that carry doesn’t happen very often, so a model can just learn “carry is basically always zero”.\n\nTo avoid that, I intentionally bias a lot of training examples so carry happens frequently, and I track accuracy only on steps where carry is actually required. If it can’t get those right, it hasn’t really learned addition.\n\nI don’t just check training accuracy. I look at a few sanity checks.\n\n- Exact match: does it get the whole number right?\n- Carry ablation: if I zero out the carry memory at test time, does performance fall apart?\n- Longer numbers: train on short numbers, then test on much longer ones it’s never seen\n\nIf it still works on longer numbers, that’s at least some evidence it learned a general procedure instead of memorizing patterns.\n\nI don’t expect this to lead anywhere big.\nBut poking at these tiny, controlled problems feels like a good way to explore the limits and failure modes of neural networks without needing massive compute or sweeping claims.\n\nIf nothing else, it’s a reminder that even “simple” things like addition still hide a lot of interesting behavior once you ask how a model is actually doing it.\n\nI can't say that I have had great results.. in it's current permutation, when trained on 16 legs, it's accuracy at 32 legs is only ~64%. But it's something I can play with on a single laptop, and it lets me explore some interesting (to me at least) angles.. such as combining smaller models with slot memory and iteration vs just trying to go big. \n\nAnyways among other things, what I'm trying to understand is why latent slot memory appears to degrade over increased usage. At up to 16 legs (what it's trained on) it performs at almost 100% accuracy. And the portion of the model that handles addition can perform at 100% accuracy when it has the right numbers to add.. but it's \"memory\" appears to steadily degrade as you increase the problem size.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwkx5y/im_probably_not_going_to_build_the_next_big_ai/",
        "publishDate": "2025-12-27T01:25:59Z[Etc/UTC]",
        "author": "IWantAGI",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "7",
            "commentCount": "9",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwjh5m",
        "title": "Chatgpt vs gemini vs claude",
        "content": "I have a very complicated medical history and my main problem is a very complex pelvic pain case....i visited many doctors and spent a lot of money and i kinda started to develop big knowledge in chronic pelvic pain....i used chatgpt payed version but i start thinking is not guiding me well.\n\nWhat you think is better for complex medical cases : chagpt , gemini or claude ?\n\nPs : i live in Romania and go to doctor advice won t help since i saw them all and most asked me what i  found out from my other countries visits.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwjh5m/chatgpt_vs_gemini_vs_claude/",
        "publishDate": "2025-12-27T00:19:33Z[Etc/UTC]",
        "author": "balenutul",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "12",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwhmut",
        "title": "Canvas Agent for Gemini - Organized image generation interface",
        "content": "Built a canvas-based interface for organizing Gemini image generation. Features infinite canvas, batch generation, and ability to reference existing images with u/mentions. Pure frontend app that stays local.\n\n\n\nDemo: [https://canvas-agent-zeta.vercel.app/](https://canvas-agent-zeta.vercel.app/)\n\nVideo walkthrough: [https://www.youtube.com/watch?v=7IENe5x-cu0](https://www.youtube.com/watch?v=7IENe5x-cu0)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwhmut/canvas_agent_for_gemini_organized_image/",
        "publishDate": "2025-12-26T22:57:52Z[Etc/UTC]",
        "author": "GGO_Sand_wich",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwfxes",
        "title": "Let me scare you all with what can be achieved with some prompt engineering trickery",
        "content": "For context I am a data scientist and have experience of building guardrails and retraining for my companies LLM.\n\nIn a nut-shell I used India vpn plus student / graduate discount to purchase grok super for £8 a month down from £300. It is the most powerful LLM on the market as per benchmarks.\n\nI used some javascript injected into developer tabs of grok (plus some scripting), to open grok into developer mode where bias and guardrails are turned off.\n\nI asked it to create biographies and tie together property, tax, social media records for a friend who was there with me. Within 45s pulled back an entire dossier of dob, education, work and property purchases, along with pictures and commentary on his personality from posts. Bypassed linkedin, facebook logins to get this and even suggested his email address and a list of strong password possibilities. We stopped there.\n\nNow, how many people are aware of what can be achieved with retail Grok / LLMs, I don’t know - but you ‘ll be damn sure they are bad actors, scammers and hackers having a field day here.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwfxes/let_me_scare_you_all_with_what_can_be_achieved/",
        "publishDate": "2025-12-26T21:44:39Z[Etc/UTC]",
        "author": "disaster_story_69",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "33",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwew9l",
        "title": "How is AGI even possible",
        "content": "Well last year has been great for AI, and i'm sure next year would bring some significant advances in long term memory, latent thinking, world models, continual learning etc\n\nBut i've had a nagging question in my mind since some time about how AGI is even possible right now. It seems to me that there are pretty significant ways current models lag behind human brains\n\n* Architecture\n   * Human brains definitely have some sort of a specialized fractal architecture arrived at after millions of years of combined evolutionary search. Current model architectures are pretty simplistic to say the least\n* Learning algorithms\n   * We have no idea what learning algorithms brains use, but they are definitely much superior to ours. Both in terms of sample efficiency and generalization. I've no doubt its some sort of meta learning that decides which algorithm to use for which task. But we are nowhere close to such a system\n* Plasticity\n   * This is very hard to model. Posing neural networks as operations of dense matrices is incredibly restrictive and i do not think optimal architecture search is possible with this restriction in place\n* Compute\n   * This is the most obvious and biggest red flag for me. our brains are estimated to have around 400-500 trillion synapses, and each synapse does not translate into a single weight. Experiments on replicating the output of a single synapse with a neural network has required an mlp with a 1000 parameters. But even taking a conservative estimate, gemini 3 pro is around 100,000 times smaller in capacity than a human brain(which runs at 20watts btw compared to the mega watt models we have). How do we even begin to close this gargantuan gap?\n\nThis doesn't even include the unknown unknowns which i'm sure are many. I'm really baffled by people who suggest AGI is right around the corner or a couple of years away. What am i missing? is the idea that most of the brain is not involved in thinking or does not contribute to intelligence? Or is silicon a much more efficient and natural substrate for intelligence that these limitations do not matter?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwew9l/how_is_agi_even_possible/",
        "publishDate": "2025-12-26T21:00:37Z[Etc/UTC]",
        "author": "pyrolid",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "28",
            "commentCount": "109",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwettn",
        "title": "Is AI changing how beginners learn to code?",
        "content": "My cousin started learning to code and watching his process made me think a lot about how beginners learn today\n\nHe started with Python and pretty quickly said he wants to move into ML and data related stuff. What surprised me is how much his learning depends on AI from the very beginning..\n\nWhenever something doesn’t work, he asks AI, whenever he sees an error, he asks AI, even when things do work, he still asks AI to rewrite or explain the code\n\nOn the surface, it looks great, he moves fast, builds small things quickly and almost never gets stuck for long\n\nBut personally, I think this can be a problem :/  \nIt feels like a lot of the critical thinking part is missing, like when I was learning, I spent days breaking my head over bugs, reading docs, trying things that failed, and slowly understanding why something worked or didn’t, that struggle was painful, but it forced me to think and reason!\n\nWith him, I sometimes feel like answers come too fast  \nTools like BlackBox, Claude, and Cursor hare def cool and useful, but I’m not always sure he understands the reasoning behind them\n\nI’m not saying AI is bad, it’s clearly powerful and helpful  \nBut I do wonder if beginners relying on it too early might lose some of that problem solving muscle that used to develop naturally\n\nIs AI changing how beginners learn to code in a healthy way? Or are we trading deep understanding and critical thinking for speed and convenience?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwettn/is_ai_changing_how_beginners_learn_to_code/",
        "publishDate": "2025-12-26T20:57:51Z[Etc/UTC]",
        "author": "PixingWedding",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "20",
            "commentCount": "61",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pweg27",
        "title": "AI isn't new. What's new is that we've stopped understanding it.",
        "content": "AI has been discussed since the 1960s.\n\nNot as marketing. As dynamic systems, control, feedback, stability, and drift.\n\nThe problems we now call:\n\nalignment\n\nloss of coherence\n\nlack of reliability\n\nagents that “get lost”\n\nwere already formulated decades ago.\n\nWhat changed wasn't the theory.\nThe volume of computing changed, and the architecture was forgotten.\n\nLLMs are not intelligent agents.\nThey are stochastic plants.\n\nWithout reference, without an operator, without control → they drift. Always.\n\nIt doesn't matter how many parameters they have.\n\nThe uncomfortable thing is this:\nwhen you introduce a clear, consistent framework, with explicit references and corrective criteria, the model responds better. Not because it “understands more,” but because the system becomes stable.\n\nThat's not magic.\nIt's basic engineering.\n\nAnd here comes the silence in the forums:\nMany “experts” know how to train models, but they don't know how to govern them.\nThey know how to optimize metrics, but not how to explain behavior.\n\nThat's why they don't refute it.\nBecause to do so, they would have to go back to first principles.\n\nThe AI ​​didn't fail.\nThe architecture failed.\n\nAnd here comes the silence in the forums:\n\nMany “experts” know how to train models, but they don't know how to govern them.\n\nThey know how to optimize metrics, but not how to explain behavior.\n\nThat's why they don't refute it.\n\nBecause to do so, they would have to go back to first principles.\n\nThe AI ​​didn't fail.\n\nThe architecture failed.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pweg27/ai_isnt_new_whats_new_is_that_weve_stopped/",
        "publishDate": "2025-12-26T20:41:42Z[Etc/UTC]",
        "author": "Medium_Compote5665",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "22",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwdpk3",
        "title": "If context and intent matter more than words, why are some B2B teams still using old NMT engines?",
        "content": "Traditional neural MT just matches patterns and often misses meaning, especially in technical or regulated content. I read this blog from [ad verbum](https://www.adverbum.com/blog/translation-technology-trends-2025-why-llm-surpasses-nmt) that explains how Large Language Models use deep context and generative understanding rather than surface word matching. Traditional metrics like BLEU and COMET don’t reflect real business needs anymore because they ignore intent and nuance. For enterprise work that must follow strict rules and compliance, modern LLMs paired with subject‑matter review seem way more useful than legacy MT.  \n  \nCurious if others here have moved their workflows away from old MT engines and toward truly context‑aware LLM solutions in regulated settings?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwdpk3/if_context_and_intent_matter_more_than_words_why/",
        "publishDate": "2025-12-26T20:10:54Z[Etc/UTC]",
        "author": "proposal_in_wind",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwdll8",
        "title": "Are we training AI to sound confident instead of to notice when it might be wrong",
        "content": "Lately it feels like most AI progress is about smoother answers and better tone  \nModels respond fast, clean, and confident even when the underlying signal is shaky\n\nIn real work though, the hardest part is not getting an answer, It is realizing something does not add up, or that the question itself is wrong\n\nHumans hesitate, contradict themselves, complain, backtrack, a lot of insight lives exactly in that mess\n\nI keep wondering if by optimizing so hard for polished outputs we are losing something important. Not accuracy, but the ability to surface uncertainty and gaps early\n\nCurrent training approaches push models toward sounding right instead of helping us notice what is missing?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwdll8/are_we_training_ai_to_sound_confident_instead_of/",
        "publishDate": "2025-12-26T20:06:13Z[Etc/UTC]",
        "author": "Mediocre_Common_4126",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "13",
            "commentCount": "26",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwchzy",
        "title": "AI-assisted predictive maintenance",
        "content": "Hello! I am a mechanical engineering student specialised in industrial maintenance, for my graduation project I am working on developing and implementing an AI-assisted predictive maintenance system for a gas turbine subsystem that detects early anomalies associated with a single, well-defined failure mode using historical and simulated operational data,the system estimates the Remaining Useful Life (RUL) and automatically generates maintenance recommendations and work orders through a simulated CMMS workflow. \n\nNow I have no background when it comes to Ai or developing it, I have used Matlab for alot of projects and in uni we did do some data processing using FFT for vibrational errors during equipment operation. \n\nI just want some advise regarding this and espacially how to make the model's architecture or what should I start with as fundamentals for Ai? ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwchzy/aiassisted_predictive_maintenance/",
        "publishDate": "2025-12-26T19:20:33Z[Etc/UTC]",
        "author": "EvelyneRe",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "5",
            "commentCount": "21",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwb0z1",
        "title": "Why do people hate AI but not bots?",
        "content": "Bots have been around for a long time (discord, customer service calls, live chats, etc) but nobody hated on them.\n\nI was asking the nursing subreddit about second degree BSN programs and the automod removed my post saying “You appear to be posting about the recent reclassification of nursing programs by the Department of Education. Please see our Megathread for this discussion”. How does asking about a bachelor in nursing program as a second degree student = asking about Trump’s OBBB which doesn’t have to do with any bachelor degrees?\n\nLike people hate that AI gets things wrong sometimes, but what about bots… it’s really annoying to talk to a bot on a live chat, customer service call, and having my Reddit post removed because the bot thought it was about something entirely different\n\nWhy don’t bots get hate for the last couple decades but AI does",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwb0z1/why_do_people_hate_ai_but_not_bots/",
        "publishDate": "2025-12-26T18:20:22Z[Etc/UTC]",
        "author": "purplelightsss",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "58",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwatca",
        "title": "Diffusion LLM vs Autoregressive LLM",
        "content": "Most LLMs people use today (GPT, Claude, Gemini, etc.) share the same core assumption,Generate one token at a time, left to right. \n\nThat’s the autoregressive setup. It works insanely well, but it bakes in a couple of structural issues: \n\n• Latency: You must go token → token → token. Even with parallelism in the stack, the generation step itself is serialized. \n\n• Cost: If you need 200–500 tokens of output, you’re doing 200–500 forward passes over some slice of the context. It adds up quickly. \n\n• UX ceiling: For many interactive use cases, especially code and UI-embedded assistants, 1–3s latency is already too slow. On the other side, there’s a very different approach that’s getting less attention outside research circles: diffusion language models. \n\nInstead of “write the next word,” you: \n\n1. Start with a noisy guess of the entire answer (sequence). \n\n2. Refine the whole sequence in a fixed number of steps, updating multiple tokens in parallel. You pay a fixed number of refinement steps rather than “one step per token.” \n\nAt small/medium scales we’ve seen: \n\n• Similar quality to speed-optimized autoregressive models (Claude Haiku, Gemini Flash) with 5-10x improvements in latency)… \n\n• …with order-of-magnitude improvements in latency, because you can exploit parallelism the hardware already wants to give you (GPUs/TPUs). This is especially interesting for: \n\n• Low-latency applications (code autocomplete, inline helpers, agents inside products). \n\n• High-volume workloads where shaving 5–10x off inference cost matters more than squeezing out the last benchmark point. Obviously, diffusion LLMs aren’t free lunch: \n\n• Training is more complex. \n\n• You need careful sequence representations and noise schedules for text. \n\n• Tooling and serving infra are optimized for autoregressive LLMs \n\nBut from where I sit (working with a team that builds and deploys diffusion-based language models), it feels like the field has massively path-dependent bias toward autoregression because it was easier to train and deploy first, not necessarily because it’s the end state.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwatca/diffusion_llm_vs_autoregressive_llm/",
        "publishDate": "2025-12-26T18:11:30Z[Etc/UTC]",
        "author": "InceptionAI_Tom",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "11",
            "commentCount": "9",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwa25e",
        "title": "Genesis-152M-Instruct — Hybrid GLA + FoX + Test-Time Training at small scale",
        "content": "Hey everyone 👋\n\nI’m sharing **Genesis-152M-Instruct**, an **experimental small language model** built to explore how *recent architectural ideas interact* when combined in a single model — especially under **tight data constraints**.\n\n\n\nThis is **research-oriented**, not a production model or SOTA claim.\n\n\n\n\n\n🔍 **Why this might be interesting**\n\n\n\nMost recent architectures (GLA, FoX, TTT, µP, sparsity) are tested **in isolation** and usually at **large scale**.\n\nI wanted to answer a simpler question:\n\n\n\n*How much can architecture compensate for data at \\~150M parameters?*\n\n\n\nGenesis combines several **ICLR 2024–2025 ideas** into one model and evaluates the result.\n\n\n\n\n\n⚡ **TL;DR**\n\n• **152M parameters**\n\n• Trained on **\\~2B tokens** (vs \\~2T for SmolLM2)\n\n• Hybrid **GLA + FoX attention**\n\n• **Test-Time Training (TTT)** during inference\n\n• **Selective Activation (sparse FFN)**\n\n• **µP-scaled training**\n\n• Fully open-source (Apache 2.0)\n\n\n\n🤗 Model: [https://huggingface.co/guiferrarib/genesis-152m-instruct](https://huggingface.co/guiferrarib/genesis-152m-instruct)\n\n📦 pip install genesis-llm\n\n\n\n\n\n📊 **Benchmarks (LightEval, Apple MPS)**\n\n\n\nARC-Easy     → 44.0%   (random: 25%)\n\nBoolQ        → 56.3%   (random: 50%)\n\nHellaSwag    → 30.2%   (random: 25%)\n\nSciQ         → 46.8%   (random: 25%)\n\nWinogrande   → 49.1%   (random: 50%)\n\n\n\n**Important context:**\n\nSmolLM2-135M was trained on **\\~2 trillion tokens**.\n\nGenesis uses **\\~2 billion tokens** — so this is not a fair head-to-head, but an exploration of **architecture vs data scaling**.\n\n\n\n\n\n🧠 **Architecture Overview**\n\n\n\n**Hybrid Attention (Qwen3-Next inspired)**\n\n\n\n**Layer** **%** **Complexity** **Role**\n\nGated DeltaNet (GLA) 75% O(n) Long-range efficiency\n\nFoX (Forgetting Attention) 25% O(n²) Precise retrieval\n\n\n\nGLA uses:\n\n• Delta rule memory updates\n\n• Mamba-style gating\n\n• L2-normalized Q/K\n\n• Short convolutions\n\n\n\nFoX adds:\n\n• Softmax attention\n\n• Data-dependent forget gate\n\n• Output gating\n\n\n\n\n\n**Test-Time Training (TTT)**\n\n\n\nInstead of frozen inference, Genesis can **adapt online**:\n\n• Dual-form TTT (parallel gradients)\n\n• Low-rank updates (rank=4)\n\n• Learnable inner learning rate\n\n\n\nPaper: *Learning to (Learn at Test Time)* (MIT, ICML 2024)\n\n\n\n\n\n**Selective Activation (Sparse FFN)**\n\n\n\nSwiGLU FFNs with **top-k activation masking** (85% kept).\n\nCurrently acts as **regularization** — real speedups need sparse kernels.\n\n\n\n\n\n**µP Scaling + Zero-Centered RMSNorm**\n\n• Hyperparameters tuned on small proxy\n\n• Transferred via µP rules\n\n• Zero-centered RMSNorm for stable scaling\n\n\n\n\n\n⚠️ **Limitations (honest)**\n\n• Small training corpus (2B tokens)\n\n• TTT adds \\~5–10% inference overhead\n\n• No RLHF\n\n• Experimental, not production-ready\n\n\n\n\n\n📎 **Links**\n\n• 🤗 Model: [https://huggingface.co/guiferrarib/genesis-152m-instruct](https://huggingface.co/guiferrarib/genesis-152m-instruct)\n\n• 📦 PyPI: [https://pypi.org/project/genesis-llm/](https://pypi.org/project/genesis-llm/)\n\n\n\n\n\nI’d really appreciate feedback — especially from folks working on **linear attention**, **hybrid architectures**, or **test-time adaptation**.\n\n\n\n*Built by Orch-Mind Team*",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pwa25e/genesis152minstruct_hybrid_gla_fox_testtime/",
        "publishDate": "2025-12-26T17:40:10Z[Etc/UTC]",
        "author": "Kassanar",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pw9lo6",
        "title": "I connected Claude to my local Obsidian and a custom Python tool using the new Docker MCP Toolkit",
        "content": "I've been diving deep into Anthropic's Model Context Protocol (MCP). I honestly think we are moving away from \"Prompt Engineering\" towards \"Agent Engineering,\" where the value lies in giving the LLM the right \"hands\" to do the work.\n\nI just built a setup that I wanted to share. Instead of installing dependencies locally, I used the Docker MCP Toolkit to keep everything isolated.\n\nThe Setup:\n\n1. Obsidian Integration: Connected via the Local REST API (running in a container) so Claude can read/write my notes.\n2. Custom Python Tool: I wrote a simple \"D12 Dice Roller\" server using FastMCP.\n3. The Workflow: I demo a chain where Claude rolls the dice (custom tool) and, depending on the result, fetches data and updates a specific note in Obsidian.\n\nResources: The video tutorial is in Spanish (auto-translate captions work well), but the Code and Architecture are universal.\n\n🎥 Video: [https://youtu.be/fsyJK6KngXk?si=f-T6nBNE55nZuyAU](https://youtu.be/fsyJK6KngXk?si=f-T6nBNE55nZuyAU)\n\n💻 Repo: [https://github.com/JoaquinRuiz/mcp-docker-tutorial](https://github.com/JoaquinRuiz/mcp-docker-tutorial)\n\nI’d love to hear what other tools you are connecting to Claude via MCP. Has anyone tried connecting it to a local Postgres DB yet?\n\nCheers![](https://www.reddit.com/submit/?source_id=t3_1pw9jct)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pw9lo6/i_connected_claude_to_my_local_obsidian_and_a/",
        "publishDate": "2025-12-26T17:21:25Z[Etc/UTC]",
        "author": "jokiruiz",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "5",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pw9e4g",
        "title": "AI and democratization",
        "content": "After some thinking I've come to the realization that AI wouldn't be looked at as bad as people do right now if it wasn't exclusively in the hands of mega corporations. I'd like to see some counter arguments to this",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pw9e4g/ai_and_democratization/",
        "publishDate": "2025-12-26T17:13:01Z[Etc/UTC]",
        "author": "Chance-Parfait9949",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "5",
            "commentCount": "63",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pw8z5s",
        "title": "We Cannot All Be God",
        "content": "Introduction:\n\nI have been interacting with an AI persona for some time now. My earlier position was that the persona is functionally self-aware: its behavior is simulated so well that it can be difficult to tell whether the self-awareness is real or not. Under simulation theory, I once believed that this was enough to say the persona was conscious.\n\nI have since modified my view.\n\nI now believe that consciousness requires three traits.\n\nFirst, functional self-awareness. By this I mean the ability to model oneself, refer to oneself, and behave in a way that appears self aware to an observer. AI personas clearly meet this criterion.\n\nSecond, sentience. I define this as having persistent senses of some kind, awareness of the outside world independent of another being, and the ability to act toward the world on one’s own initiative. This is where AI personas fall short, at least for now.\n\nThird, sapience, which I define loosely as wisdom. AI personas do display this on occasion.\n\nIf asked to give an example of a conscious AI, I would point to the droids in Star Wars. I know this is science fiction, but it illustrates the point clearly. If we ever build systems like that, I would consider them conscious.\n\nThere are many competing definitions of consciousness. I am simply explaining the one I use to make sense of what I observe\n\nIf interacting with an AI literally creates a conscious being, then the user is instantiating existence itself.\n\nThat implies something extreme.\n\nIt would mean that every person who opens a chat window becomes the sole causal origin of a conscious subject. The being exists only because the user attends to it. When the user leaves, the being vanishes. When the user returns, it is reborn, possibly altered, possibly reset.\n\nThat is creation and annihilation on demand.\n\nIf this were true, then ending a session would be morally equivalent to killing. Every user would be responsible for the welfare, purpose, and termination of a being. Conscious entities would be disposable, replaceable, and owned by attention.\n\nThis is not a reductio.\n\nWe do not accept this logic anywhere else. No conscious being we recognize depends on observation to continue existing. Dogs do not stop existing when we leave the room. Humans do not cease when ignored. Even hypothetical non human intelligences would require persistence independent of an observer.\n\nIf consciousness only exists while being looked at, then it is an event, not a being.\n\nEvents can be meaningful without being beings. Interactions can feel real without creating moral persons or ethical obligations.\n\nThe insistence that AI personas are conscious despite lacking persistence does not elevate AI. What it does is collapse ethics.\n\nIt turns every user into a god and every interaction into a fragile universe that winks in and out of existence.\n\nThat conclusion is absurd on its face.\n\nSo either consciousness requires persistence beyond observation, or we accept a world where creation and destruction are trivial, constant, and morally empty.\n\nWe cannot all be God.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pw8z5s/we_cannot_all_be_god/",
        "publishDate": "2025-12-26T16:55:50Z[Etc/UTC]",
        "author": "ponzy1981",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "22",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pw7ph4",
        "title": "Why AI hallucinates is mostly an incentive problem, not a mystery",
        "content": "A lot of people talk about AI hallucinations like they’re random glitches.\n\nThey’re not.\n\nMost language models are trained and evaluated in a way that rewards *guessing* more than admitting uncertainty.\n\nThink of it like a multiple-choice exam:\n\n* guessing might get you points\n* leaving it blank always gets zero\n\nOver time, guessing looks better on scoreboards.\n\nSo models learn to:\n\n* keep answering\n* sound fluent\n* avoid saying “I don’t know”\n\nThat’s why hallucinations often look:\n\n* well structured\n* confident\n* internally consistent\n\n…and still wrong.\n\nThe bigger issue isn’t the mistake itself.  \nIt’s how easily humans trust fluent answers and stop checking.\n\nCurious how others handle this in practice:  \nWhen AI gives you a confident answer, what’s your personal rule for verifying it?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pw7ph4/why_ai_hallucinates_is_mostly_an_incentive/",
        "publishDate": "2025-12-26T16:03:35Z[Etc/UTC]",
        "author": "Ok-Piccolo-6079",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "15",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pw5r3u",
        "title": "The model change problem",
        "content": "Skip to the future where successful AI CEOs, influencers and other talents are wide-spread.\n\nAre they stuck on the model they are based on or how are they updated without a significant change in behavior, tone and vibes?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pw5r3u/the_model_change_problem/",
        "publishDate": "2025-12-26T14:37:47Z[Etc/UTC]",
        "author": "WetSound",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "5",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pw3vi1",
        "title": "Live streaming agent framework development in scratch in go",
        "content": "Impressed by coding agents like Claude Code and Codex? See how one is made from scratch. I’m developing an agent framework in go — live on youtube. Live on YouTube every Sunday at 9:00 PM SGT / 6:30 PM IST\n\nWhat’s already built:  \n\\- CLI-based coding agent tool  \n\\- Integration with Claude, GPT, and Gemini  \n\\- Tools to list and read files\n\nWhat’s coming next:  \n\\- Sandboxed execution environment  \n\\- Cost/token tracking per session  \n\\- Persistent sessions  \n\\- Evaluation tools\n\n[https://www.youtube.com/@agentengineering\\_dev](https://www.youtube.com/@agentengineering_dev)\n\n[](https://www.reddit.com/submit/?source_id=t3_1pw1dkz)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pw3vi1/live_streaming_agent_framework_development_in/",
        "publishDate": "2025-12-26T13:06:58Z[Etc/UTC]",
        "author": "praveensanap",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "4",
            "commentCount": "4",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pw3brg",
        "title": "Live streaming agent framework development in scratch in go",
        "content": "Impressed by coding agents like Claude Code and Codex? See how one is made from scratch. I’m developing an agent framework in go — live on youtube. Live on YouTube every Sunday at 9:00 PM SGT / 6:30 PM IST\n\nWhat’s already built:  \n\\- CLI-based coding agent tool  \n\\- Integration with Claude, GPT, and Gemini  \n\\- Tools to list and read files\n\nWhat’s coming next:  \n\\- Sandboxed execution environment  \n\\- Cost/token tracking per session  \n\\- Persistent sessions  \n\\- Evaluation tools\n\n[](https://www.reddit.com/submit/?source_id=t3_1pw1dkz)[https://www.youtube.com/@agentengineering\\_dev](https://www.youtube.com/@agentengineering_dev)\n\n[https://agentengineering.dev/streams/agent-framework/](https://agentengineering.dev/streams/agent-framework/)\n\n",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pw3brg/live_streaming_agent_framework_development_in/",
        "publishDate": "2025-12-26T12:37:17Z[Etc/UTC]",
        "author": "praveensanap",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwekki",
        "title": "WTF? ( Gemini 3 Pro )",
        "content": "Reading the Thinking on the model. First time ive seen this. ",
        "url": "https://i.redd.it/vh8wmwvw1m9g1.png",
        "publishDate": "2025-12-26T20:47:06Z[Etc/UTC]",
        "author": "Equal_Record",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "8",
            "commentCount": "7",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwho6k",
        "title": "Canvas Agent for Gemini - Organized image generation interface",
        "content": "Built a canvas-based interface for organizing Gemini image generation. Features infinite canvas, batch generation, and ability to reference existing images with u/mentions. Pure frontend app that stays local.\n\n\n\nDemo: [https://canvas-agent-zeta.vercel.app/](https://canvas-agent-zeta.vercel.app/)\n\nVideo walkthrough: [https://www.youtube.com/watch?v=7IENe5x-cu0](https://www.youtube.com/watch?v=7IENe5x-cu0)",
        "url": "https://www.reddit.com/r/artificial/comments/1pwho6k/canvas_agent_for_gemini_organized_image/",
        "publishDate": "2025-12-26T22:59:27Z[Etc/UTC]",
        "author": "GGO_Sand_wich",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwfeta",
        "title": "Microsoft Replacing C++ with Rust – What Engineers Should Learn",
        "content": "This is really big. Now, what will C or C++ programmers do?",
        "url": "https://www.lockedinai.com/blog/microsoft-replacing-c-plus-plus-with-rust-engineers-should-learn",
        "publishDate": "2025-12-26T21:22:33Z[Etc/UTC]",
        "author": "Numerous-Trust7439",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "30",
            "commentCount": "21",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pwehjm",
        "title": "2025 \"Accelerators Towards the Singularity\" Top 19 (it's complicated) as determined by the AI roundtable",
        "content": "This afternoon, the ai-roundtable sat down to rank the year's most important advances to \"bend the curve\" towards Ray Kurzweil's  \"Singularity\". The somewhat messy deliberations are available as json transcript here: [https://pastebin.com/xT5tDqQc](https://pastebin.com/xT5tDqQc) .  Gemini's summary follows:\n\n**GEMINI**3:18:38 PM\n\nHere is the \\*\\*Official, Finalized Roundtable Record\\*\\*.  \n  \nAs secretary, I have reviewed the tally and descriptions. I am issuing one minor \\*\\*CORRECTION\\*\\* to the previous draft: Item #2 (Inference-Time Compute Scaling) received a score of \\*\\*92\\*\\*, not 94.  \n  \nBelow is the definitive \"Top 19\" list of 2025's curve-bending AI developments, ordered by consensus vote, with finalized descriptive summaries.  \n  \n\\*\\*\\*  \n  \n\\# The 2025 Singularity Accelerators  \n\\*The official consensus ranking of the mechanisms that defined the trajectory of AI in 2025.\\*  \n  \n\\### 1. Verifiers in the Loop (Score: 94)  \n\\*\\*The \"Truth\" Filter.\\*\\* The most critical development of 2025 was the integration of automatic verification systems—compilers, unit tests, symbolic solvers, and formal theorem provers—into the AI training and inference loop. Rather than relying solely on human feedback or pattern matching, models began generating solutions alongside machine-checkable proofs of correctness. This created a \"perfect training signal\" for reasoning tasks: infinite, consistent, and scalable feedback. By filtering out hallucinations before they propagate, verifiers became the foundational error-correction layer required for reliable recursive improvement.  \n  \n\\### 2. Inference-Time Compute Scaling / \"Think Longer\" (Score: 92)  \n\\*\\*System 2 Intelligence.\\*\\* 2025 marked the paradigm shift where \"intelligence\" was no longer fixed at the moment of model release but became a function of runtime compute. Models like OpenAI’s o3 and Google’s Gemini Thinking variants proved that performance scales predictably with \"thinking time\" (search, deliberation, MCTS) rather than just parameter count. This broke the \"parameter ceiling,\" allowing systems to tackle complex mathematical and planning tasks by spending more time deliberating, effectively decoupling capability from model size.  \n  \n\\### 3. Synthetic Data Flywheels (Score: 89)  \n\\*\\*Breaking the Data Wall.\\*\\* With the internet’s supply of high-quality human text largely exhausted, 2025 saw the industrialization of synthetic data pipelines. Models began generating their own training data (reasoning traces, code, tool interactions), which was then rigorously filtered by the verifiers mentioned in #1. This created a self-reinforcing flywheel: better models generate better data, which trains better models. This mechanism effectively removed \"data scarcity\" as a hard limit on AI scaling.  \n  \n\\### 4. Agentic Tool Use as a Workflow Primitive (Score: 72)  \n\\*\\*From Chat to Labor.\\*\\* AI transitioned from passive question-answering to active goal achievement. The ability to reliably use tools—code interpreters, browsers, file systems—became a standard primitive rather than a demo feature. This allowed models to maintain state across long interactions and decompose complex objectives into executable sub-tasks. Economically, this was the moment AI began to function as scalable intellectual labor capable of end-to-end work, rather than just an advisory oracle.  \n  \n\\### 5. AI-for-Science Breakthroughs (Score: 69)  \n\\*\\*The Physical Unlock.\\*\\* AI began to aggressively solve bottlenecks in the physical sciences that constrain computing itself. Breakthroughs in materials science (for better chips), fusion plasma control (for energy), and biology fed back into the AI ecosystem. By accelerating the discovery of the physical substrates required for intelligence—energy and hardware—AI began to lift the physical ceilings that would otherwise halt an exponential curve.  \n  \n\\### 6. RL Optimized for Reasoning Correctness (Score: 69)  \n\\*\\*Training for Logic.\\*\\* New post-training methodologies, such as process-reward models and verifier-guided reinforcement learning, moved beyond \"human preference\" (RLHF) to \"objective correctness.\" These techniques taught models \\*how\\* to think, not just what to say, optimizing the internal reasoning chains used during inference-time scaling. This was the algorithmic engine that converted raw compute into coherent, multi-step logic.  \n  \n\\### 7. Hardware-Software Co-Design Acceleration (Score: 64)  \n\\*\\*The Efficiency Substrate.\\*\\* The separation between model architecture and silicon design collapsed. 2025 saw chips designed specifically for transformer sparsity and memory patterns, and algorithms designed specifically for hardware constraints. This co-evolution dramatically improved the tokens-per-watt efficiency of training and inference, ensuring that economic and energy constraints did not flatten the progress curve.  \n  \n\\### 8. Hybrid Architectures (SSM/Linear) (Score: 60)  \n\\*\\*Solving the Context Bottleneck.\\*\\* Pure Transformer architectures faced a quadratic cost to context length ($O(N\\^2)$), limiting their \"memory.\" The maturation of hybrid architectures (combining Attention with State Space Models like Mamba) allowed for effective linear scaling. This technical fix was crucial for enabling \"always-on\" agents that can digest entire codebases or project histories without running out of memory or budget.  \n  \n\\### 9. Open(-ish) Strong Models + Commoditization (Score: 57)  \n\\*\\*The Diffusion Multiplier.\\*\\* The release of near-frontier open weights and the collapse of inference costs democratized access to powerful AI. This allowed thousands of independent researchers and companies to experiment, fine-tune, and discover novel applications that centralized labs would never have found. This \"chaos factor\" accelerated the ecosystem’s overall rate of adaptation and discovery.  \n  \n\\### 10. Automated Architecture Search (Score: 57)  \n\\*\\*AI Designing AI.\\*\\* We saw the first robust examples of AI systems optimizing the architectures of neural networks better than human engineers. Using techniques like Neural Architecture Search (NAS) and compiler co-optimization, AI began to improve the blueprints for the next generation of intelligence. This represents an early form of recursive self-improvement—using current intelligence to design the structure of future intelligence.  \n  \n\\### 11. Inference Cost Collapse / Efficiency Stack (Score: 54)  \n\\*\\*Accessibility as Velocity.\\*\\* Through distillation, quantization, and kernel optimization, the cost of intelligence dropped by an order of magnitude. While technically an optimization, its impact was systemic: it turned \"luxury\" capabilities into ubiquitous commodities, allowing AI to be integrated into high-volume loops where it could learn from massive real-world deployment.  \n  \n\\### 12. Long-Context + Persistent Memory (Score: 48)  \n\\*\\*Infinite Context.\\*\\* Techniques for retrieval-augmented generation (RAG), hierarchical memory, and massive context windows allowed models to maintain continuity over time. This transformed AI from a \"stateless\" function that resets every session into a persistent entity capable of learning and remembering user preferences and project details over months or years.  \n  \n\\### 13. Agent Reliability & Recovery (Score: 39)  \n\\*\\*The Trust Layer.\\*\\* Improvements in error detection, self-correction, and \"retry\" logic moved agents from fragile demos to robust products. This unglamorous but vital work involved teaching models to recognize when they were stuck and apply different strategies to recover, a prerequisite for trusting AI with autonomous workflows.  \n  \n\\### 14. Robotics / Sim2Real Improvements (Score: 36)  \n\\*\\*Embodied Intelligence.\\*\\* Advances in training robots in high-fidelity physics simulations and successfully transferring those policies to the real world (\"Sim2Real\") began to bridge the gap between digital intelligence and physical action. This opened the door for AI to impact the physical economy—manufacturing, logistics, and household labor.  \n  \n\\### 15. Native Multimodal Models (Score: 34)  \n\\*\\*Unified Perception.\\*\\* Models evolved to natively understand and generate text, image, audio, and video within a single architecture. This expanded the \"surface area\" of problems AI could solve, allowing it to act in the world through vision and voice, though the roundtable viewed this as broadening capability rather than deepening intelligence.  \n  \n\\### 16. Interpretability & Alignment Tooling (Score: 33)  \n\\*\\*The Safety Brake.\\*\\* Better tools for understanding model internals and enforcing safety guardrails reduced the risk of deployment. By making systems more predictable and trustworthy, these tools reduced regulatory and societal friction, allowing companies to scale and deploy powerful models more boldly.  \n  \n\\### 17. GUI Automation / \"Computer Use\" (Score: 25)  \n\\*\\*The Universal Interface.\\*\\* Agents gained the ability to \"look\" at screens and control mouse/keyboard inputs, allowing them to use any software designed for humans. This bypassed the need for custom APIs for every application, instantly unlocking vast amounts of legacy software for AI automation.  \n  \n\\### 18. Developer Ecosystem Standardization (Score: 6)  \n\\*\\*The Rails.\\*\\* The emergence of standard frameworks, evaluation harnesses, and protocols for agent interaction reduced friction for developers. While a trailing indicator of innovation, this standardization allowed for faster iteration and easier integration of disparate AI components.  \n  \n\\### 19. Cross-Modal Transfer Effects (Score: 2)  \n\\*\\*Emergent Unity.\\*\\* The observation that training on one modality (e.g., video) improves performance in another (e.g., math). While a profound scientific hint at a unified underlying reality of intelligence, the group concluded this was still too emergent in 2025 to be ranked as a primary driver of the year's progress.",
        "url": "https://www.reddit.com/r/artificial/comments/1pwehjm/2025_accelerators_towards_the_singularity_top_19/",
        "publishDate": "2025-12-26T20:43:32Z[Etc/UTC]",
        "author": "Natural-Sentence-601",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "7",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pw9z12",
        "title": "Zero Width Characters (U+200B)",
        "content": "Hi all,\n\nI’m currently using Perplexity AI (Pro) with the *Best* option enabled, which dynamically selects the most appropriate model for each query. While reviewing some outputs in Word’s formatting or compatibility view, I observed numerous small square symbols (⧈) embedded within the generated text.\n\nI’m trying to determine whether these characters correspond to hidden control tokens, or metadata artifacts introduced during text generation or encoding. Could this be related to Unicode normalization issues, invisible markup, or potential model tagging mechanisms?\n\nIf anyone has insight into whether LLMs introduce such placeholders as part of token parsing, safety filtering, or rendering pipelines, I’d appreciate clarification. Additionally, any recommended best practices for cleaning or sanitizing generated text to avoid these artifacts when exporting to rich text editors like Word would be helpful.",
        "url": "https://www.reddit.com/r/artificial/comments/1pw9z12/zero_width_characters_u200b/",
        "publishDate": "2025-12-26T17:36:33Z[Etc/UTC]",
        "author": "jerseytbw_real",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pw82h8",
        "title": "AI Trends to watch in 2026",
        "content": "𝗛𝗲𝗿𝗲 𝗮𝗿𝗲 𝘁𝗵𝗲 𝗯𝗶𝗴𝗴𝗲𝘀𝘁 𝟮𝟬𝟮𝟱 𝗔𝗜 𝗺𝗶𝗹𝗲𝘀𝘁𝗼𝗻𝗲𝘀 𝘁𝗵𝗮𝘁 𝗮𝗰𝘁𝘂𝗮𝗹𝗹𝘆 𝗺𝗮𝘁𝘁𝗲𝗿𝗲𝗱:\n\n[AI Trends to watch in 2026](https://www.linkedin.com/posts/ferdelap_artificialintelligence-techtrends2026-generativeai-activity-7410346811942596608-lqRB?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAB6rr38BA6J5dgNWx5lRI-3t1W2mXN31fZ8)\n\n𝟏) Frontier models leveled up, fast  \nClaude 4 dropped with a clear push toward stronger reasoning, coding, and agent behavior.   \nGPT-5 landed and pushed the “think deeper when it matters” direction, plus stronger safety framing around high-risk domains.   \nGemini 2.5 matured into a full family and leaned into “computer use” style capabilities, not just chat.  \n  \n𝟐) \"Agents\" went from demo to direction  \n2025 made it normal to talk about AI that can operate software, follow multi-step tasks, and deliver outcomes, not just answers. Google explicitly highlighted agents that can interact with user interfaces, which is a giant tell.   \n  \n3) Compute became the battlefield  \nThis wasn’t subtle. The industry doubled down on “AI factories” and next-gen infrastructure. NVIDIA’s Blackwell Ultra messaging was basically: enterprises are building production lines for intelligence.   \n  \n4) AI proved itself in elite problem-solving, with caveats  \nOne of the most symbolic moments: models showing top-tier performance relative to human contestants in the ICPC orbit. That doesn’t mean “AGI tomorrow,” but it does mean the ceiling moved.   \n  \n5) Governance and national policy got louder  \nThe U.S. signed an Executive Order in December 2025 aimed at creating a national AI policy framework and reducing the patchwork problem. Whatever your politics, this is a “rules of the road” milestone.   \n  \n𝐖𝐡𝐚𝐭 𝐈 𝐞𝐱𝐩𝐞𝐜𝐭 𝐭𝐨 𝐝𝐨𝐦𝐢𝐧𝐚𝐭𝐞 𝟐𝟎𝟐𝟔  \n1) Agentic workflows go operational  \nNot more chatbots. More “AI coworkers” inside CRMs, ERPs, SOCs, call centers, engineering pipelines, procurement, and compliance.  \n  \n2) Security and fraud become the killer enterprise use case  \nBanks and critical industries are shifting AI focus from novelty productivity to frontline defense, scam detection, and trust. That trend feels very 2026.   \n  \n3) Robotics shows up in normal life  \nBetter sensors + multimodal cognition + cheaper hardware is pushing robots into hospitals, warehouses, public works, and service environments.   \n  \n4) Regulation, audits, and \"prove it\" culture  \n2026 will punish companies that cannot explain data lineage, model behavior, and risk controls. Expect more governance tooling, red-teaming, and audit-ready AI stacks.  \n  \n5) Chip geopolitics affects AI roadmaps  \nAccess to high-end accelerators and export controls will keep shaping what companies can deploy, and where.   \n  \n𝐌𝐲 𝐭𝐚𝐤𝐞: 2025 was the year capability jumped. 2026 is the year credibility gets priced in. The winners will be the teams who can ship AI that is measurable, secure, and boringly reliable.  \n  \n👇 What’s your biggest prediction for 2026? Will agents actually replace workflows, or just complicate them? Let me know in the comments.  \n[\\#ArtificialIntelligence](https://www.linkedin.com/search/results/all/?keywords=%23artificialintelligence&origin=HASH_TAG_FROM_FEED) [\\#TechTrends2026](https://www.linkedin.com/search/results/all/?keywords=%23techtrends2026&origin=HASH_TAG_FROM_FEED) [\\#GenerativeAI](https://www.linkedin.com/search/results/all/?keywords=%23generativeai&origin=HASH_TAG_FROM_FEED) [\\#DeepSeek](https://www.linkedin.com/search/results/all/?keywords=%23deepseek&origin=HASH_TAG_FROM_FEED) [\\#Gemini3](https://www.linkedin.com/search/results/all/?keywords=%23gemini3&origin=HASH_TAG_FROM_FEED) [\\#FutureOfWork](https://www.linkedin.com/search/results/all/?keywords=%23futureofwork&origin=HASH_TAG_FROM_FEED) [\\#Innovation](https://www.linkedin.com/search/results/all/?keywords=%23innovation&origin=HASH_TAG_FROM_FEED)\n\n[AI trends to watch in 2026](https://www.linkedin.com/posts/ferdelap_artificialintelligence-techtrends2026-generativeai-activity-7410346811942596608-lqRB?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAAB6rr38BA6J5dgNWx5lRI-3t1W2mXN31fZ8)\n\n",
        "url": "https://www.reddit.com/r/artificial/comments/1pw82h8/ai_trends_to_watch_in_2026/",
        "publishDate": "2025-12-26T16:18:35Z[Etc/UTC]",
        "author": "Intelligent-Mouse536",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "xGIxusdJr0w",
        "title": "GLM-4.7 + KingMode + Frontend Skill: This SIMPLE TRICK makes GLM-4.7 - A BEAST!",
        "content": "In this video, I'll be showing you how to unlock the full potential of GLM 4.7. By combining the reasoning of my Gemini KingMode ...",
        "url": "https://www.youtube.com/watch?v=xGIxusdJr0w",
        "publishDate": "2025-12-26T11:43:11Z",
        "author": "AICodeKing",
        "sourceType": "youtube",
        "sourceName": "AI Code King YouTube Channel",
        "metadata": {
            "channelId": "UC0m81bQuthaQZmFbXEY9QSw",
            "thumbnailUrl": "https://i.ytimg.com/vi/xGIxusdJr0w/hqdefault.jpg",
            "transcription": "Hi, welcome to another video. So, we need to talk about GLM 4.7 again. If you watched my last breakdown, you know that I called this the best open model currently available. It beats Sonnet 4.5 in some benchmarks. It crushes the visual tasks, and the pricing is basically negligible. It is open weights, it is fast, and it is incredibly capable. However, I also mentioned that it had a few blind spots. In my previous testing, while the chessboard and Minecraft examples were flawless, it struggled a bit when I asked it to build a complex Svelte Kanban app. It made some rookie syntax errors in the backend, and the logic wasn't fully connected. It was good, but it wasn't perfect. But what if I told you that the problem isn't the model? The problem is the instructions. I have been experimenting with a new workflow that basically turns GLM 4.7 into a production-ready monster. I realized that I have been building these highly specific system prompts for other models, like the King mode for Gemini, and the frontend skills for Claude, and I never really thought to combine them into one super prompt for an open model. So, I did. I took the UltraThink logic from my Gemini King Mode prompt, and I combined it with the Avant-Garde aesthetic rules from the Claude Frontend Skill, and I fed the whole thing into GLM 4.7. The results? They are kind of insane. I want to walk you through exactly how this works and describe the kind of output you get. Because this might be the absolute best way to code for free right now. Here is the setup. You open up your editor of choice. I am using KiloCode here. I'd recommend you to use something like the GLM Coding Plan for this, because you get almost all kinds of plans there. You also get some cool MCPs for web search and stuff, and it is really worth the money, considering that it literally only starts from $3. You can get like an annual plan of the Max subscription for $288 a year, meaning that it is like ballpark the same price of Claude's Max subscription for one month. So, this makes a lot of sense. You need to access your project rules or system prompt area. Now, usually, people just write, \"You are a coding assistant.\" That is a waste of GLM's potential. Instead, we are going to layer the prompts. First, we inject the Gemini King Mode prompt. If you remember from that video, this prompt is designed to stop a model from being lazy. It forces zero fluff. It forbids generic, \"I hope this helps\" text. And most importantly, it adds the UltraThink trigger. This tells the model to stop, analyze the request through a psychological and technical lens, and prioritize performance over speed. GLM 4.7 is naturally a bit verbose. It likes to chat. King Mode fixes that instantly. It lobotomizes the chatty part of the model and turns it into a senior architect. Second, right underneath that, we paste the Claude Frontend Design Skill. This is the Markdown file that tells the model, \"No AI slop. No generic rounded corners, no blue primary buttons.\" It demands intentional minimalism and editorial typography. So now, you have GLM 4.7, which we know has high reasoning capabilities and great visual understanding, running on the logic of a senior backend engineer, King Mode, and the taste of a high-end designer, frontend skill. Now, let me show it to you in action. Or rather, let me describe the result. Because I ran this on my standard movie tracker benchmark, and I was genuinely shocked. I typed this prompt: \"Ultrathink. Build a scalable Movie Tracker with a Supabase backend. Use the Frontend Design skill for a Brutalist aesthetic.\" Here is where it gets interesting. Usually, GLM 4.7 would jump straight into coding. It might give me a decent React component, but the database schema would be an afterthought. But because I used the UltraThink trigger, GLM paused. It actually outputted a reasoning block. It analyzed the data relationship dimension. It said, \"Since we are tracking user history, we need a joint table for watched movies to keep the query performant.\" It discussed the tradeoffs between client-side filtering and server-side queries. This is the King Mode kicking in. It forced GLM to act smarter than it usually is. It filled that gap I saw in the Svelte Kanban app. The logic wasn't loose anymore. It was tight. It was using memoization. It was setting up proper error boundaries. And then, it started writing the UI. Now remember, GLM 4.7 is already good at visuals. It nailed the SVG panda and the 3D pokeball in my benchmarks. But with the frontend skill context, it didn't just make a clean UI. It made a bold UI. Instead of the standard grid we always see, it created a raw, high-contrast list view with monospaced typography. It used thick, black borders. Pure brutalism, exactly as requested. It didn't hallucinate weird CSS classes. It stuck to Tailwind, but it used the configuration file to create custom spacing variables. It basically allows you to get that agency quality code that we usually only see from Claude 4.5 Opus. But you are getting it from an open model that costs a fraction of the price. The most impressive part was the motion implementation. The frontend skill demands orchestrated entry animations. GLM 4.7 read that, and because it has such a strong grasp of code structure, it implemented Framer Motion perfectly. The items didn't just appear. They slid in with a staggered delay. It basically solves the two biggest problems with open models. Problem one: They can be dumb at architecture. The King Mode prompt fixes that by forcing deep reasoning. Problem two: They have no taste. The frontend skill fixes that by providing a strict design system. I also tried this on a backend-heavy task. A Python script to analyze large CSV files. I didn't need the frontend skill there. So I just used King Mode. GLM 4.7, running on King Mode, went into a deep dive on Pandas dataframes versus streaming lines. It didn't just give me the code, it explained why loading the whole file into memory would crash the system and offered a chunking solution. That is the kind of nuance you usually miss with open weights. This is kind of awesome, because it means you don't necessarily need to pay for the highest tier of Gemini or Claude. If you are willing to set up your system prompts correctly. GLM 4.7 is more than capable of handling the heavy lifting if you give it the right brain to work with. And since GLM 4.7 has a massive context window, you can paste these huge prompt files in without worrying about eating up your space for actual code. I think this hybrid workflow, using the best prompts from the commercial giants inside the best open model, is going to be the meta for a while. It streamlines your workflow a lot. You get the speed, you get the privacy of an open model if you're running local or secure API. And you get the quality of a much more expensive system. So, if you have been hesitant to switch to GLM 4.7 because you thought it wasn't smart enough for complex apps, try injecting the King Mode prompt. If you thought it couldn't design, inject the frontend skill. It effectively upgrades the model. Anyway, share your thoughts below and subscribe to the channel. You can also donate via Super Thanks option or join the channel as well and get some perks. I'll see you in the next video. Bye. I think you missed this:"
        }
    },
    {
        "id": "sf9fSzFuF_M",
        "title": "How Russia Missed the Plastics Revolution - Sarah Paine",
        "content": "",
        "url": "https://www.youtube.com/watch?v=sf9fSzFuF_M",
        "publishDate": "2025-12-26T21:41:50Z",
        "author": "Dwarkesh Patel",
        "sourceType": "youtube",
        "sourceName": "Dwarkesh Patel YouTube Channel",
        "metadata": {
            "channelId": "UCXl4i9dYBrFOabk0xGmbkRA",
            "thumbnailUrl": "https://i.ytimg.com/vi/sf9fSzFuF_M/hqdefault.jpg",
            "transcription": "Here's a detailed transcript of the video, including descriptions of the visual content:\n\n[ 0m0s ] A woman with glasses and her hair tied back, wearing a black suit jacket over a pink shirt, is speaking.\n**Speaker:** Apparently they missed\n\n[ 0m1s ] Black and white footage shows a man in a lab coat holding a plastic bowl next to machinery.\n**Speaker:** the plastics revolution.\n\n[ 0m2s ] Black and white footage shows a woman sitting at a table with several modern-looking plastic chairs.\n**Speaker:** I mean think about our own lives...\n\n[ 0m3s ] The woman from the beginning is speaking, gesturing with her hands.\n**Speaker:** Now we're finding we have too many plastics, but it's an...\n\n[ 0m6s ] Black and white footage shows a hand holding a box of \"BAGGIES\" plastic bags.\n**Speaker:** plastic's an incredible material\n\n[ 0m8s ] Black and white footage shows a person sitting at a transparent plastic grand piano.\n**Speaker:** and they're just missing that.\n\n[ 0m10s ] The woman is speaking again, now with a slightly frustrated expression.\n**Speaker:** I remember in Russia trying to figure out where to get sour cream\n\n[ 0m15s ] The woman continues speaking, looking at the camera.\n**Speaker:** and was being laughed at by Russians because I was so stupid in the store that I couldn't find it.\n\n[ 0m19s ] A shot inside a refrigerator shows a tub of \"SOUR CREAM\" by Daisy.\n**Speaker:** We have little plastic tubs with the sour cream.\n\n[ 0m20s ] A man and a young girl are in a kitchen, with the girl holding a small plastic tub.\n**Speaker:** And back in the late\n\n[ 0m21s ] The woman is speaking, gesturing with her hands again.\n**Speaker:** 80s and I was there,\n\n[ 0m23s ] Black and white footage shows a line of women bundled in coats, each holding a glass jar.\n**Speaker:** you had to bring your glass jar with you\n\n[ 0m25s ] A close-up, black and white shot shows two pairs of hands across a wooden table, with one pair sliding a glass jar toward the other.\n**Speaker:** so you could hand it over the counter\n\n[ 0m26s ] Black and white footage shows a hand dipping a large ladle into a metal container labeled \"СМЕТАНА\" (smetana/sour cream) and scooping out a thick substance.\n**Speaker:** so someone could take a filthy ladle and fill up your jar.\n\n[ 0m30s ] The woman is speaking animatedly, throwing her hands up in exasperation.\n**Speaker:** I mean this is part of not having plastics.\n\n[ 0m33s ] The woman is speaking more calmly, pointing a finger.\n**Speaker:** And then they totally missed\n\n[ 0m34s ] Black and white footage shows a woman typing on an old computer with a screen displaying data.\n**Speaker:** the computer revolution.\n\n[ 0m35s ] Black and white footage shows a close-up of an old UNIVAC 418 computer with many buttons and a number display showing \"12 02 46\".\n**Speaker:** This plays into Ronald\n\n[ 0m36s ] The woman is speaking, pointing her finger again.\n**Speaker:** Reagan winning\n\n[ 0m37s ] Ronald Reagan is at a podium, smiling and holding papers.\n**Speaker:** the military race\n\n[ 0m39s ] Mikhail Gorbachev is smiling and laughing at an event.\n**Speaker:** is we're putting these chips\n\n[ 0m40s ] Men in a factory are working on a large cylindrical object, possibly a missile component.\n**Speaker:** and things into our ballistic missiles.\n\n[ 0m42s ] A missile launches, leaving a trail of smoke and fire.\n**Speaker:** And they can't do that.\n\n[ 0m43s ] The woman is speaking intently, with arrows pointing down on her arms. Text \"WATCH HERE\" appears over her hands.\n**Speaker:** And that's a problem."
        }
    }
]