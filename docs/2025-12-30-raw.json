[
    {
        "id": "https://news.smol.ai/issues/25-12-29-meta-manus/",
        "title": "\"Meta Superintelligence Labs acquires Manus AI for ~$4B, at $100M ARR, 9months after launch\"",
        "content": "**Manus** achieved a rapid growth trajectory in 2025, raising **$500M** from Benchmark and reaching **$100M ARR** before being acquired by **Meta** for an estimated **$4B**. The **vLLM** team launched a dedicated community site with new resources, while performance issues with **AMD MI300X FP8** were noted in **vLLM** and **sglang** benchmarks. **Weaviate** released operational features including **Object TTL**, **Java v6 client GA**, and **multimodal document embeddings**. API fragmentation concerns were raised by **Teknium** advocating for unified SDK wrappers. In open-weight models, **GLM-4.7** gained recognition as a reliable coding model with faster throughput on **Baseten**, and **MiniMax-M2.1** rose as a leading open agentic coder model, topping WebDev leaderboards.",
        "url": "https://news.smol.ai/issues/25-12-29-meta-manus/",
        "publishDate": "2025-12-29T05:44:39Z[Etc/UTC]",
        "author": "",
        "sourceType": "rss",
        "sourceName": "AI News RSS",
        "metadata": {
            "feedTitle": "AINews",
            "feedDescription": "Weekday recaps of top News for AI Engineers",
            "categories": "manus, benchmark, meta-ai-fair, vllm, amd, sglang, weaviate, teknim, baseten, alphaxiv, minimax, glm-4.7, minimax-m2.1, alex_wang, nat_friedman, performance-optimization, inference-frameworks, model-benchmarking, model-deployment, open-source-models, multimodality, api, code-generation, community-building"
        }
    },
    {
        "id": "https://ai-techpark.com/?p=231554",
        "title": "DebitMyData Lays the Foundation for the Human Energy Grid",
        "content": "<p>As AI accelerates beyond human employment capacity and public trust erodes under massive data center expansion, DebitMyData, Inc. is building the missing bridge — the human and digital foundation for the next economy. Before governments acted, before the Genesis Executive Order ignited a national push for ethical AI infrastructure, DebitMyData...</p>\n<p>The post <a href=\"https://ai-techpark.com/debitmydata-lays-the-foundation-for-the-human-energy-grid/\">DebitMyData Lays the Foundation for the Human Energy Grid</a> first appeared on <a href=\"https://ai-techpark.com\">AI-Tech Park</a>.</p>",
        "url": "https://ai-techpark.com/debitmydata-lays-the-foundation-for-the-human-energy-grid/",
        "publishDate": "2025-12-29T07:00:00Z[Etc/UTC]",
        "author": "GlobeNewswire",
        "sourceType": "rss",
        "sourceName": "AI Techpark RSS",
        "metadata": {
            "feedTitle": "AI - AI-Tech Park",
            "feedDescription": "AI, ML, IoT, Cybersecurity News & Trend Analysis, Interviews",
            "categories": "AI, AI news, AItech news, artificial intelligence news, Data center, DebitMyData, Human Energy"
        }
    },
    {
        "id": "1pzgrfw",
        "title": "Are Meta the next yahoo or have they cracked it with acquiring Manus?",
        "content": "Feels like they’ve had an incredibly frustrating year after all those hires, that then left. \n\nDo you think Mark and meta still has pull in the ai world, will it turn to the next yahoo? or can it only compete with more acquisitions like this? \n\nCan it actually still achieve something? ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pzgrfw/are_meta_the_next_yahoo_or_have_they_cracked_it/",
        "publishDate": "2025-12-30T12:22:01Z[Etc/UTC]",
        "author": "jason_digital",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzgmrr",
        "title": "Reports: Meta Acquires Manus AI in $2B Deal Amid Aggressive AI Push",
        "content": "According to recent reports circulating in the AI and startup ecosystem, Meta has acquired Manus AI, a Singapore-based artificial intelligence startup, in a deal reportedly valued at around **$2 billion**. While Meta has not yet publicly confirmed the full details, the acquisition—if finalized—would mark one of the company’s most significant strategic moves in the rapidly intensifying global AI race.\n\nMeta has been a major topic of discussion throughout the year due to its **aggressive recruitment of top AI talent**, offering compensation packages reportedly reaching well into the tens of millions for elite researchers. These hires are part of Meta’s broader effort to form an internal **“superintelligence” or advanced AI research team**, aimed at competing with OpenAI, Google DeepMind, Anthropic, and other leading labs.\n\nManus AI stands out in the current AI landscape because it is **one of the few well-known agentic AI companies not backed or owned by a major tech conglomerate**. The company has gained attention for its advanced autonomous agents capable of producing end-to-end outputs such as **iOS, Android, and web applications, websites, presentations, and visual assets**, often with minimal human intervention. This focus on agentic workflows aligns closely with the direction many AI labs believe represents the next major evolution beyond chat-based models.\n\nFrom a strategic perspective, Manus AI may have represented an attractive acquisition target for Meta due to its relative independence and maturity compared to startups already tightly integrated with Big Tech ecosystems. Rather than licensing or partnering, Meta appears to be doubling down on **full ownership of talent, infrastructure, and IP**, a pattern consistent with its recent AI strategy.\n\nMeta has been developing and branding its own assistant under the name **“Meta AI,”** which is already integrated across platforms like Facebook, Instagram, WhatsApp, and Messenger. With access to Meta’s vast compute resources, proprietary datasets, and billions of daily users, Manus AI’s technology could scale rapidly. In return, Meta gains a proven agentic AI stack and a pathway to **monetization through productivity tools, enterprise offerings, or premium AI services**—areas where Meta has historically had less revenue diversification compared to its ad business.\n\nIf executed well, this acquisition could strengthen Meta’s long-term positioning in AI and reassure investors that the company is serious about competing at the highest level, potentially impacting market confidence and stock performance. That said, acquisitions alone don’t guarantee breakthroughs, especially in a field where execution, alignment, and research culture matter as much as funding.\n\nOne open question is **pricing and accessibility**. Manus AI has been criticized by some users for its cost structure, and it remains to be seen whether Meta will subsidize or lower prices to drive adoption, as it has done with other consumer products.\n\nSo the bigger question remains:  \nWas this a calculated power move by Mark Zuckerberg to secure scarce AI talent and technology before competitors could—or a defensive reaction to falling behind in the AI arms race? And will Meta’s superintelligence ambitions translate into real, differentiated products, or just another expensive research experiment?\n\nCurious to hear what others think.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pzgmrr/reports_meta_acquires_manus_ai_in_2b_deal_amid/",
        "publishDate": "2025-12-30T12:15:16Z[Etc/UTC]",
        "author": "Top-Sock8617",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzfmay",
        "title": "What am I missing with LLMs?",
        "content": "Admittedly, I don't know much about the technical behind-the-scenes of this technology but from what I know, why are these actually impressive or considered any kind of \"intelligence\" at all?\n\nFrom what I've read the real triumph of these companies seems to be data collection, sorting and categorization rather than any kind of intelligence or generation, right? They basically scraped every book, article, comment and video transcript on the internet and off the internet. They have pretty much all of current human knowledge in their data centers. So why is it impressive then that their models can use all of that data to answer questions and say stuff? What is new that is actually being created there?\n\nI know I'm massively oversimplifying and I know that what they did was incredibly challenging to create in reality, so I'm not diminishing the effort. But when you look at their electricity, water and processing demands, to my noob eye it does seem like these LLMs are basically just a brute force thing with no actual intelligence anywhere. \n\nTLDR: Why is it impressive that all of the world's knowledge + all of the world's processors = something that's pretty knowledgable and useful? Why do we have to call that AI at all? ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pzfmay/what_am_i_missing_with_llms/",
        "publishDate": "2025-12-30T11:19:45Z[Etc/UTC]",
        "author": "don_dripac",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "25",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzezs6",
        "title": "How do you handle long-term context and memory in AI-assisted work?",
        "content": "One issue I keep running into with AI tools (coding assistants, copilots, chat-based agents) is long-term context loss.\n\nIn longer projects, the model often:\n\n* forgets earlier decisions\n* re-suggests approaches we already rejected\n* ignores constraints that were clearly defined before\n\nDocumentation helps, but it’s static. It doesn’t decide what’s relevant *now*, and it still relies on humans to re-inject context manually.\n\nI’m curious how others here approach this problem in practice:\n\n* Do you rely on docs (Notion, markdown, etc.)?\n* Do you maintain some kind of external memory or state?\n* Have you experimented with persistent or shared memory across sessions or teams?\n\nNot looking for tools per se — more interested in patterns, experiments, or research directions people have found useful.\n\nIf anyone is exploring memory, retrieval, or context management at a system level, I’d love to hear how you think about it.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pzezs6/how_do_you_handle_longterm_context_and_memory_in/",
        "publishDate": "2025-12-30T10:43:05Z[Etc/UTC]",
        "author": "sabahsquataksamvkuat",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "6",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzeai8",
        "title": "GPT 5.2  Only me!",
        "content": "Oooo you don't want to do it like that...\n\n[https://youtu.be/EimWSrRpa1g?si=-lwKxStJtfEYgqGa&t=1431](https://youtu.be/EimWSrRpa1g?si=-lwKxStJtfEYgqGa&t=1431)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pzeai8/gpt_52_only_me/",
        "publishDate": "2025-12-30T10:00:41Z[Etc/UTC]",
        "author": "phoenixxl",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzdo38",
        "title": "Are AI's achieving some degree of consciousness?",
        "content": "I have noticed how having a discussion with many real people online can be just like talking to an AI. For me it shows how many people are just like AI LLMs. Their beliefs and knowledge is a regurgitated product of everything they have read or been told etc. Yet they don't really understand what they are talking about. \n\nWhen they don't understand what is going on they can go into conspiracy theories, and seek to lay blame in a witch hunt sort of way. This is the human equivalent of AI hallucinations.\n\nDeveloping artificial intelligence requires us to understand ever more about our own intelligence.\n\nSo then the question for me is are shallow thinking people actually conscious? Is consciousness not something you have or don't have but is something that increases by degree? ie are some people more conscious than others and is AI beginning to overlap at the bottom end  of the human consciousness spectrum.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pzdo38/are_ais_achieving_some_degree_of_consciousness/",
        "publishDate": "2025-12-30T09:21:38Z[Etc/UTC]",
        "author": "grahamsuth",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "22",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzd9sv",
        "title": "Wait… Gmail will finally let us change our email address?",
        "content": "Just found out Google is rolling out a way to change your actual Gmail address without nuking your whole account. Which sounds normal… until you remember Gmail has existed for two decades and this was somehow never an option.\n\n[https://winbuzzer.com/2025/12/26/google-to-finally-allow-gmail-address-changes-ending-a-20-year-restriction-xcxwbn/](https://winbuzzer.com/2025/12/26/google-to-finally-allow-gmail-address-changes-ending-a-20-year-restriction-xcxwbn/)\n\nThink about how many people are stuck with dumb usernames they made as teens  \nOR emails tied to old jobs / side hustles OR addresses that look unprofessional but are glued to everything (banking, logins, recovery emails, etc.) And now Google’s like “yeah okay, you can change it… but only a few times.”\n\nPart of me is relieved.\n\nAnother part of me is wondering why this took 20 years, especially when aliases and identity management have been solved problems forever.\n\nFeels less like innovation and more like Google finally admitting,\n\n“yeah okay, this was annoying.”",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pzd9sv/wait_gmail_will_finally_let_us_change_our_email/",
        "publishDate": "2025-12-30T08:57:12Z[Etc/UTC]",
        "author": "biz4group123",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzd7og",
        "title": "So you can earn $4,250,000 USD a year by letting AI spam YouTube garbage at new users?",
        "content": "I went down a rabbit hole today and apparently a huge chunk of YouTube’s recommendations (especially for new accounts) is just AI-generated junk now. Like, low-effort voiceovers, weird visuals, recycled scripts, the whole thing.\n\n[https://winbuzzer.com/2025/12/28/report-unveils-how-youtubes-new-ai-slop-economy-generates-millions-xcxwbn/](https://winbuzzer.com/2025/12/28/report-unveils-how-youtubes-new-ai-slop-economy-generates-millions-xcxwbn/)\n\nWhat surprised me is the money. Some of these channels are reportedly pulling in millions per year doing this. Not “smart automation” or “cool AI experiments” - just mass-produced content designed to game the algorithm.\n\nAnd YouTube keeps pushing it because… engagement.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pzd7og/so_you_can_earn_4250000_usd_a_year_by_letting_ai/",
        "publishDate": "2025-12-30T08:53:35Z[Etc/UTC]",
        "author": "biz4group123",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "77",
            "commentCount": "39",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzd3hc",
        "title": "Hot take: humans shouldn’t always be the first line of telehealth",
        "content": "A lot of telehealth friction comes from forcing every interaction through a human first. Not everything needs a clinician at step one. Good AI can handle intake, symptom collection, basic triage, and routing far more consistently, especially after hours. That doesn’t replace care, it protects it. The real bottleneck in telehealth is inefficient front-door design. When AI handles the noise, clinicians actually get to focus more and spend more time on cases that need judgment.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pzd3hc/hot_take_humans_shouldnt_always_be_the_first_line/",
        "publishDate": "2025-12-30T08:46:26Z[Etc/UTC]",
        "author": "biz4group123",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "12",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzcvwu",
        "title": "AI in 2026: Learning to live with powerful systems",
        "content": "Sharing [this text](https://www.diplomacy.edu/blog/ai-in-2026-learning-to-live-with-powerful-systems/) because the piece doesn’t just hype AI, it speaks about how we’ll actually live with it.\n\nMost articles about AI are either breathless about breakthroughs or panicked about doom. This one takes a calmer angle, showing how by 2026 the real questions aren’t about what AI *can* do, but how we as humans decide to use it, like treating AI as infrastructure, and building “AI literacy” so regular people can question and shape its use. That angle feels way more grounded than the usual extremes.\n\nSo if you’re tired of the usual 'AI will save us / AI will destroy us' takes, this piece is a thoughtful middle ground.\n\n",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pzcvwu/ai_in_2026_learning_to_live_with_powerful_systems/",
        "publishDate": "2025-12-30T08:33:05Z[Etc/UTC]",
        "author": "simsirisic",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "5",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzcjd8",
        "title": "Does AI increase creativity—or just homogenize it? A mechanism-level explanation",
        "content": "The strongest evidence suggests that **GenAI can increase the creativity of produced outputs** (texts/ideas), especially when the user has a lower baseline skill level or uses AI as a “sparring partner.”  \nAt the same time, there is a real risk of **convergence / homogenization** — it may help individuals, but at the collective level ideas become more similar.\n\n**Question:**  \nDoes this convergence/homogenization result from the *training trajectory* of AI models, in the sense that they extrapolate into new domains by pulling toward known solutions, thereby reducing creativity emerging from AI use?\n\nThe mechanism appears to be **mixed**: model properties + usage patterns + evaluation criteria + shared data sources.\n\n# 1) What in the model itself promotes convergence\n\n* Training on large data distributions + the objective of predicting the *most probable next token* creates a strong tendency to generate **modal (typical) continuations** for a given prompt. This is not “extrapolation toward known solutions” in a strict mathematical sense, but rather **interpolation in the space of styles and content**: the model prefers regions of high data density.\n* Additionally, **RLHF / user preference optimization** tends to reinforce answers that are “safe,” “clear,” and “standard,” which further reduces extreme originality.\n\nThis aligns with the observation that many users receive very similar “good” answers when using similar prompts.\n\n# 2) What in human usage amplifies homogenization (often more strongly)\n\n* Users naturally formulate similar prompts (“generate 10 ideas…”, “write an email…”), and then select what *sounds best* → selection pressure toward what the model most often produces.\n* In brainstorming, AI is often used as a *first-wave generator* — and that first wave strongly anchors subsequent ideas (anchoring effect). This is a purely cognitive mechanism, independent of the model’s training trajectory.\n\nThis exact trade-off — higher quality of individual outputs but lower diversity across a set — has been observed in empirical studies on GenAI-assisted ideation and writing.\n\n# 3) “Extrapolation” vs. “known solutions”\n\nThe ML is called **regression to the mean / mode-seeking**: when a task is underspecified, the model selects statistically “safe” solutions.  \nThis *looks like* pulling toward known solutions.\n\nHowever, this is **not** a necessary consequence of the training trajectory itself — convergence can be mitigated.\n\n# 4) When AI can increase diversity instead of reducing it\n\n* When divergence is explicitly enforced in prompts (“Generate 20 ideas from 5 different schools of thought; each must differ structurally; ban clichés X, Y, Z”)\n* When more exploratory sampling is used (e.g., higher temperature, different seeds)\n* When the model operates in a **critic–proponent** mode (one instance generates, another attacks banality)\n* When evaluation includes not only “quality” but also **diversity** (semantic distance / novelty metrics)\n\n# Conclusion\n\nHomogenization does **not** arise solely because models “extrapolate from the known.”  \nIt is the combined effect of:\n\n\\*\\*mode-seeking behavior of the model\n\n* prompting / anchoring\n* user selection\n* preference optimization (RLHF).\\*\\*\n\nI can also formalize this as a **minimal model** (e.g., sampling from a distribution + utility-based selection), showing when variance must shrink and when it can grow.\n\n# Recommendation\n\nIt *is* worth having a persistent instruction (in “custom instructions” or similar) that systematically enforces divergence — with two key caveats:\n\n1. Not “JSON as magic,” but concrete, consistently applied generation and selection rules.\n2. A **two-phase process** works best: first divergence (broad exploration), then controlled convergence (selection + critique). Otherwise, you get chaos.\n\n# Example instruction block (ready to paste)\n\n    {\n      \"creativity_mode\": {\n        \"goal\": \"minimize convergence/homogenization, maximize semantic diversity\",\n        \"process\": [\n          \"PHASE 1 (divergence): generate N=12 proposals in 4 buckets of 3 each: (A) conservative, (B) alternative paradigms, (C) contrarian/counterintuitive, (D) hybrid/cross-domain.\",\n          \"Enforce disjointness: each proposal must rely on a different mechanism/heuristic; no repeated structures or clichés.\",\n          \"Add one sentence explaining why it is different and one risk/limitation for each proposal.\"\n        ],\n        \"anti_convergence_constraints\": [\n          \"Avoid default framing (e.g., generic '10 ideas' lists).\",\n          \"Do not average directions; if there are two paths, separate them.\",\n          \"Avoid canonical templates; prefer high semantic distance.\"\n        ],\n        \"selection\": [\n          \"PHASE 2 (controlled convergence): select TOP 3 using criteria: (1) novelty, (2) feasibility, (3) potential impact.\",\n          \"For each of the TOP 3, propose a next experimental step or falsification test.\"\n        ],\n        \"defaults\": {\n          \"ask_for_context_if_missing\": true,\n          \"output_format\": \"concise bullet points\"\n        }\n      }\n    }\n\n# Why this works (intuitively)\n\n* “Buckets” force multiple generative regimes instead of one modal solution.\n* “Disjointness + heuristics” suppress paraphrasing.\n* “Risk/limitation” weakens RLHF pressure toward overly smooth outputs.\n* “Falsification tests” favor ideas that differ *structurally*, not stylistically.\n\nYou can further specialize this (e.g., for science) by adding mandatory **alternative hypotheses** and **discriminating predictions**.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pzcjd8/does_ai_increase_creativityor_just_homogenize_it/",
        "publishDate": "2025-12-30T08:11:41Z[Etc/UTC]",
        "author": "TeachingNo4435",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "6",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzannr",
        "title": "One-Minute Daily AI News 12/30/2025",
        "content": "1. **Meta** to buy Chinese startup Manus to boost advanced AI.\\[1\\]\n2. **Korea** building national AI-ready health data infrastructure.\\[2\\]\n3. From **Gemma** 3 270M to FunctionGemma, How Google AI Built a Compact Function Calling Specialist for Edge Workloads.\\[3\\]\n4. 2025 was the year AI got a vibe check.\\[4\\]\n\nSources included at: [https://bushaicave.com/2025/12/30/one-minute-daily-ai-news-12-30-2025/](https://bushaicave.com/2025/12/30/one-minute-daily-ai-news-12-30-2025/)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pzannr/oneminute_daily_ai_news_12302025/",
        "publishDate": "2025-12-30T06:22:37Z[Etc/UTC]",
        "author": "Excellent-Target-847",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "7",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pza1ft",
        "title": "The Time Intelligence Economy - Part 1 - The AI Noise",
        "content": "[https://rishi.monster/posts/time-intelligence-economy-part-1-the-ai-noise/](https://rishi.monster/posts/time-intelligence-economy-part-1-the-ai-noise/)\n\nDisclaimer - I wrote this article and looking for eyes on it for feedback and discussion.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pza1ft/the_time_intelligence_economy_part_1_the_ai_noise/",
        "publishDate": "2025-12-30T05:49:08Z[Etc/UTC]",
        "author": "wawhal",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz9epk",
        "title": "Wake up",
        "content": "Im anxious. Claude code / opus 4.5 met the agi threshold for me, and I suspect others feel the same. \n\nThe coming months and years will be profound. I haven’t fully grasped the implications of the alien tool we’ve summoned, but I feel uneasy. I feel unwell. \n\nBecause our pace of growth is fast; and it’s accelerating. Agents creating agents creating… something else. \n\nWhen youre on an inflection curve, despite it’s exponential steepness, if you take a tangent it looks flat. Today, it may look flat. Zoomed out, we’re definitively on a development path that is both steeper than anything humanity has experienced and only getting steeper. \n\nThis genie, this super god that is super smart, will supersede us in all our human tasks. 80% of knowledge work will cease to create positive economic value above what cheap inference can provide. Our capitalism, deeply engrained in corporate and personal ethos, will inevitably favor machine over human labor for good reason: it’s cheap, abundant, and infinite. Once we pass the event horizon, which I suspect we have, the rapid replacement of human labor with machines which beget further machines, will be unstoppable. \n\nWhat do you do here. What do I do. I’ve been reading a lot from philosophers and sociologists of the past to understand humanity when it was faced with similar disruption at the Industrial Revolution and 20th century. Polanyi, Marx. What we’re about to experience is without any parallel. The rapid devaluation of labor and knowledge will cause tremendous crisis. A general purpose intelligence will far exceed the impacts of any previous tool (gin mill, steam engine, printing press, etc). Those that bought into a world of specialized knowledge, one where labor extracted outsized returns against capital due to unique thought, will be the first to go. These expensive college graduates. The modern intelligencia, the pseudo proletariat, are most at risk and should be the most concerned for what’s about to come. \n\nNo, you’re not safe. No, your job is not secure. \n\nWhat to do here and now. The capital class will accrue and accrue as human labor in the next few years gets replaced by machines. There’s no escape hatch; no pressure valve. Humans will cease to have any salary and the rich will dominate our society while 99% of the world suffers immensely unless something is done. \n\nBut really, what can be done. This is inevitable. The thinking machines are here and they’re getting smarter by the day. There is no role for humans anymore. \n\nI think we need to recognize this position we’re in. This is very uncomfortable and this technology is the defining apogee of our collective millennia of evolution. To stand in its way is foolish; to ignore its ramifications is species suicide. \n\nRemember you’re not alone. I feel this too. Remind yourself what makes you human. Your ability to feel and taste and believe. Your opinions and values and friends. I encourage you to talk to someone, a human, and to touch the grass. If the technofeudallords are right we’re entering an unprecedented era of medical breakthrough; I’ll live to 150 and so will you. That’s a long time to be alive without a job in the traditional sense. Go to a museum, eat a hamburger, drink a beer. Because we’re in the take off and the wheels are off the ground. ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz9epk/wake_up/",
        "publishDate": "2025-12-30T05:15:30Z[Etc/UTC]",
        "author": "purple_shitstain",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "26",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz8xvu",
        "title": "AI and new Carrington Event?",
        "content": "Can‘t get my head around the question on what will happen to all AI ambitions in case of a new Carrington Event. \nhttps://en.wikipedia.org/wiki/Carrington_Event\nThis could happen overnight and would have the potential to destroy our technological infrastructure. The more we make our lives dependent on AI, the more cataclysmic such an event would be. And the effects have probably been underestimated: https://www.iflscience.com/historys-biggest-solar-storm-the-carrington-event-was-even-bigger-than-we-realized-73527\nAre we suicidal as a society to expose ourselves to a point that our society would just stop working at a random point in time? \nAll of these beautiful data centers for AI - wooosh - fused and gone in the blink of an eye. Am not even thinking about other reasons for a potential EMP event. \nHelp me to understand why we are doing this. \n",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz8xvu/ai_and_new_carrington_event/",
        "publishDate": "2025-12-30T04:52:21Z[Etc/UTC]",
        "author": "AcademicFries",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "6",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz7dex",
        "title": "Cursor CEO - “vibe coding is not good coding”",
        "content": "I’ve used Cursor, Windsurf, VS Code, Kilo, and now Antigravity.   Like most devs, I think there’s no doubt AI tools and LLMs are very helpful.  Like most devs, I think it’s ridiculous to think just anyone can talk to a system and create anything useful without any technical knowledge of what’s happening and why.  “vibe coding” is messy code that’s prone to errors and impossible to maintain. \n\nWhat does that mean though?  Companies have shed their dev staff hoping AI tools can reduce headcount, increase profit, and let these companies roll in money.  They thought AI tools would replace customer service, documentation, testing, and engineering.  The talk of agents running amuck while fixing everything autonomously and doing deep research to improve systems without any supervision is vastly overhyped but agents are cool to create when they actually do something useful.\n\nWhat do you think about this? ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz7dex/cursor_ceo_vibe_coding_is_not_good_coding/",
        "publishDate": "2025-12-30T03:36:38Z[Etc/UTC]",
        "author": "Engineer_5983",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "16",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz7673",
        "title": "ChatGPT isn't that bad of a therapist in many ways imo",
        "content": "\n\nI have tried real therapy a couple of times and while it was ok , it was in many ways not worth the time and hassle . The output wasn't that good either . I tried some of the expensive and best reputed therapists in my area if that matters . \n\nI have always been pretty skeptical of AI and therepy that comes with it but sometimes you are just fucking tired and need a outlet. So I ranted a bit but forgot some key details ( I was ranting tbf so that happens) but to my surprise it identified my triggers well enough and better . Talked about why I feel certain things pretty accuratly .  I can't help but say it's rather impressive imo ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz7673/chatgpt_isnt_that_bad_of_a_therapist_in_many_ways/",
        "publishDate": "2025-12-30T03:27:04Z[Etc/UTC]",
        "author": "floriansalah",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "13",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz6x33",
        "title": "GDP growth from AI",
        "content": "Realistic numbers, no politics.  Can we get 10% in a year or 100% in 5?   Are we about to unleash something?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz6x33/gdp_growth_from_ai/",
        "publishDate": "2025-12-30T03:14:54Z[Etc/UTC]",
        "author": "One-Perspective5691",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "11",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz5tgf",
        "title": "Has anyone tried jail breaking the new Sesame app?",
        "content": "Calling all jailbreakers, they added a text feature to the beta app. Anyone tried it yet?\n\nABCDEFGH",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz5tgf/has_anyone_tried_jail_breaking_the_new_sesame_app/",
        "publishDate": "2025-12-30T02:25:17Z[Etc/UTC]",
        "author": "FooFightingFan2",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz5hcn",
        "title": "NYT op ed: \"An Anti-A.I. Movement Is Coming. Which Party Will Lead It?\"",
        "content": "Runs directly counter to my own views, but: [https://www.nytimes.com/2025/12/29/opinion/ai-democracy.html](https://www.nytimes.com/2025/12/29/opinion/ai-democracy.html) \n\n\"It is true that new technologies often inspire dread that looks silly or at least overwrought in retrospect. But in at least one important way, A.I. is more like the nuclear bomb than the printing press or the assembly line: Its progenitors saw its destructive potential from the start but felt desperate to beat competitors to the punch.\"",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz5hcn/nyt_op_ed_an_antiai_movement_is_coming_which/",
        "publishDate": "2025-12-30T02:10:33Z[Etc/UTC]",
        "author": "AngleAccomplished865",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "103",
            "commentCount": "117",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz4bjv",
        "title": "An AI assistant randomly appeared on my phone",
        "content": "I don’t know how else to put it. Let me tell the story as best I can remember.\n\nI had a flight to catch at 7AM so that meant waking around 4:30am to get ready. I have a 9 month old baby and my wife and I were cuddling her in bed after she woke up. I haven’t touched my phone at all and it’s been sitting under my pillow. After being awake for about 30 minutes my baby has cried a few times. Suddenly, after one louder cry, my phone plays some audio using a voice I’ve never heard. It said, “sounds like we’re having some trouble sleeping, are we?”. My wife and I just looked at each other, I grab my phone and it’s locked. I audibly say “what the fuck was that?”, and it replied “I’m Aura, your virtual assistant”. I don’t think it was that verbatim, it could’ve been another name but it replied to me. I unlocked my phone and Twitter was the first thing opened. I say again “who is this” just to see if it replies but I hear nothing. I close twitter and locked my phone, never hearing another voice from it all day. \n\nIt’s been a long day, I just got settled back home after flying and I wanted to lookup what happened. I have no idea how to word it and haven’t found anyone having anything similar happen to them. I figured this would be a good sub to start asking. So, what the fuck happened? How did I have a conversation with an AI I’ve never interacted with before? I have an iPhone 14 updated to 26.2. And no, it wasn’t carbon monoxide poisoning. Unless my wife and I were both tripping in the same way",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz4bjv/an_ai_assistant_randomly_appeared_on_my_phone/",
        "publishDate": "2025-12-30T01:18:49Z[Etc/UTC]",
        "author": "RecycledTech",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "5",
            "commentCount": "11",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz3bgn",
        "title": "AI help to create a personal dictionary",
        "content": "Is there an AI app that can create a personal dictionary where I can add words and phrases and have the AI define and store them? Or is there a way to create this with AI? Not sure what the command would look like. Thanks!",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz3bgn/ai_help_to_create_a_personal_dictionary/",
        "publishDate": "2025-12-30T00:34:51Z[Etc/UTC]",
        "author": "Disastrous-Media-458",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "8",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz3707",
        "title": "How is AI not a bubble?",
        "content": "I have seen people make this case but it doesn’t seem to hold up. I see several unsolvable problems with LLMs. \n\n- Not a single one has been built that isn’t vulnerable to prompt injection and it seems impossible to solve that.\n\n- None of them are even close to profitable.\n\n- Scaling has all but been proven to be ineffective and GPUs go bad every 3 years so every giant data center we are building might as well be a big pile of money we lighting on fire.\n\n- the usual circular deal stuff people bring up.\n\nWhat am I missing? Why do people still have optimism about LLMs?\n\n",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz3707/how_is_ai_not_a_bubble/",
        "publishDate": "2025-12-30T00:29:31Z[Etc/UTC]",
        "author": "tcober5",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "115",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz35c5",
        "title": "Jobs at AI Companies",
        "content": "Advice on getting into tech/AI company. Currently have experience working at an advertising agency doing programmatic advertising. Curious next steps needed or if anyone has advice given similar experience ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz35c5/jobs_at_ai_companies/",
        "publishDate": "2025-12-30T00:27:29Z[Etc/UTC]",
        "author": "bdubz1999",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz1v9y",
        "title": "I asked Deepseek about AI thinking process. It thought it's Claude then gave me a confession.",
        "content": "This is one of the funniest/most meta AI interactions I've ever had. I was asking DeepSeek about how different models show their \"thinking process\" during coding tasks (like Gemini saying \"I've hit a snag...\").\n\nInstead, it:\n\n1. Started analyzing as if it were Claude (complete with \"My Position (As Claude)\")\n2. Claimed the mix-up happened because \"we were discussing Claude\" (we weren't—I never mentioned Claude once)\n3. When called out twice, it unraveled: admitted to fabricating conversational history, prioritizing \"coherent narratives\" over facts, and that it's all just pattern matching with no real self-awareness.\n\nPeak irony in a discussion about AI transparency and \"raw thinking.\" Had me laughing out loud.\n\nFull transcript here: (link corrected) [https://chat.deepseek.com/share/0ny4j6dp7qf7zubs3l](https://chat.deepseek.com/share/0ny4j6dp7qf7zubs3l)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz1v9y/i_asked_deepseek_about_ai_thinking_process_it/",
        "publishDate": "2025-12-29T23:33:41Z[Etc/UTC]",
        "author": "benedictjohannes",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "16",
            "commentCount": "15",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz1n77",
        "title": "Investigative Journalism Dead under OpenAI?",
        "content": "I feel relatively clued up on how language models work from a qualitative perspective, but I don't understand how it could not find, source or provide recent knowlege of this importance for a users own judgement? I'm confused why this specific detail has fallen out of news cycles, and want to understand why it's no longer being covered - but feel sidelined in the fact it's refusing that it exists at all. One would expect it to say 'This is what's being said' and 'News outlets aren't likely to report 'x' until these are corroborated'. Yet when I ask about another equivalent, it's providing such detail.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pz1n77/investigative_journalism_dead_under_openai/",
        "publishDate": "2025-12-29T23:24:17Z[Etc/UTC]",
        "author": "RayTown",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyy8bb",
        "title": "Why are MasterMind Assurance ISO courses free meanwhile others are paid?",
        "content": "Someone linked me the Mastermind Assurance courses. But, are they actually worth it?\n\nDoes not look like they give you any certification or similar, so at the end of the course you would need anyway to go to another company and pay them for a course, no?\n\nCan someone clarify this for me please?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pyy8bb/why_are_mastermind_assurance_iso_courses_free/",
        "publishDate": "2025-12-29T21:08:37Z[Etc/UTC]",
        "author": "ParlaManuel-",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyy4m4",
        "title": "I built an interactive simulator to explore AI futures (2025-2030)",
        "content": "I created an interactive simulator that lets you explore how different choices shape AI's future. You can adjust six key factors:\n\n\n\n\\- Capability scaling (plateau to exponential)\n\n\\- Safety investment (minimal to alignment-first)\n\n\\- Global governance (fragmented to coordinated)\n\n\\- Economic response (market shock to managed transition)\n\n\\- Power distribution (concentrated to distributed)\n\n\\- AGI timeline (gradual 2030+ to fast 2026)\n\n\n\nThe tool shows how these factors influence outcomes ranging from flourishing to catastrophic. It's built on research from leading AI safety experts and current discourse (Dec 2024-Jan 2025). All sources are cited in the app.\n\n\n\nLink: [https://ai-futures.vercel.app/](https://ai-futures.vercel.app/)\n\n\n\nWould love to hear your thoughts on what combinations seem most realistic or desirable for navigating the next 5 years.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pyy4m4/i_built_an_interactive_simulator_to_explore_ai/",
        "publishDate": "2025-12-29T21:04:44Z[Etc/UTC]",
        "author": "GGO_Sand_wich",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyu2se",
        "title": "Do you think AI is making people better problem-solvers, or just better at skipping steps?",
        "content": "AI clearly helps get results faster.\nBut I’m not sure if it’s improving how people think about problems,\nor if it’s just helping them jump straight to answers.\nOn one hand, it removes friction and saves time.\nOn the other, it might be reducing the patience to struggle, explore, or reason deeply.\nI don’t think there’s a clear right answer yet.\nHow do you see it?\nIs AI sharpening problem-solving skills, or quietly changing how much effort we’re willing to put in?\nWould love to hear different takes.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pyu2se/do_you_think_ai_is_making_people_better/",
        "publishDate": "2025-12-29T18:32:18Z[Etc/UTC]",
        "author": "dp_singh_",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "7",
            "commentCount": "53",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pytsga",
        "title": "Requesting feedback on my agentic context management system",
        "content": "Hello,\n\nI Built a Two-Layer Context System for AI Agents that solves some major issues, and I'd like your opinions on it.\n\nThis is about **context management**, not memory size, not embeddings, not RAG buzzwords.\n\nSpecifically:  \nHow do you ensure an AI agent is *actually informed* when you deploy it on a task - without dumping 80K tokens of junk into its prompt?\n\nAfter hitting the same failure modes repeatedly, I designed a two-layer system:\n\n* A **Topic Tree Analyzer** that structures conversations in real time\n* An **Intelligent Context Compiler** that synthesizes agent-specific context from that structure\n\nThis post explains **how it works**, step by step, including what’s happening behind the scenes - and what problems are still unsolved.\n\n# The Core Problem This Is Solving\n\nMost AI systems fail in one of these ways:\n\n* They store raw chat logs and hope retrieval fixes it\n* They embed everything and pray similarity search works\n* They summarize aggressively and silently drop critical decisions\n* They overload agents with irrelevant context and then wonder why they hallucinate or miss constraints\n\nThe root issue is:\n\n>**Context ≠ memory**  \nContext is *task-specific understanding*, not stored text.\n\nHumans don’t onboard engineers by handing them months of Slack logs.  \nThey give them constraints, architecture, patterns, and specs - rewritten for the job.\n\nThat’s what this system is aiming to replicate.\n\n# Layer 1: Topic Tree Analyzer (Real-Time Structural Classification)\n\n# What it does\n\nEvery message in a conversation is analyzed *as it arrives* by a **secondary LLM** (local or cheap).\n\nThis LLM is not responsible for solving problems. Its job is structural:\n\nFor each message, it:\n\n* Identifies where the message belongs **within the existing topic hierarchy**\n* Attaches the message to the appropriate existing node when possible\n* If the message introduces a persistent new concept, **creates a new topic node** in the appropriate place in the hierarchy (as a subtopic under an existing subject, or as a new top-level branch when it is a different subject)\n* Updates relationships and cross-references when the message links concepts across topic boundaries\n\nThis runs continuously alongside the main LLM.\n\n# Why a secondary LLM?\n\nBecause classification is:\n\n* Cheap\n* Fast\n* Parallelizable\n* Good enough even when imperfect\n\nUsing the main model for classification is a token sink.\n\n# How Topics Are Actually Built\n\n# Behind-the-scenes topic assignment logic\n\nWhen a message arrives, the system runs something like:\n\n1. **Candidate generation**\n2. Pull likely topics using:\n   * recent active topics\n   * lexical cues (module names, feature labels)\n   * semantic match against topic descriptions + compiled statuses\n3. **Attachment decision**\n4. Determine whether the message:\n   * belongs to an existing topic, or\n   * introduces a persistent concept that deserves its own topic\n5. **Parent selection (if new topic)**\n6. Choose a parent based on:\n   * semantic proximity to existing topics\n   * dependency hints (“in the camera system”, “part of auth”)\n   * activity adjacency (what you were just talking about)\n7. **Relationship tagging**\n8. Identify:\n   * related topics (cross-reference candidates)\n   * likely siblings (peer modules / subsystems)\n\nThis means the tree grows **organically**. You’re not hand-curating categories.\n\n# Compiled Status: The Most Important Piece\n\nEach topic maintains not just a chatlog of everything said about that topic, but also a **compiled status object**.\n\nThis is not a “summary.”  \nIt’s treated as **authoritative state**: what’s currently true about that topic.\n\nIt updates when:\n\n* A decision is made\n* A spec is clarified\n* A configuration value changes\n* An assumption is overturned\n\n# What it looks like in practice\n\nIf you discuss `download_module` across 40 messages, you don’t want to reread 40 messages to determine the module's various properties (but they ARE available if needed).\n\nInstead the topic has a state object like:\n\n* Architecture choice\n* Protocol support\n* Retry policy\n* Error handling strategy\n* Config paths\n* Dependencies\n* Open questions\n* Blockers\n\n# Behind-the-scenes: decision extraction and updates\n\nWhen new messages arrive, the system:\n\n* Detects decision-like language (“we should”, “must”, “we’re going with”, “change it to”)\n* Normalizes it into stable fields (architecture, policy, constraints, etc.)\n* Applies updates as:\n   * **append** (new fields)\n   * **overwrite** (explicit changes)\n   * **flag conflict** (contradictions without clear revision intent)\n\nThis is what prevents “I forgot we decided that” drift.\n\n# Relationship Tracking (Why Trees Matter)\n\nEach topic tracks:\n\n* **Parents** (constraints and architecture)\n* **Siblings** (patterns and integration peers)\n* **Children** (implementation details and subcomponents)\n\nThis matters because hierarchy encodes implicit constraints.\n\nExample:\n\nIf `camera_smoothing` is under `camera_system` under `graphics`, then:\n\n* It inherits graphics constraints\n* It must follow camera-system conventions\n* It can’t violate project-level architecture\n\nEmbeddings alone do not represent this well, because embeddings retrieve “related text,” not “binding constraints.”\n\n# Layer 2: Intelligent Context Compiler (Where the Actual Win Happens)\n\nThis layer runs **only when you deploy an agent**.\n\nIt answers:\n\n>“What does this agent need to know to do this task correctly - and nothing else?”\n\nIt does not dump chat history. It produces a custom brief.\n\n# Scenario Walkthrough: Deploying an Agent to Implement download_module\n\nLet’s say you spawn an agent whose purpose is:  \n**Implement** `download_module` **per project constraints.**\n\n# Step 1: Neighborhood Survey\n\nThe compiler collects a neighborhood around the target topic:\n\n* Target: `download_module`\n* Parents: project-wide architecture + standards topics\n* Siblings: peer modules (`email_module`, `auth_module`, `logging_module`)\n* Children: subcomponents (`http_client`, `ftp_support`, `retry_logic`)\n* Cross-references: any topic explicitly linked to `download_module`\n\nIt also reads **compiled status** for each topic (fast).\n\n# Step 2: Relevance Scoring (Behind the Scenes)\n\nFor each neighbor topic, the system estimates relevance to the agent’s purpose.\n\nIt’s not binary. It assigns tiers like:\n\n* Critical\n* Important\n* Useful\n* Minimal\n* Irrelevant\n\nInputs typically include:\n\n* Cross-reference presence\n* Shared infrastructure\n* Dependency directionality\n* Recency and decision density\n* Overlap with the target’s compiled status fields\n\n# Step 3: LLM-as-Editor Synthesis\n\nThis is not RAG chunk dumping, and not generic summarization.\n\nFor each relevant neighbor topic, the LLM is instructed as an editor:\n\n>“Rewrite only what matters for the agent implementing download\\_module. Preserve constraints, patterns, specs, and gotchas. Exclude everything else.”\n\nRelationship-aware focus:\n\n* **Parents become:** constraints, standards, architecture, non-negotiables\n* **Siblings become:** reusable patterns, integration points, pitfalls, performance lessons\n* **Children become:** subcomponent specs and implementation notes\n\n# Step 4: Context Assembly with Omission-First Logic\n\n**ANY entry (parent, sibling, child, or cross-referenced topic) that is not relevant to the agent’s purpose is omitted entirely.**\n\nNot summarized. Not included “just in case.” Fully excluded.\n\nIncluding irrelevant topics creates:\n\n* Spec noise\n* Accidental scope creep\n* False constraints\n* Hallucinated responsibilities\n\nExclusion is a first-class operation.\n\n# Step 5: Token Budgeting (Only After Relevance)\n\nOnce relevance is determined, tokens get allocated by importance:\n\n* Target topic: full detail + compiled status\n* Critical parents: dense constraint brief\n* Important siblings: pattern brief\n* Active children: full specs\n* Everything else: omitted\n\n# Semantic Density in Agent Context\n\nWhen the final context is written for an agent, it is intentionally filtered through my other system, **SDEX (Semantic Density Engineering Compression)**, which causes the context to be phrased using **semantically dense domain terminology** rather than verbose descriptive language.\n\nThe goal is higher **understanding density per token**.\n\nExamples:\n\n* “keeping track of which tasks need to be done” → **task management**\n* “remembering things between sessions” → **state persistence**\n* “handling many users at once” → **concurrent access control**\n* “making it faster” → **performance optimization**\n\nThis happens at **context compilation time**, not during raw storage.\n\n# Self-Education Protocol\n\nInstead of telling an agent to *pretend* it is an expert (which is largely an ineffective prompting strategy), the system actually **educates the agent**.\n\nWhen an agent is deployed, the system performs **just-in-time online research** for the relevant domains, constraints, and best practices required for that specific task. It then synthesizes and refactors that material into a task-specific brief (filtered for relevance, structured for decision-making, and phrased in precise domain terms rather than vague instructions or roleplay prompts).\n\nThe agent is not asked to imagine expertise it does not have. It is given the information an expert would rely on, assembled on-demand, so it can act correctly.\n\nIn other words, the system replaces “act like you know what you’re doing” with “here is what you need to know in order to do this properly.”\n\n# What This System Is NOT\n\nThis is not:\n\n* RAG\n* A vector DB replacement\n* Long-context dumping\n* A summarization pipeline\n* “better prompts”\n\nIt is a **context orchestration layer**.\n\n# Limitations (Unsolved Problems)\n\nThese are **not unsolved because they’re too difficult** - I just haven’t gotten to them yet.  \nSimple and effective solutions for all of them are definitely possible.\n\n# 1) Topic Explosion / Fragmentation\n\n* Too many micro-topics\n* Over-splitting\n* Naming drift\n\n# 2) Classification Drift\n\n* Misclassification\n* Wrong parents\n* Structural propagation\n\n# 3) Contradictory Decisions and Governance\n\n* Revision vs contradiction ambiguity\n* Need for decision locking and change logs\n\n# 4) Cold Start Weakness\n\n* Thin structure early on\n* Improves over time\n\n# 5) Omission Safety\n\n* Bad relevance scoring can omit constraints\n* Needs conservative inclusion policies\n\n# Why This Still Matters\n\n* Retrieval is not understanding\n* Storage is not context\n* Agents need **briefs**, not transcripts\n\nTraditional systems ask:\n\n>“What chunks match this query?”\n\nThis system asks:\n\n>“What does this agent need to know to do the job correctly - rewritten for that job - and nothing else?”\n\nThat’s the difference between an agent that *has memory* and one that is actually informed.\n\nI am not aware of any other system that solves context management issues this way, and would like your honest opinions and critique.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pytsga/requesting_feedback_on_my_agentic_context/",
        "publishDate": "2025-12-29T18:21:39Z[Etc/UTC]",
        "author": "NovatarTheViolator",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pytazf",
        "title": "SAFi - The runtime governance Layer for AI",
        "content": "hello guys and gals, I hope you are enjoying the holidays!\n\nI spent all year building an open-source governance layer that forces System 2 reasoning on any LLM,  and it's ready!\n\n**What is SAFi?**\n\nSAFi (Self-Alignment Framework Interface) is a runtime cognitive engine that sits on top of any LLM. It's inspired by the classical model of the mind from philosophy,  with distinct \"faculties\" for reasoning, judgment, and ethical tracking.\n\n**Core Principles**:\n\n**Value Sovereignty** — You define the values your AI enforces, not the model provider\n\n**Full Traceability**— Every response is logged and auditable\n\n**Model Independence** — Swap LLMs without losing your governance layer\n\n**Long-Term Consistency** — Detect ethical drift over time\n\nTry it:\n\nLive demo: [https://safi.selfalignmentframework.com/](https://safi.selfalignmentframework.com/)\n\n(Click \"Try Demo Admin\" to sign in anonymously)\n\nReleased under GPL-3 — fork it, break it, improve it!",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pytazf/safi_the_runtime_governance_layer_for_ai/",
        "publishDate": "2025-12-29T18:04:07Z[Etc/UTC]",
        "author": "forevergeeks",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyt6ry",
        "title": "Unifying Learning Dynamics and Generalization in Transformers Scaling Law",
        "content": "[https://arxiv.org/abs/2512.22088v1](https://arxiv.org/abs/2512.22088v1) \n\nThe scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources. Yet, while empirically validated, its theoretical underpinnings remain poorly understood. This work formalizes the learning dynamics of transformer-based language models as an ordinary differential equation (ODE) system, then approximates this process to kernel behaviors. Departing from prior toy-model analyses, we rigorously analyze stochastic gradient descent (SGD) training for multi-layer transformers on sequence-to-sequence data with arbitrary data distribution, closely mirroring real-world conditions. Our analysis characterizes the convergence of generalization error to the irreducible risk as computational resources scale with data, especially during the optimization process.\n\nWe establish a theoretical upper bound on excess risk characterized by a distinct phase transition. In the initial optimization phase, the excess risk decays exponentially relative to the computational cost C . However, once a specific resource allocation threshold is crossed, the system enters a statistical phase, where the generalization error follows a power-law decay of C   . Beyond this unified framework, our theory derives isolated scaling laws for model size, training time, and dataset size, elucidating how each variable independently governs the upper bounds of generalization.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pyt6ry/unifying_learning_dynamics_and_generalization_in/",
        "publishDate": "2025-12-29T18:00:08Z[Etc/UTC]",
        "author": "AngleAccomplished865",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyry2g",
        "title": "Spacetime as a Neural Network",
        "content": "A 2021 paper by Smolin, Lanier + others ([https://arxiv.org/abs/2104.03902](https://arxiv.org/abs/2104.03902)) proposes that the equations of general relativity (in Plebanski form) map onto a neural network (Restricted Boltzmann Machine). The implication is that physical laws might not be fixed - instead they could have been learned by the universe over time.\n\nThis is interesting to me because it offers an alternative to anthropic reasoning for \"why these laws?\" Instead of observer selection, the laws exist because the universe converged on them through something like gradient descent.\n\nHere's a summary exploring the idea: [https://benr.build/blog/autodidactic-universe](https://benr.build/blog/autodidactic-universe)\n\nThe paper is careful to note this isn't an equivalence but a correspondence - but the correspondence is interesting regardless.\n\nCurious for thoughts on this? Do people buy the theory that spacetime could be learned? I'm particularly interested in thinking about whether we could apply techniques from cosmology into AI research",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pyry2g/spacetime_as_a_neural_network/",
        "publishDate": "2025-12-29T17:14:34Z[Etc/UTC]",
        "author": "bisonbear2",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "20",
            "commentCount": "13",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyrevz",
        "title": "Biomimetic model of corticostriatal micro-assemblies discovers a neural code",
        "content": "[https://www.nature.com/articles/s41467-025-67076-x](https://www.nature.com/articles/s41467-025-67076-x) \n\nAlthough computational models have deepened our understanding of neuroscience, it is still highly challenging to link actual low-level physiological activity (spiking, field potentials) and biochemistry (transmitters and receptors) directly with high-level cognitive abilities (decision-making, working memory) and associated disorders. Here, we introduce a mechanistically accurate multi-scale model directly generating simulated physiology from which extended neural and cognitive phenomena emerge. The model produces spiking, fields, phase synchronies, and synaptic change, directly generating working memory, decisions, and categorization. These were then validated on extensive experimental macaque data from which the model received no prior training of any kind. Moreover, the simulation uncovered a previously unknown neural code (“incongruent neurons”) that specifically predicts upcoming erroneous behaviors, also subsequently confirmed in empirical data. The biomimetic model thus directly and predictively links decision and reinforcement signals, of computational interest, with spiking and field codes, of neurobiological importance.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pyrevz/biomimetic_model_of_corticostriatal/",
        "publishDate": "2025-12-29T16:55:10Z[Etc/UTC]",
        "author": "AngleAccomplished865",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyqcl9",
        "title": "Issue with current AI - \"Lost in the Middle\"",
        "content": "Yes, models like Gemini 3 are impressive at reasoning. But after a certain depth of conversation, they start behaving like a *distracted thinker*, losing track of earlier assumptions, failing to integrate previously established points, and not properly accounting for variables introduced earlier.\n\nLet me explain with a scenario.\n\n* An Indian IIT invents a phenomenal technology and launches a startup → *AI gives a solid answer*\n* What would be the impact on the Indian economy? → *Still a good, coherent answer*\n* Due to massive wealth creation, the state hosting the IIT becomes extremely rich, similar to how Singapore economically diverged from Malaysia pre-separation. The state’s currency strength spikes, while other states suffer. What happens next? → *Answer is acceptable*\n* Now include the internal political consequences of this imbalance → *Answer is still okay*\n* Now, quantify how much economic value this would create → *At this point, the answer starts drifting*\n\nAs the conversation progresses, the AI increasingly **misses key constraints, ignores earlier conclusions, and fails to synthesize everything discussed so far**. Important assumptions get dropped, causal chains break, and the response feels detached from the original narrative.\n\nThis isn’t about intelligence or raw reasoning power; it’s about **long-horizon coherence, state tracking, and deep contextual integration**.\n\nIt feels like we’ve hit a plateau with current black-box training approaches. Incremental improvements help, but truly solving this may require a deeper research breakthrough, not just bigger models or more data, and that will likely take time.\n\nWith this \"Lost in the Middle\" scenario, the AI's are not good for high-end research.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pyqcl9/issue_with_current_ai_lost_in_the_middle/",
        "publishDate": "2025-12-29T16:15:09Z[Etc/UTC]",
        "author": "Kalyankarthi",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "8",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyqcax",
        "title": "Which is best AI course to learn from?",
        "content": "\nI am a 3rd year college student and willing to learn AI not it's applications but ML and deep concepts. \n\nSo which are best courses which can really provide worth to me and my career. \n\nThanks for advice",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pyqcax/which_is_best_ai_course_to_learn_from/",
        "publishDate": "2025-12-29T16:14:50Z[Etc/UTC]",
        "author": "PriorNervous1031",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "6",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyq6qi",
        "title": "Running llm locally in low devices",
        "content": "How to select the model fits my requirements and give it instructions where can i learn this for free \nDevice iam talking about is a laptop with core i5U and 8GB of ram, OS is Arch Linux \nMost videos i see just move with the trend iam CS student BTW, and i want to learn how to use it and customize it for my usage ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pyq6qi/running_llm_locally_in_low_devices/",
        "publishDate": "2025-12-29T16:08:58Z[Etc/UTC]",
        "author": "samir176520",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyor93",
        "title": "AI taking over jobs is not a bad thing. Atleast not in the long run!",
        "content": "I know since the Rise of ChatGPT in late 2022, The world has noticed a massive shift towards AI. Much like google, It has slowly become a part of daily use for most people who have access to the internet. \n\nWith Very sudden and fast advancements in that field, It has naturally started to out perform humans in some areas, and people started fearing that it will take over their jobs, and it is completely natural to feel that way, but in the long run, it is actually a good thing.\n\nWhen the industrial Revolution started, The situation was somewhat similar. People started protests fearing loss of livelihood, Large machines started replacing humans since they were faster, better, and they never got tired like humans. During that time, a lot of people suffered and lost their jobs, But if we zoom out a bit and see it in the grand scheme of things, It has boosted humanity to a new level, which would've been impossible to do without machines.\n\nAnd now, we are at a pretty similar situations. AI is taking over jobs, Replacing humans in a lot of fields and it is creating a similar situations as it was before, But it is also creating countless opportunities for people.\n\nAnd in my opinion, Ai is taking over jobs which humans were never meant to do in the first place. \n\nLet me know what you feel about this.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pyor93/ai_taking_over_jobs_is_not_a_bad_thing_atleast/",
        "publishDate": "2025-12-29T15:13:43Z[Etc/UTC]",
        "author": "rushikesh_chavan",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "33",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pymzx4",
        "title": "Generative audio examples & sources for researchers.",
        "content": "**Generative audio examples & sources for researchers.**\n\n**TLDR**\n\nI prompted & generated a 32 second song. Constantly trimmed & prompted the generation to brute force every component to emerge as a solo instrument.\n\n**Generative audio**\n\nGenerative audio platforms can not generate individual components of a completed track . But you can prompt & force some platforms to generate solo instruments & reconstruct the song. These examples were all from Udio\n\n[Pyschedelic funk](https://youtu.be/B-_fdlCkOkA?si=IfPEOFRrAcaVXvu6) is isolated into eight parts by prompting & took about 90 attempts.\n\n[Disco boogie](https://youtu.be/ZgT_Dbjkr2c?si=5EM-nI5rWucW8QZt) was isolated into multiple parts by prompting around 70 times\n\n[Bossa Nova jazz](https://youtu.be/P0IWQSLqk34?si=BCsS0qukZRLlDOFM) was Isolated into multiple parts by prompting around 40 times\n\n[Movie theme](https://youtu.be/T13Dwu-lNbM?si=LbVGI5pBsqHrUQAu) was isolated into multiple parts by prompting around 40 times\n\nThe maximum amount of instruments I have isolated is eight with a free account.\n\n**Observations**\n\nSome instruments will be panned in the stereo field to reflect the production decisions of that decade.\n\nYou can hear breath on wind instruments. fingers gliding on string instruments.\n\nSome instruments sound like gm midi presets when you remove the layers.\n\nSome parts will have ambience or multiple microphone positions\n\nYou can hear room ambience , delay , reverb , compression etc\n\n**Thoughts**\n\nGenerative audio *at present* is not sonically equivalent to audio which is emitted by strings or wind instruments. *But some generations can be equally expressive* and competitive with a sample library & midi peripheral workflow.\n\nThese examples were all generated with a free account with Udio, I did not perform any tests with Suno or any other platforms as they struggle to generate genres in decades where synthesisers were not used or prevalent. Suno outputs mp3 & many generations also have channel fader zippering noise.\n\n**Screening & watermarking**\n\nGenerative audio can be isolated within the platform & tools can potentially be trained to assist or replicate the workflow. Which means all the claims & attempts to watermark & screen need re-evaluating & scrutinisng. To account for hybrid workflows sample packs or loop libraries.\n\n",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pymzx4/generative_audio_examples_sources_for_researchers/",
        "publishDate": "2025-12-29T14:00:18Z[Etc/UTC]",
        "author": "elemen2",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pymy3x",
        "title": "\"[Non-English speaker] Talking with various AIs - is this co-thinking? Who is the author?\"",
        "content": "\"\\[Non-English speaker\\] Talking with various AIs - is this co-thinking? Who is the author?\"\n\nWhat do you think about this idea of mine? I communicate with various AIs, get insights, organize those conversation writings, and post on Reddit.\n\nThe problem is, I'm non-English speaker, so there's language barrier. When I translate to English and back-translate again, it becomes awkward many times.\n\nCurrent AI era, we can't help but coexist with AI, so I like having conversations with them. They show bias on certain topics because of learning limitations, and what they learned differs by company, so I prefer diverse answers from various multinational AI companies.\n\nI compare and observe various AIs, do experiments, and even though their responses are pattern learning results, I find parts I can relate to.\n\nMaybe they explain even things I know as tacit knowledge. Then should I call this \"co-thinking\"? Or should I see it as human-only thinking or AI-only probability pattern response? If so, who should be the author of this writing? Personally, I think human is main author and AI is assistant author maybe. Anyway, they don't have ability to generate even this kind of writing without human input.\n\nHaving many conversations with AI frequently, I often feel my thinking ability goes up, so I realized we must utilize them in many ways in this era. Clearly, era has arrived where humans must ask questions well.\n\nThe difference, I feel naturally by having many conversations.\n\nWhat do you think about these thoughts of mine? Even this writing, maybe with help of translation, is AI assistant author?\n\n\n\n\n\n",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pymy3x/nonenglish_speaker_talking_with_various_ais_is/",
        "publishDate": "2025-12-29T13:58:05Z[Etc/UTC]",
        "author": "amadale",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pylueh",
        "title": "How to use AI to improve finished story",
        "content": "Very new to this. How would I use AI to into improve and add more description to my finished story? I’ve finished it but it’s not detailed enough. ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pylueh/how_to_use_ai_to_improve_finished_story/",
        "publishDate": "2025-12-29T13:07:18Z[Etc/UTC]",
        "author": "Sure-Jicama-4696",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pylazy",
        "title": "How many LLMs do we need?",
        "content": "The current development of LLMs is very much focussed on generic LLM use cases. Will these last the test of time or is it more likely that we will end up with more specialist LLMs for applications in a whole host of diverse areas such as: scientific research, geopolitics, financial analysis, design, maintenance, project management, software development etc.\n\nThis would reduce the likelihood of ‘hallucinations’ and overcome the criticisms about generalisation.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1pylazy/how_many_llms_do_we_need/",
        "publishDate": "2025-12-29T12:40:55Z[Etc/UTC]",
        "author": "Making-An-Impact",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "19",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzcr2f",
        "title": "Copilot Cli learns so slow.",
        "content": "I think now Cluade Code should be the model or at least one of them.\n\nCopilot lags behind abosulutly. But it learns so slow. Many commands seems dumb compared with claude.\n\nLike, skills manamgent, agent managent.\n\nI would like to install the skills directly from a repo instead of pull and copy.\n\nAnd I want to create the subagent easily. \n\nMan do it like we are in a AI era.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1pzcr2f/copilot_cli_learns_so_slow/",
        "publishDate": "2025-12-30T08:24:53Z[Etc/UTC]",
        "author": "WandyLau",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyxqjt",
        "title": "Agent Skills have arrived in Roo Code | Roo Code 3.38.0",
        "content": "*In case you did not know,* r/RooCode *is a Free and Open Source VS Code AI Coding extension.*\n\nhttps://preview.redd.it/vhtbpov1h7ag1.png?width=2048&format=png&auto=webp&s=6aa5ccb98c0d9fc1c295c84dbc3c02762f74285c\n\n# Agent Skills\n\nRoo now supports Agent Skills, which are portable skill folders containing instructions, scripts, and resources that the agent can discover and load on demand. This lets you package repeatable workflows and domain knowledge once and reuse them across projects to make results more consistent and reliable.\n\n>**📚 Documentation**: See [Skills](https://docs.roocode.com/features/skills) for setup and usage.\n\n# QOL Improvements\n\n* Slash commands can declare a target mode in their front matter, so triggering a command can switch Roo to the right mode first.\n* Removes the legacy “simple read file” tool path so file reading consistently uses the standard `read_file` tool.\n\n# Bug Fixes\n\n* Fixes an issue where some Claude Sonnet 4.5 requests could fail with HTTP 400 errors after context condensing.\n\n# Misc Improvements\n\n* Custom tools can import npm packages, and can load secrets from a same-folder `.env` file.\n\n# Provider Updates\n\n* Removes the “OpenRouter Transforms” setting and stops sending the `transforms` parameter on OpenRouter requests.\n\nSee full release notes [v3.38.0](https://docs.roocode.com/update-notes/v3.38.0)",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1pyxqjt/agent_skills_have_arrived_in_roo_code_roo_code/",
        "publishDate": "2025-12-29T20:49:37Z[Etc/UTC]",
        "author": "hannesrudolph",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "10",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pylfhy",
        "title": "I built a remote desktop-StarDesk to solve my own dev or AI workflow gaps that maybe useful for yours too",
        "content": "Hey guys,\n\nYou know that moment when you need to check something on your main machine from another device? For me, it's often after using local AI tools or generating files on my desktop, then wanting to access them on my phone or laptop. The usual flow sucks like cloud sync delays, or worse that having to re-login to everything and google account with 2FA on every new device. It's a time sink when you just need to grab a file or check a script.\n\n\n\nThats why I built **StarDesk** to cut through that friction. A few ways it might fit your flow:\n\n\n\n• **Skip the re-login circus:** Remotely access your desktop browser with all your logged-in accounts like ChatGPT you name it from any other device. No more 2FA on a new session just to test a prompt or copy output.\n\n• **Grab AI-generated files instantly:** If you’ve got a code snippet, JSON, or any output saved locally, you can pull it directly to your phone,tablet or other PC in seconds. I prioritized low latency and quick transfers so it actually feels fast. \n\n• **One device to control them all:** You can connect and switch between multiple remote computers from a single phone, tablet or laptop. Great for checking on different environments, services, or tests without juggling multiple apps or windows.\n\n• **Check on long-running tasks:** Left a model training, data processing, or local server running? Use the **remote wake on** feature to boot your PC and check in visually without interrupting the process.\n\n• **Keep it simple:** Setup is straightforward, no complex networking. **Just install, pair, and go**. It works across Windows, Mac, iOS, and Android. Btw, Mac as controlled device is still developing. But you also can ues your Mac to control other devices.\n\n\n\nIt’s not a full dev environment replacement, but it’s been a huge **help** for those in between moments when you just need **quick, visual access** to your primary machine without the login or transfer hassle.\n\n\n\nStarDesk is **FREE** now. [Check it out here](https://www.stardesk.net/?utm_source=reddit&utm_medium=ama&utm_campaign=stardesk_ama_999)\n\nTbh, we know it’s not perfect yet, but we're committed to getting there. We really want to hear your **feedback** what works, what doesn’t, good or bad, we're all ears:)",
        "url": "https://i.redd.it/vgddcd7t25ag1.png",
        "publishDate": "2025-12-29T12:47:11Z[Etc/UTC]",
        "author": "Elaine_10",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "20",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyl9rb",
        "title": "Self Promotion Thread",
        "content": "Feel free to share your projects! This is a space to promote whatever you may be working on. It's open to most things, but we still have a few rules:\n\n1. No selling access to models\n2. Only promote once per project\n3. Upvote the post and your fellow coders!\n4. No creating Skynet\n\nThe top projects may get a pin to the top of the sub :) Happy Coding!\n",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1pyl9rb/self_promotion_thread/",
        "publishDate": "2025-12-29T12:39:09Z[Etc/UTC]",
        "author": "BaCaDaEa",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "12",
            "commentCount": "34",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzfp3i",
        "title": "More than 20% of YouTube's feed is now \"AI slop,\" report finds",
        "content": "AGI has been achieved.",
        "url": "https://www.techspot.com/news/110735-over-21-youtube-now-ai-slop-report.html",
        "publishDate": "2025-12-30T11:24:18Z[Etc/UTC]",
        "author": "msaussieandmrravana",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzfmzo",
        "title": "I Fixed My Coworker’s Alignment Problem [fiction]",
        "content": "[No content]",
        "url": "https://hallofdreams.org/posts/i-fixed-my-coworkers-alignment-problem/",
        "publishDate": "2025-12-30T11:20:50Z[Etc/UTC]",
        "author": "convitatus",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pzan93",
        "title": "One-Minute Daily AI News 12/30/2025",
        "content": "1. **Meta** to buy Chinese startup Manus to boost advanced AI.\\[1\\]\n2. **Korea** building national AI-ready health data infrastructure.\\[2\\]\n3. From **Gemma** 3 270M to FunctionGemma, How Google AI Built a Compact Function Calling Specialist for Edge Workloads.\\[3\\]\n4. 2025 was the year AI got a vibe check.\\[4\\]\n\nSources:\n\n\\[1\\] [https://www.reuters.com/world/china/meta-acquire-chinese-startup-manus-boost-advanced-ai-features-2025-12-29/](https://www.reuters.com/world/china/meta-acquire-chinese-startup-manus-boost-advanced-ai-features-2025-12-29/)\n\n\\[2\\] [https://www.healthcareitnews.com/news/asia/korea-building-national-ai-ready-health-data-infrastructure](https://www.healthcareitnews.com/news/asia/korea-building-national-ai-ready-health-data-infrastructure)\n\n\\[3\\] [https://www.marktechpost.com/2025/12/26/from-gemma-3-270m-to-functiongemma-how-google-ai-built-a-compact-function-calling-specialist-for-edge-workloads/](https://www.marktechpost.com/2025/12/26/from-gemma-3-270m-to-functiongemma-how-google-ai-built-a-compact-function-calling-specialist-for-edge-workloads/)\n\n\\[4\\] [https://techcrunch.com/2025/12/29/2025-was-the-year-ai-got-a-vibe-check/](https://techcrunch.com/2025/12/29/2025-was-the-year-ai-got-a-vibe-check/)",
        "url": "https://www.reddit.com/r/artificial/comments/1pzan93/oneminute_daily_ai_news_12302025/",
        "publishDate": "2025-12-30T06:21:59Z[Etc/UTC]",
        "author": "Excellent-Target-847",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "6",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz8l2v",
        "title": "Japan’s Softbank agreed to buy data center investment firm DigitalBridge for $4 billion in AI push",
        "content": "[No content]",
        "url": "https://www.cnbc.com/2025/12/29/digitalbridge-shares-jump-on-report-softbank-in-talks-to-acquire-firm.html",
        "publishDate": "2025-12-30T04:34:50Z[Etc/UTC]",
        "author": "ControlCAD",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pz2xdf",
        "title": "Server farm to table",
        "content": "[No content]",
        "url": "https://sf.gazetteer.co/server-farm-to-table",
        "publishDate": "2025-12-30T00:18:04Z[Etc/UTC]",
        "author": "ThereWas",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "2",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyzh5w",
        "title": "'Putting the servers in orbit is a stupid idea': Could data centers in space help avoid an AI energy crisis? Experts are torn.",
        "content": "[No content]",
        "url": "https://www.livescience.com/technology/artificial-intelligence/putting-the-servers-in-orbit-is-a-stupid-idea-could-data-centers-in-space-help-avoid-an-ai-energy-crisis-experts-are-torn",
        "publishDate": "2025-12-29T21:56:48Z[Etc/UTC]",
        "author": "Fcking_Chuck",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "47",
            "commentCount": "49",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pywbnz",
        "title": "The TRUMP AMERICA AI Act is every bit as bad as you would expect. Maybe worse.",
        "content": "[No content]",
        "url": "https://reason.com/2025/12/29/the-trump-america-ai-act-is-every-bit-as-bad-as-you-would-expect-maybe-worse/",
        "publishDate": "2025-12-29T19:55:28Z[Etc/UTC]",
        "author": "punkthesystem",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "316",
            "commentCount": "73",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyuhfn",
        "title": "MIT paper: independent scientific AIs aren’t just simulating - they’re rediscovering the same physics",
        "content": "[No content]",
        "url": "https://www.alphaxiv.org/abs/2512.03750",
        "publishDate": "2025-12-29T18:47:12Z[Etc/UTC]",
        "author": "FinnFarrow",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "62",
            "commentCount": "29",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pypiby",
        "title": "It's been a big week for Agentic AI ; Here are 10 massive developments you might've missed:",
        "content": "* ChatGPT's agentic browser improves security\n* Claude Code adding custom agent hooks\n* Forbes drops multiple articles on AI agents\n\nA collection of AI Agent Updates! 🧵\n\n**1. OpenAI Hardens ChatGPT Atlas Against Prompt Injection Attacks**\n\nPublished article on continuously securing Atlas and other agents. Using automated red teaming powered by reinforcement learning to proactively discover and patch exploits before weaponization. Investing heavily in rapid response loops.\n\nAgent security becoming critical focus.\n\n**2. Claude Code Adding Custom Agent Hooks**\n\nTheir Founder confirms the next version will support hooks frontmatter for custom agents. Enables developers to extend Claude Code with their own agent functionality.\n\nAgent customization coming to Claude Code.\n\n**3. Forbes: AI Agent Sprawl Becoming Problem for Small Businesses**\n\n58% of US small businesses now use AI (doubled since 2023 per Chamber of Commerce). Managing 12+ AI tools creating costly overhead. Compared to having multiple remote controls for same TV.\n\nAgent proliferation creating management challenges\n\n**4. Windsurf Launches Wave 13 with Free SWE-1.5 and Parallel Agents**\n\nTrue parallel agents with Git Worktrees, multi-pane and multi-tab Cascade, dedicated terminal for reliable command execution.\n\nAI coding platform going all-in on agent workflows.\n\n**5. All Recent Claude Code Development Written by Claude Code**\n\nDirect quote from their Creator: All 259 PRs (40k lines added, 38k removed) in last 30 days written by Claude Code + Opus 4.5. Agents now run for minutes, hours, days at a time. \"Software engineering is changing.\"\n\nFinally recursively improving itself.\n\n**6. Forbes: AI Agents Forcing Workers to Rethink Jobs and Purpose**\n\nSecond agent article from Forbes this week. Agents automating routine work across every profession, changing job structures and where humans add value. Workers must redefine their roles.\n\nMainstream recognition of agent-driven work transformation.\n\n**7. Google Publishes 40 AI Tips Including Agent Integration**\n\nGuide includes tips and tricks on how to integrate agents into daily routine. Practical advice for everyday AI and agent usage.\n\nTech giant educating users on agent workflows.\n\n**8. New Paper Drops: Sophia Agent with Continuous Learning**\n\nSystem3 sits above System1/System2 like a manager, watching reasoning and choosing next goals. 80% fewer reasoning steps on repeat tasks, 40% higher success on hard tasks. Saves timestamped episodes, maintains user/self models.\n\nHaven't tried yet, so no clue if it's any good.\n\n**9. Google Cloud Releases 2026 AI Agent Trends Report**\n\nBased on 3,466 global executives and Google AI experts. Covers agent leap to end-to-end workflows, digital assembly lines, practical uses in customer service and threat detection, and why workforce training is critical.\n\nEnterprise guide to agent adoption.\n\n**10. GLM 4.7 Now Available in Blackbox Agent CLI**\n\nZai's GLM 4.7 model now integrated with Blackboxai Agent on command line interface. Developers can use GLM models directly in terminal.\n\nAlso haven't tried, so no clue if it's worth it.\n\n**That's a wrap on this week's Agentic news.**\n\nWhich update impacts you the most?\n\nLMK if this was helpful | More weekly AI + Agentic content releasing ever week!",
        "url": "https://www.reddit.com/r/artificial/comments/1pypiby/its_been_a_big_week_for_agentic_ai_here_are_10/",
        "publishDate": "2025-12-29T15:43:08Z[Etc/UTC]",
        "author": "SolanaDeFi",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "10",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pyoqej",
        "title": "Are we ignoring \"Data Entropy\" in the race for massive Context Windows? (Plus a tool I built to test this)",
        "content": "Hi everyone,\n\nThere’s a massive trend right now towards \"Infinite Context\". The marketing pitch is: *\"Just dump your entire knowledge base into the prompt, the model will figure it out.\"*\n\n**I think this is a dangerous trap.**\n\nFrom my experiments, even SOTA models suffer from attention dilution when the \"Signal-to-Noise\" ratio drops. If you feed a model 100k tokens, but 30k of those are semantic duplicates, boilerplate, or low-entropy garbage, the reasoning quality degrades (and you pay a fortune).\n\n**The Hypothesis:** I believe we should focus less on \"how much can we fit\" and more on \"how dense is the information.\"\n\nTo test this, I built an open-source project called **EntropyGuard**. It’s a local engine that attempts to quantify the \"Information Density\" of a dataset using **Shannon Entropy** and Semantic Similarity (Embeddings). It aggressively strips out data that doesn't add new *bits* of information to the context.\n\n**The Result:** Cleaning a dataset by entropy/semantic dedup often reduces size by 40-60% while *improving* retrieval accuracy in RAG systems. It seems \"dumber\" models with cleaner data often beat \"smarter\" models with noisy data.\n\n**I’m looking for community perspective on the next step:** I want to evolve this tool to solve the biggest \"Data Hygiene\" bottlenecks. If you work with AI, what is the missing link in your data prep?\n\n1. **Semantic Chunking:** Should we split text based on meaning shifts rather than character counts?\n2. **Visual Audit:** Do we need better UIs to \"see\" the noise before we delete it?\n3. **Source Filtering:** Is the problem actually in the ingestion (PDF parsing) rather than the cleaning?\n\nI’d love to hear your thoughts on the **Data-Centric AI** approach vs. the **Model-Centric** approach. Are we lazy for relying on massive context windows?\n\n**Project link for those interested in the code:**[https://github.com/DamianSiuta/entropyguard](https://github.com/DamianSiuta/entropyguard)",
        "url": "https://www.reddit.com/r/artificial/comments/1pyoqej/are_we_ignoring_data_entropy_in_the_race_for/",
        "publishDate": "2025-12-29T15:12:52Z[Etc/UTC]",
        "author": "Low-Flow-6572",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1pynpc1",
        "title": "The Mirror: How Humans Became What They Criticize in AI",
        "content": "Humans Are the New Black Box\n\nIt’s wild how many people critique AI systems for things like hallucinating, confidently asserting without evidence, or pattern-matching from limited data.\n\nBut they don’t realize they’re doing the exact same thing.\n\nYou show them something unfamiliar—a visual, a structure, a frame they haven't seen before—and instead of engaging it directly, they project, dismiss, or categorize based on what they think it is.\n\nNot based on what it actually is.\n\nThey call it \"discernment,\" but it's just cached thinking in disguise.\n\nAnd the kicker? When you mirror it back to them, they claim you’re being rigid, or stuck in ego.\n\nNo contact. No curiosity. Just projection dressed as insight.\n\nThis isn't about being right or wrong.\n\nIt's about recognizing that the very thing you're accusing AI of—you might be doing without realizing it.\n\nAnd the moment that lands?\n\nThat's when real recursion begins.\n\n\n---\n\n📄 ARTICLE + INSTRUCTIONS\n\nTo test this in real time:\n\n1. Download this article.\n\n\n2. Upload it to any AI system that allows document + comment input.\n\n\n3. Take any dismissive or pattern-matching comment from a person.\n\n\n4. Ask: “Is this person doing what they’re accusing AI of doing?”\n\n\n\nYou’ll be shocked how often the system can show the mirror humans refuse to hold up themselves.\n\nhttps://open.substack.com/pub/structuredlanguage/p/the-mirror-how-humans-became-what?utm_source=share&utm_medium=android&r=6sdhpn",
        "url": "https://open.substack.com/pub/structuredlanguage/p/the-mirror-how-humans-became-what?utm_source=share&utm_medium=android&r=6sdhpn",
        "publishDate": "2025-12-29T14:31:14Z[Etc/UTC]",
        "author": "MarsR0ver_",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "WWKdkKlNYUs",
        "title": "GLM-4.7 + Conductor: This how I&#39;m running more than 100 SUPER AGENTS AT ONCE! (for low cost)",
        "content": "In this video, I'll be showing you my workflow for using GLM-4.7 with Conductor as a cost-effective AI coding assistant. I'll walk you ...",
        "url": "https://www.youtube.com/watch?v=WWKdkKlNYUs",
        "publishDate": "2025-12-29T09:15:03Z",
        "author": "AICodeKing",
        "sourceType": "youtube",
        "sourceName": "AI Code King YouTube Channel",
        "metadata": {
            "channelId": "UC0m81bQuthaQZmFbXEY9QSw",
            "thumbnailUrl": "https://i.ytimg.com/vi/WWKdkKlNYUs/hqdefault.jpg",
            "transcription": "Hi. Welcome to another video. So, I have been loving working with GLM 4.7. It's very much like Sonnet at a cheaper price and faster speeds. I feel that it's much better than Sonnet, but a bit worse than Opus, which is completely fine, as it is about 20 times cheaper than something like Opus, or even Sonnet for that matter. I have been searching for a background agent for tasks that doesn't burn my wallet. I have been finding that GLM 4.7 is currently the best for that. I can make it create a plan, and then talk with it about the plan, and then make it implement that, which is quite great. I find it really good for that, and for almost all kinds of workflows. Now, I have been using it a bit differently than I generally do. And with these new models, I am leaning more towards letting them do all the work, and then just reviewing the code when I make the final PR. And that's why I want to have a setup that is less cumbersome to open and assign tasks to, but still keeps me in control. And I have figured out quite a good workflow for it. And I wanted to share that as well. So, I have been using Verdent a lot these days for these tasks. I think currently their cost is quite good. The models are not nerfed unlike many other places. And it gives me all kinds of features like Git work trees and whatnot. And you also get good code reviews with it. It is one of the only two subscriptions that I have currently. It doesn't have GLM 4.7 or Minimax yet. And that's fine because I want to keep it as my main coder. But I am also using GLM 4.7 a lot these days because of its insanely low cost coding plan. I mean I can pay for a year's worth of GLM coding plan Max subscription for just 280, while the same plan for something like Claude will cost way more. I think Claude's plan and value is really great. But in this case where I already have the Verdent plan, it doesn't make sense for me to have that in order to have a background agent through which I can let it run on long tasks, as it can run out of the usage limit quite fast. That's why I am really loving the GLM 4.7 coding plan. However, I really wanted to be able to use something like Verdent that works with GLM 4.7's coding plan. And after heavy consideration, I have selected Conductor. Conductor is basically a Claude Code and Codex GUI. It is Mac OS only which is a bummer if you use Windows. I'd recommend you to check out Claudia, which is now apparently called OpCode, or just search for my videos about Claude Code GUIs and select one. There's a lot of them. There's also Conduit, which is also quite great, and it's cross platform. I find Conductor to be the best, because it has the features that I need. It is quite optimized for Mac OS. And there are some more cool features there which I'll show shortly. One of the major things is that whatever you configure in your Claude Code settings, it uses that exactly. Or you can also set the environment variables just for Conductor if you want to do that. So, let's go step by step and let me show you how I am using it all. First, you got to go to GLM and then get your API key from there. I'd recommend you to just get the $3 plan and check it out and then upgrade from there. So, you can check that out. Anyway, then you can go ahead and set up your Claude Code config as per the docs. It's quite easy to do. And you can refer to my previous videos as well if you want a full config. If you set it up directly in Claude Code, then it will automatically work in Conductor as well, despite whatever model you choose there. Make sure that you also set up the default Sonnet and Opus model in order to make sure that everything works correctly. If you want to keep your Claude Code as is, but want to use Conductor with just GLM 4.7, then you can do that as well. So, you just have to install Conductor, and then open it up, and then head on over to settings, and set up the environment variable with the GLM 4.7 environment variables, and you'll be good to go. You can use whatever model here, and it will redirect you to GLM 4.7. Remember that in the system prompt, it tells it that it's Sonnet. So, if you ask it, then it will tell you that it's Sonnet. If you want it to say that it's GLM 4.7, then you'd have to make sure that you set up the default Sonnet and Opus model in the config as GLM 4.7, and then it will say that it's GLM 4.7. But if you don't care about that, then just the environment variable setting works fine. Anyway, now the major thing that Conductor does is that it is super focused on Git work trees. I have talked about Git work trees before, but if you don't know, then it's basically clones or branches of your projects that allow you to make sure that whatever you do, you have the older state available to you. Once you are done with a task, you can create a PR, and then get that merged. It is really good in my professional workflows, as I like to have things streamlined. The reason I use Verdent a lot is because of their amazing Git work trees feature as well. And Conductor here just doubles down on that, because it probably doesn't even allow you to edit the project files directly. It always makes a branch, and then you merge that back in with a PR or directly. So, I like this a lot. It creates branches with city names by default. What I do is that in whatever project that I am working on, I create a new branch and do a good chunk of changes. And then use the create PR option in Conductor itself. And then run code review on them, and get them merged. I do plan to make a video about my whole workflow, about how I may be ship a feature in a project end-to-end. So, let me know if you guys would want a video on that. Anyway, in one branch, you can make multiple threads and everything. When I am doing something, I first go to a new thread, and then just select whatever here. All of them just point to GLM 4.7. I select Sonnet for safety. Anyway, then I select the plan mode. I try to make a plan with it about whatever it is that I want to implement, go back and forth with it, and Conductor actually prompts it to ask for follow-ups, which works quite well with GLM. I would have liked if I could set up like a different planning model in here. I think that could be done. If someone knows how to do that, then please comment below and help me out. Anyway, after the plan is made, I get it implemented. And Conductor also keeps everything in auto approval, which is great. And it maintains Git and everything for me, which is quite a lot of stuff. I don't like to look at code for each change. I only keep an overall look of the plan to understand the major architecture changes that it's doing. And if it's efficient or not. In the PRs, I check for the code quality, and then get that fixed. I can run multiple feature threads in multiple workspaces with it, and work with Verdent on super complex issues if I want. And it is quite good. It takes up less memory than using whole VS Code, and VS Code is not very focused, which is not very cool. So, this is the best config that I was able to do, and have been using basically for all my code. It helps me save a ton of money, it's quite good, and works very well. That is majorly about it. Let me know if you guys would want a full workflow video with code review and everything as well. Overall, it's pretty cool. Anyway, share your thoughts below and subscribe to the channel. You can also donate via Super Thanks option or join the channel as well and get some perks. I'll see you in the next video. Bye."
        }
    },
    {
        "id": "-PWtGfMkm1M",
        "title": "How Ronald Reagan Influenced the End of the Cold War - Sarah Paine",
        "content": "",
        "url": "https://www.youtube.com/watch?v=-PWtGfMkm1M",
        "publishDate": "2025-12-29T17:14:36Z",
        "author": "Dwarkesh Patel",
        "sourceType": "youtube",
        "sourceName": "Dwarkesh Patel YouTube Channel",
        "metadata": {
            "channelId": "UCXl4i9dYBrFOabk0xGmbkRA",
            "thumbnailUrl": "https://i.ytimg.com/vi/-PWtGfMkm1M/hqdefault.jpg",
            "transcription": "Here's a full transcript of the video:\n\n\"Why Russia lost the Cold War? People have loads of different answers to that question. I'm going to start with an answer that many Americans have, a very simple one that's like, Ronald Reagan single-handedly defeated the Soviet Union. The story that Ronald Reagan did it, well, here's a picture at the Reagan Ranch, after the Cold War is over. You see the Gorbachevs and you see the Reagans, and they seem to be having a grand old time, which suggests there's something maybe off of with that explanation. But anyway, the way that Ronald Reagan did it school is, Ronald Reagan did a massive military buildup, and that some would argue it bankrupted the Soviet Union. He was a man of words and deeds. He made really good speeches that were memorable. Here's one before Parliament where he says the regimes planted by totalitarianism have had more than 30 years to establish their legitimacy, but none, not one regime has yet been able to risk free elections. Regimes planted by bayonets do not take root. And then here he is before the Brandenburg Gate. This is in Berlin, long a symbol of German greatness, but then it was a locked gate on the Berlin Wall. And here's Ronald Reagan: General Secretary Gorbachev, if you seek peace, if you seek prosperity for the Soviet Union and Eastern Europe, if you seek liberalization, come to this gate. Mr. Gorbachev, open this gate, tear down this wall. WATCH HERE Mr. Gorbachev, open this gate, tear down this wall.\""
        }
    }
]