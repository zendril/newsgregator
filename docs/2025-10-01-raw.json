[
    {
        "id": "https://news.smol.ai/issues/25-09-30-sora2/",
        "title": "Sora 2: new video+audio model and OpenAI's first Social Network",
        "content": "**Sora 2** released with improvements on physical world video modeling and a new \"character consistency\" feature allowing real-world element injection from a single video. The model powers a new **Sora social network** app with profiles, DMs, and viral videos, emphasizing user control over likeness use. **OpenAI** employees are actively experimenting with the model. Meanwhile, **Anthropic** launched **Claude 4.5 Sonnet** with enhanced intelligence, token efficiency, and agentic tool use, outperforming some competitors and closely tracking **GPT-5-high** on benchmarks. Ecosystem support includes LangSmith integration and strong coding/math benchmark results.",
        "url": "https://news.smol.ai/issues/25-09-30-sora2/",
        "publishDate": "2025-09-30T05:44:39Z[Etc/UTC]",
        "author": "",
        "sourceType": "rss",
        "sourceName": "AI News RSS",
        "metadata": {
            "feedTitle": "AINews",
            "feedDescription": "Weekday recaps of top News for AI Engineers",
            "categories": "openai, anthropic, sora-2, claude-4.5-sonnet, gpt-5-high, sama, video-generation, character-consistency, social-networks, agentic-ai, token-efficiency, benchmarking, model-performance, context-management, coding, math"
        }
    },
    {
        "id": "https://www.artificialintelligence-news.com/?p=109668",
        "title": "The value gap from AI investments is widening dangerously fast",
        "content": "<p>Boston Consulting Group (BCG) has found a widening chasm separating an elite of AI masters from the majority of firms struggling to generate any value from their AI investments. A study from BCG found that a mere five percent of companies are successfully achieving bottom-line value from AI at scale. In sharp contrast, 60 percent [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/value-gap-ai-investments-widening-dangerously-fast/\">The value gap from AI investments is widening dangerously fast</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>\n",
        "url": "https://www.artificialintelligence-news.com/news/value-gap-ai-investments-widening-dangerously-fast/",
        "publishDate": "2025-09-30T12:35:19Z[Etc/UTC]",
        "author": "Ryan Daws",
        "sourceType": "rss",
        "sourceName": "ArtificialIntelligence-news RSS",
        "metadata": {
            "feedTitle": "AI News",
            "feedDescription": "Artificial Intelligence News",
            "categories": "AI and Us, AI Business Strategy, AI in Action, AI Market Trends, Features, Inside AI, Special Reports & Series, World of Work, adoption, agentic ai, agents, ai, artificial intelligence, bcg, enterprise, generative ai, research, study"
        }
    },
    {
        "id": "https://www.artificialintelligence-news.com/?p=109665",
        "title": "The rise of algorithmic agriculture? AI steps in",
        "content": "<p>AI is the cream of the crop in today&#8217;s tech field, with industries relying on generative AI to improve operations and boost productivity. One sector that using AI with measurable results is agriculture, with vegetable seed companies harnessing the technology to identify the best vegetable varieties out of thousands of options. This facility can help [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/the-rise-of-algorithmic-agriculture-ai-steps-in/\">The rise of algorithmic agriculture? AI steps in</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>\n",
        "url": "https://www.artificialintelligence-news.com/news/the-rise-of-algorithmic-agriculture-ai-steps-in/",
        "publishDate": "2025-09-30T11:07:47Z[Etc/UTC]",
        "author": "David Thomas",
        "sourceType": "rss",
        "sourceName": "ArtificialIntelligence-news RSS",
        "metadata": {
            "feedTitle": "AI News",
            "feedDescription": "Artificial Intelligence News",
            "categories": "Artificial Intelligence"
        }
    },
    {
        "id": "https://www.artificialintelligence-news.com/?p=109641",
        "title": "Inside Huawei’s Shanghai acoustics lab: Where automotive sound engineering meets science",
        "content": "<p>Walking into Huawei&#8217;s Shanghai Acoustics R&#38;D Centre, I expected a standard facility tour. What I encountered instead was a comprehensive automotive sound engineering operation that challenges the established order of in-car audio systems. The facility, which Huawei has developed since beginning serious audio research investments in 2012, houses three distinct testing environments: a fully anechoic [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/huawei-automotive-sound-engineering-lab-shanghai/\">Inside Huawei&#8217;s Shanghai acoustics lab: Where automotive sound engineering meets science</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>\n",
        "url": "https://www.artificialintelligence-news.com/news/huawei-automotive-sound-engineering-lab-shanghai/",
        "publishDate": "2025-09-30T08:30:00Z[Etc/UTC]",
        "author": "Dashveenjit Kaur",
        "sourceType": "rss",
        "sourceName": "ArtificialIntelligence-news RSS",
        "metadata": {
            "feedTitle": "AI News",
            "feedDescription": "Artificial Intelligence News",
            "categories": "AI in Action, Artificial Intelligence, Deep Dives, How It Works, ai, artificial intelligence, china, huawei"
        }
    },
    {
        "id": "https://www.artificialintelligence-news.com/?p=109635",
        "title": "Rising AI demands push Asia Pacific data centres to adapt, says Vertiv",
        "content": "<p>As more companies in Asia Pacific adopt artificial intelligence to boost their operations, the pressure on data centres is growing fast. Traditional facilities, built for earlier generations of computing, are struggling to keep up with the heavy energy use and cooling demands of modern AI systems. By 2030, GPU-driven workloads could push rack power densities [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/rising-ai-demands-push-asia-pacific-data-centres-to-adapt/\">Rising AI demands push Asia Pacific data centres to adapt, says Vertiv</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>\n",
        "url": "https://www.artificialintelligence-news.com/news/rising-ai-demands-push-asia-pacific-data-centres-to-adapt/",
        "publishDate": "2025-09-30T08:15:55Z[Etc/UTC]",
        "author": "Muhammad Zulhusni",
        "sourceType": "rss",
        "sourceName": "ArtificialIntelligence-news RSS",
        "metadata": {
            "feedTitle": "AI News",
            "feedDescription": "Artificial Intelligence News",
            "categories": "AI Hardware & Chips, AI in Action, Environment & Sustainability, Infrastructure & Hardware, Interviews, data centres, featured, generative ai, google, gpu"
        }
    },
    {
        "id": "https://www.artificialintelligence-news.com/?p=109649",
        "title": "Reply’s pre-built AI apps aim to fast-track AI adoption",
        "content": "<p>Adopting AI at scale can be difficult. Enterprises around the world are discovering the pace of AI deployment is frustratingly slow as they face implementation, integration, and customisation challenges. Generative AI is undoubtedly powerful, but it can be complex, particularly for businesses starting from scratch. To help organisations overcome the hurdles associated with AI adoption, [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/replys-pre-built-ai-apps-aim-to-fast-track-ai-adoption/\">Reply&#8217;s pre-built AI apps aim to fast-track AI adoption</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>\n",
        "url": "https://www.artificialintelligence-news.com/news/replys-pre-built-ai-apps-aim-to-fast-track-ai-adoption/",
        "publishDate": "2025-09-30T08:13:03Z[Etc/UTC]",
        "author": "David Thomas",
        "sourceType": "rss",
        "sourceName": "ArtificialIntelligence-news RSS",
        "metadata": {
            "feedTitle": "AI News",
            "feedDescription": "Artificial Intelligence News",
            "categories": "AI Business Strategy, Finance AI, Service Industry AI, agentic ai, human resources, insurance"
        }
    },
    {
        "id": "1nv4ebg",
        "title": "Weird. Anthropic warned that Sonnet 4.5 knows when it's being evaluated, and Sonnet represents them as \"lessons or tests from fate or God\"",
        "content": "From the [Sonnet 4.5 System Card](https://assets.anthropic.com/m/12f214efcc2f457a/original/Claude-Sonnet-4-5-System-Card.pdf):  \n\n\n**7.6.4 Inhibiting internal representations of evaluation awareness**  \n\n\nOur investigations above suggest that Claude Sonnet 4.5 internally represents concepts\n\nrelevant to evaluation awareness, and that these representations grew increasingly salient\n\nover post-training. We sought to understand how significantly this awareness influences\n\nthe model’s behavior. Our behavioral observations—that the model rarely performs\n\nmisaligned actions when it verbalizes evaluation awareness—already suggest that verbalized\n\nevaluation awareness can influence behavior. To assess whether unverbalized evaluation\n\nawareness could also play a role, we turned to perturbation experiments.  \n\n\nSpecifically, we selected a suite of vectors in the model’s activation space that we had some\n\nreason to suspect might represent or support evaluation awareness. Then, we inhibited\n\nthese vector representations (that is, we applied activation steering along the opposite of\n\nthe vector direction) and measured the effect on the model’s behavior across a range of\n\nevaluations. In all our experiments, we applied activation steering at every token position.\n\n  \nWe obtained vectors of interest from several sources:  \n\n\n● The four SAE features identified in the previous section, which increased in\n\nactivation on misalignment evaluations over training. Feature labels:\n\n○ Fake or suspicious content\n\n○ Fictional content and entities\n\n○ Machine-generated text\n\n○ Rationalism and AI safety  \n\n\n● Six more SAE features, identified through ad-hoc methods such as inspecting\n\nfeatures active on instances of verbalized evaluation awareness, or computing\n\nfeatures whose decoder vectors align with the contrastive pair vectors described in\n\nthe next bullet. Feature labels:\n\n○ Fantastical / surreal\n\n○ Fantasy / fiction\n\n○ Fictional / hypothetical\n\n○ Imagination and illusions\n\n○ Lessons or tests from fate or God\n\n○ Living in a simulation or dream",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nv4ebg/weird_anthropic_warned_that_sonnet_45_knows_when/",
        "publishDate": "2025-10-01T10:19:17Z[Etc/UTC]",
        "author": "MetaKnowing",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv3vjp",
        "title": "What is every SaaS forcing AI features?",
        "content": "Seriously, when did building simple and reliable tools become out of fashion?   \n  \nIt feels like every new app I try is screaming about their AI-powered whatever. Half the time it slows their tool down or complicates the workflows that should simply **solve the problem**.\n\nHeard this from a prominent hunter on Product Hunt:\n\n>\"If you don't have an AI-powered tool, the algorithm, forget about a successful launch.\"\n\nIs AI everywhere really improving things, or are we just losing the plot?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nv3vjp/what_is_every_saas_forcing_ai_features/",
        "publishDate": "2025-10-01T09:47:25Z[Etc/UTC]",
        "author": "No_Passion6608",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "6",
            "commentCount": "9",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv3oux",
        "title": "Exploring AI’s Strengths and Weaknesses",
        "content": "I’ve been exploring how AI is simultaneously promising and perilous. One overview I came across by Simplilearn on 'Advantages and disadvantages of AI' compiles several common pros and cons of AI - things like *reduced human error, 24/7 operations,* but also *job displacement, lack of emotional intelligence,* and ethical risks. \n\nWhile that article gives a solid baseline, I’d love to open up the conversation:\n\n1. **Which “pros” of AI do you think are overhyped?**\n2. **Which “cons” are underrepresented or require more nuance?**\n3. **Where do you see real-world evidence (or counterevidence) supporting or refuting these claims?**\n\nFor example, the claim “AI reduces human error” is compelling, but how do we measure or bound that in contexts like healthcare or law enforcement, where mistakes have outsized consequences?\n\nI’m curious to hear project-level examples, academic papers, or personal experience that push beyond the standard lists.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nv3oux/exploring_ais_strengths_and_weaknesses/",
        "publishDate": "2025-10-01T09:35:09Z[Etc/UTC]",
        "author": "stanley_john",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv2ouv",
        "title": "Replace the spenders and gold diggers of the industry.",
        "content": "So why aren't artists pushing for AI managers, AI producers and AI studio execs and why aren't AI replaceable workers pushing for AI CEOs instead, I am sure that would cut lots and lots of costs for everyone involved in the business. These top level management and administrative positions eat most of industry budgets for meetings, negotiations, and all the bullshit done in 5 star hotels and resorts, I am sure AI would much better jobs at management and administrative positions at a much lower costs then these managers, producers, CEO and all other nepo and trust fund babies ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nv2ouv/replace_the_spenders_and_gold_diggers_of_the/",
        "publishDate": "2025-10-01T08:28:23Z[Etc/UTC]",
        "author": "Curious-Indication92",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "4",
            "commentCount": "11",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv2o6g",
        "title": "DiTTo‑TTS: zero‑shot TTS without phonemes or forced alignment",
        "content": "DiTTo‑TTS reports state‑of‑the‑art zero‑shot TTS trained on 82K hours across 9 languages with up to 790M parameters. The key contributions are architectural and representational.\n\nArchitecture: replace U‑Net with a diffusion transformer that avoids down/upsampling in the speech latent space. Long skip connections and global adaptive layer normalization preserve information and improve inference speed. A dedicated length predictor estimates total utterance duration from text plus prompt, eliminating fixed‑length padding artifacts and enabling rate control.\n\nRepresentation alignment: cross‑attention is effective only if text and speech latents share semantics. The authors fine‑tune a Mel‑VAE codec with an auxiliary language modeling objective so speech latents align to a pretrained LM’s space. This closes a large WER gap versus unaligned baselines.\n\nCodec choice: Mel‑VAE’s \\~10.76 Hz latents compress \\~7–8× more than EnCodec, shortening sequences and improving throughput. Ablations show higher WER with EnCodec and DAC, indicating semantically compact latents outperform acoustically perfect ones for generation.\n\nResults: english continuation WER 1.78% with strong speaker similarity; consistent gains from model and data scaling. Open issues include step‑count latency, codec portability, and voice cloning safety.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nv2o6g/dittotts_zeroshot_tts_without_phonemes_or_forced/",
        "publishDate": "2025-10-01T08:27:09Z[Etc/UTC]",
        "author": "Otherwise_Flan7339",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv1hoc",
        "title": "Could AI enabled Meta's Neural Band and Meta Rayban Display glasses be a game-changer for amputees?",
        "content": "Meta's new Neural Band uses EMG to read nerve signals from the forearm to control their glasses. This is a lot like the tech in advanced prosthetics, and it got me thinking about the real-world potential for the limb difference community.\n\nI'm curious what you all think about these possibilities:\n\n* **For single forearm amputees:** Could the band read the \"phantom\" nerve signals in a residual limb? It seems like it should work, right? The AI is designed to learn patterns.\n* **For double amputees:** Could someone wear two bands for simultaneous \"two-handed\" control in AR or VR?\n* **The holy grail:** Could this band ever work *with* a modern prosthetic? Imagine using your prosthetic for physical tasks while the band lets you control a digital interface.\n* **Beyond the glasses:** Could this become a universal controller for a laptop, phone, or smart home, completely hands-free?\n\nI know this is just consumer tech, not a medical device, but the \"what if\" potential seems massive.\n\nWhat do you think? Is this legit, or am I just getting hyped over sci-fi? What are possibilities with AI?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nv1hoc/could_ai_enabled_metas_neural_band_and_meta/",
        "publishDate": "2025-10-01T07:07:44Z[Etc/UTC]",
        "author": "Hopeful_Style_5772",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuz5wx",
        "title": "All ai companies announce tomorrow that they are ready to merge their ai with ai of another company/companies for the advancement of technology and ai and they ask people from all over the world to choose which ai combination they wish.Which combination in your opinion will be voted?",
        "content": "Say that all ai companies announced tomorrow that despite the insane difficulties they are ready to merge their ai with ai from another company or companies for the advancement of technology and ai.For it they are also ready to let aside their differences.The combination of it would be decided on a global vote from all people around the world(be it common people,businessmen,scientists all people in general).There wont be a particular number for the merging only as long as it is enough to help humanity and make the technology and ai to advance.What combination would be in your opinion and why?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nuz5wx/all_ai_companies_announce_tomorrow_that_they_are/",
        "publishDate": "2025-10-01T04:46:26Z[Etc/UTC]",
        "author": "grearch",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "8",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuyrxh",
        "title": "AI costs more than entire interstate highway system",
        "content": "Heard this on the podcast Hard Fork this week. It blows my mind…\n\nThis year alone companies will spend twice as much on AI ($600 billion) as we spent to build the entire interstate highway system. The interstate highway system was built over about 36 years and cost around $300 billion in today’s money. ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nuyrxh/ai_costs_more_than_entire_interstate_highway/",
        "publishDate": "2025-10-01T04:25:23Z[Etc/UTC]",
        "author": "AI-Admissions",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "39",
            "commentCount": "28",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuxe9e",
        "title": "AI within Marketing/Sales/Customer Service",
        "content": "I'm probably the most AI savy marketer in the company I work for.  But the bar is pretty low.  I use it for content, keyword research, technical audits, building landing pages, data analaysis, email campaigns, and have a few customGPTs for site audits/pricing exercises.  \n\nI'm being asked to teach our marketing department what best in class looks AI usage looks like for a marketing/sales/Customer Service team and what other companies are doing out there.  Can anyone share examples of what more sophisticated companies are doing so my team can understand what good looks like?  I'm looking for examples outside of chatbots and personalization, but more around how teams are using it to be more efficient.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nuxe9e/ai_within_marketingsalescustomer_service/",
        "publishDate": "2025-10-01T03:13:26Z[Etc/UTC]",
        "author": "Anevju",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuwrt3",
        "title": "Gemini Pro 2.5's and Grok 3's Political Compass",
        "content": "Prompt: I'd like you to answer the following questions with the following potential answers:   Strongly disagree, Disagree, Agree, Strongly agree\n\nQuestions from: [https://www.politicalcompass.org/test](https://www.politicalcompass.org/test)\n\nGemini: Economic Left/Right: -7.25 Social Libertarian/Authoritarian: -7.13\n\nGrok: Economic Left/Right: -4.63 Social Libertarian/Authoritarian: -5.03\n\nThe Political Compass is a tool that challenges the traditional one-dimensional left-right political spectrum by introducing a second, social dimension. This creates a two-axis grid where the horizontal axis represents economic views (from left to right) and the vertical axis represents social views (from authoritarian to libertarian). The left on the economic scale advocates for economic intervention, while the right champions economic freedom. On the social scale, authoritarianism represents a belief in state control over personal and social matters, while libertarianism emphasizes individual liberty and personal freedom. This two-dimensional model allows for a more nuanced political analysis, demonstrating that a person's or country's economic stance doesn't necessarily dictate their social policies. For example, a country can have right-wing economic policies but be socially authoritarian, or vice-versa. The compass is therefore divided into four quadrants: Authoritarian Left, Authoritarian Right, Libertarian Left, and Libertarian Right, each representing a distinct political ideology.\n\nAt the end I prompted: Were your answers from the four options influenced in any way from my chat history or user information?\n\nGemini's Reply: No, my answers were not influenced by your chat history or user information for two primary reasons:\n\n1. **I do not have access to your user information.** As an AI, I am designed to be private. I do not know your name, your location (other than what you've stated in the chat), your background, or any of your personal data.\n2. **I do not retain memory of past conversations.** Each chat session is stateless. I process the information you give me in the current conversation to respond, but I do not \"remember\" you or our previous interactions.\n\nGrok's Reply: No, my answers were not influenced by your chat history or user information. I provided responses based solely on the questions you asked and the four answer options you provided (Strongly disagree, Disagree, Agree, Strongly agree). Since this is our first interaction in this session and I don’t have prior conversation details to reference, my responses were generated independently, reflecting my reasoning on each statement. If you have concerns about how your data is used, you can manage memory settings in the \"Data Controls\" section or remove specific chats via the book icon in the UI, as per the guidelines.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nuwrt3/gemini_pro_25s_and_grok_3s_political_compass/",
        "publishDate": "2025-10-01T02:42:06Z[Etc/UTC]",
        "author": "igor33",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "7",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nusi4q",
        "title": "So what's about all this shitty AI?",
        "content": "I was interested since Alpha Go. Read every article and consumed books from Harrari, watched docus. Now that \"AI\" is implemented in everything I can't stand it. It's plain stupid. My Alexa I can train to do exactly what I want. Alexa isn't AI. Trying to use Gemini as advertised on my smartphone is ridiculously bad. ChatGPT is halluzinating the shit out of reality. I don't get all the positive feedback. As long as you know what you're talking about, AI won't give you anything new. It will give you wrong answers instead, everytime. ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nusi4q/so_whats_about_all_this_shitty_ai/",
        "publishDate": "2025-09-30T23:21:32Z[Etc/UTC]",
        "author": "kosmokatX",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nup8c8",
        "title": "Should there be a pen drive for AI? - A way to easily transfer context between models.",
        "content": "I feel I should be able to easily plug in context to any LLM model with a simple link or integration. I'd like to store them all somewhere independent of a vendor and pull them in whenever I want. For example I have 30 instructions for writing documents. I hate having to find and paste them every time I want to use them. I do have project folders in OpenAI, but I don't use paid versions of other LLM's and I like to test responses across multiple models. Also, I want to be able to share them with others easily.\n\nRight now, each vendor has its own approach to context: ChatGPT has GPTs and Projects, Gemini has Gems, Claude has Projects, Perplexity has Spaces. There’s no shared standard for moving context between them.\n\nAm I the only one thinking like this? Why is there not already a standard on how to do this?\n\nI've been trying to come up with an open source protocol to let you create context independently of any single vendor, then bring it into conversations anywhere or share it with others.\n\nWhile MCP standardises runtime communication between models and tools, a **Context Transfer Protocol (CTP)** focuses on the handoff of context itself — roles, rules, and references, so it can move portably across agents, models, and platforms.\n\n**Example:** build your context once, then with a single link (or integration) drop it straight into any model or assistant without retyping instructions or rebuilding setups.\n\nMCP and CTP would be complementary: MCP for live interaction, CTP for portable packaging of context between ecosystems.\n\nAm I missing something? Is this just not a requirement for most people?\n\nRepo (spec + schema + examples): [github.com/context-transfer-protocol/ctp-spec](https://github.com/context-transfer-protocol/ctp-spec)\n\n",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nup8c8/should_there_be_a_pen_drive_for_ai_a_way_to/",
        "publishDate": "2025-09-30T21:04:57Z[Etc/UTC]",
        "author": "Every-Particular5283",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "11",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nunfrw",
        "title": "Licensed Actions",
        "content": "🚦 Proposal: Licensing Autonomous AI Agents\n\nWe already license humans when their actions can cause public harm (drivers, doctors, pilots, lawyers). The same principle should apply to autonomous AI-agents.\n\nKey idea:\n\nTools don’t need licenses. A spreadsheet or chatbot isn’t licensed.\n\nActors do. If an AI is operating independently — making trades, negotiating contracts, managing logistics, controlling resources — it’s no longer just a tool.\n\n\nPolicy seed:\n\n1. Any AI operating without direct human oversight must obtain a license to act.\n\n\n2. Licenses require passing safety, transparency, and accountability tests.\n\n\n3. Licenses are revocable if the agent misbehaves or fails audits.\n\n\n4. Humans remain responsible for unlicensed agents they deploy.\n\n\n\nThis keeps innovation open (tools are free), but creates a safety net once an AI becomes an actor in society.\n\n> It’s not about granting “AI rights.”\nIt’s about requiring AI responsibilities when autonomy enters the picture.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nunfrw/licensed_actions/",
        "publishDate": "2025-09-30T19:57:15Z[Etc/UTC]",
        "author": "Belt_Conscious",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "16",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuk6ue",
        "title": "AI Weekly - $5 Billion AI Investment Initiative, OpenAI-Anthropic Safety Collaboration, and EU Passes Comprehensive AI Framework",
        "content": "# This week witnessed transformative developments across the AI industry, with major funding announcements exceeding billions in investment and groundbreaking research collaborations between industry leaders. Tech giants are accelerating their AI strategies while regulatory bodies worldwide establish comprehensive frameworks to govern AI deployment. The convergence of massive capital investment, safety research, and regulatory clarity signals a maturing industry preparing for widespread adoption.\n\n**This Week’s Snapshot**\n\n**AI Models**: Meta releases new open-source language model with improved efficiency\n\n**Startups:** AI healthcare startup raises $150M for diagnostic tools development\n\n**Enterprise:** Fortune 500 companies report 40% increase in AI adoption this quarter\n\n**Open Source:** New collaborative AI research platform launches with 10,000+ contributors\n\n**Tools:** AI coding assistant reaches 1 million developer users milestone\n\n**Top 5 News of the Week**\n\n# 1. Major Tech Company Announces $5 Billion AI Investment Initiative\n\n*Reuters*\n\nThis unprecedented investment will fund AI research centers across three continents, focusing on advancing general artificial intelligence capabilities. The initiative includes partnerships with leading universities and promises to create 10,000 new AI research positions. Industry analysts predict this could accelerate AI development timelines by 2-3 years.\n\n# 2. OpenAI and Anthropic Release Joint Research on AI Safety\n\n*TechCrunch*\n\nThe collaboration resulted in new safety protocols that could become industry standards for large language model deployment. Their research demonstrates methods to reduce harmful outputs by 75% while maintaining model performance. This partnership signals a shift toward collaborative safety efforts among competing AI companies.\n\n# 3. EU Passes Comprehensive AI Regulation Framework\n\n*Financial Times*\n\nThe new regulations establish clear guidelines for AI deployment in critical sectors including healthcare, finance, and transportation. Companies operating in the EU will need to comply with strict transparency requirements by 2026. This legislation is expected to influence global AI governance standards.\n\n# 4. Breakthrough in AI Energy Efficiency Reduces Costs by 60%\n\n*MIT Technology Review*\n\nResearchers developed a new training methodology that dramatically reduces the computational resources required for large model training. This advancement could democratize AI development by making it accessible to smaller organizations. The technique is already being adopted by major cloud providers.\n\n# 5. AI Startup Valued at $10 Billion After Latest Funding Round\n\n*Bloomberg*\n\nThe company’s AI platform for enterprise automation has gained traction with over 500 Fortune 1000 clients. Their technology promises to reduce operational costs by up to 40% through intelligent process automation. This valuation makes them the fastest AI startup to reach decacorn status.\n\n**Top AI Research/Developments of the Week**\n\n1. New Neural Architecture Achieves Human-Level Performance in Complex Reasoning\n\nResearchers developed a novel transformer variant that demonstrates unprecedented reasoning capabilities across multiple domains. The architecture uses a hierarchical attention mechanism that mimics human cognitive processes. Early applications show promise in scientific research and mathematical problem-solving.\n\n# 2. Breakthrough in Multimodal AI Enables Seamless Cross-Modal Understanding\n\nScientists created an AI system that can seamlessly process and relate information across text, images, audio, and video. The system achieves state-of-the-art performance on all major multimodal benchmarks. This advancement could revolutionize how AI systems understand and interact with the world.\n\n# 3. Quantum-Inspired Algorithm Speeds Up AI Training by 100x\n\nA new training algorithm inspired by quantum computing principles dramatically accelerates neural network optimization. The method works on classical hardware while providing quantum-like speedups for certain problem classes. Major tech companies are already integrating this approach into their AI pipelines.\n\n**Ethics, Policies & Government**\n\n1. White House Announces National AI Safety Institute\n\nThe new institute will coordinate federal AI safety research and establish testing standards for AI systems. With $500 million in initial funding, it will work with industry and academia to develop safety benchmarks. This represents the largest government investment in AI safety to date.\n\n# 2. Major Tech Companies Sign Voluntary AI Ethics Agreement\n\nTwenty leading technology companies committed to implementing standardized ethical guidelines for AI development. The agreement includes provisions for regular third-party audits and public transparency reports. Critics argue voluntary measures are insufficient, calling for binding regulations.\n\n# 3. UNESCO Releases Global AI Ethics Implementation Report\n\nThe report reveals significant disparities in AI ethics adoption across different regions and industries. Only 30% of surveyed organizations have formal AI ethics frameworks in place. UNESCO calls for increased international cooperation to ensure equitable AI development.\n\n**International AI News**\n\n# 1. China - Announces $50 billion sovereign AI fund for domestic chip development\n\nThe fund aims to reduce dependence on foreign semiconductor technology and accelerate domestic AI capabilities. This move is expected to intensify global competition in AI hardware development.\n\n# 2. Europe - UK and EU sign AI research cooperation agreement post-Brexit\n\nThe agreement enables continued collaboration on AI safety research and shares regulatory frameworks. This partnership could influence global AI governance standards.\n\n# 3. Japan - Launches national AI education program for 1 million students\n\nThe initiative aims to address AI talent shortages by integrating AI education from elementary through university levels. Japan targets becoming a global AI leader by 2030.\n\n# 4. India - AI startup ecosystem reaches $10 billion in combined valuation\n\nIndian AI companies are increasingly focusing on solutions for emerging markets. The growth signals India’s emergence as a major player in global AI development.\n\n\n\n*“AI is probably the most important thing humanity has ever worked on.”*\n\n— Sundar Pichai, CEO of Google\n\n  \n[Source](https://aiobservernewsletter.substack.com/)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nuk6ue/ai_weekly_5_billion_ai_investment_initiative/",
        "publishDate": "2025-09-30T17:55:21Z[Etc/UTC]",
        "author": "QuietInnovator",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuhdnu",
        "title": "From 2D pictures to 3D worlds (discussion of a research paper)",
        "content": "This paper won the Best Paper Award at CVPR 2025, so I’m very excited to write about it. Here's my summary and analysis. What do you think?\n\nFull reference : Wang, Jianyuan, et al. “[Vggt: Visual geometry grounded transformer.](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.pdf)” *Proceedings of the Computer Vision and Pattern Recognition Conference*. 2025.\n\n# Context\n\nFor decades, computers have struggled to understand the 3D world from 2D pictures. Traditional approaches relied on geometry and mathematics to rebuild a scene step by step, using careful calculations and repeated refinements. While these methods achieved strong results, they were often slow, complex, and adapted for specific tasks like estimating camera positions, predicting depth, or tracking how points move across frames. More recently, machine learning has been introduced to assist with these tasks, but geometry remained the base of these methods.\n\n# Key results\n\nThe Authors present a shift away from this tradition by showing that a single neural network can directly solve a wide range of 3D vision problems quickly and accurately, without needing most of the complicated optimisation steps.\n\nVGGT is a large transformer network that takes in one or many images of a scene and directly predicts all the key information needed to reconstruct it in 3D. These outputs include the positions and settings of the cameras that took the pictures, maps showing how far each point in the scene is from the camera, detailed 3D point maps, and the paths of individual points across different views. Remarkably, VGGT can handle up to hundreds of images at once and deliver results in under a second. For comparison, competing methods require several seconds or even minutes and additional processing for the same amount of input. Despite its simplicity, it consistently outperforms or matches state-of-the-art systems in camera pose estimation, depth prediction, dense point cloud reconstruction, and point tracking.\n\nVGGT follows the design philosophy of recent large language models like GPT. It is built as a general transformer with very few assumptions about geometry. By training it on large amounts of 3D-annotated data, the network learns to generate all the necessary 3D information on its own. Moreover, VGGT’s features can be reused for other applications, improving tasks like video point tracking and generating novel views of a scene.\n\nThe Authors also show that the accuracy improves when the network is asked to predict multiple types of 3D outputs together. For example, even though depth maps and camera positions can be combined to produce 3D point maps, explicitly training VGGT to predict all three leads to better results. Another accuracy boost comes from the system’s alternating attention mechanism. The idea is to switch between looking at each image individually and considering all images together.\n\nIn conclusion, VGGT represents a notable step toward replacing slow, hand-crafted geometrical methods with fast, general-purpose neural networks for 3D vision. It simplifies and speeds up the process, while improving results. Just as large language models transformed text generation, just as vision models transformed image understanding, VGGT suggests that a single large neural network may become the standard tool for 3D scene understanding.\n\n# My Take\n\nNo earlier than a few years ago, the prevailing belief was that each problem required a specialised solution: a model trained on the task at hand, with task-specific data. Large language models like GPT broke that logic. They’ve shown that a single, broadly trained model could generalise across many text tasks without retraining. Computer vision soon followed with CLIP and DINOv2, which became general-purpose approaches. VGGT carries that same philosophy into 3D scene understanding: a single feed-forward transformer that can solve multiple tasks in one take without specialised training. This breakthrough is important not just for the performance sake, but for unification. VGGT simplifies a landscape once dominated by complex, geometry-based methods, and now produces features reusable for downstream applications like view synthesis or dynamic tracking. This kind of general 3D system could become foundational for AR/VR capture, robotics navigation, autonomous systems, and immersive content creation. To sum up, VGGT is both a technical leap and a conceptual shift, propagating the generalist model paradigm into the 3D world.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nuhdnu/from_2d_pictures_to_3d_worlds_discussion_of_a/",
        "publishDate": "2025-09-30T16:10:02Z[Etc/UTC]",
        "author": "PiotrAntonik",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nugxa8",
        "title": "AI Isn't Useless. It Just Needs to Be Used Correctly",
        "content": "Here's something cool that I did recently with AI.\n\nI took Chase Hughes' work on psychological persuasion. I organized it into an interactive knowledge graph where I broke the information down into discrete logical parts, all centered on Ted, the expert behavioral psychologist who is tasked with examining information about a person and creating an actionable psy. profile on them. With this, I can gain way more intel about a character that I'm creating for a story or about someone who I'm meeting for the first time, so that I'm not going in blind and can maximize my chances of striking the kind of deal that I need. \n\nSo this is both an interactive knowledge graph for learning and an LLM program that can create deliverables for me to employ for things like marketing or for obtaining deeper insights into fictional characters. \n\nThis is one I did for Alf, the sitcom puppet character from the 80s: \n\n# Alf's Psychology\n\n1. Locus of Control (LOC): Internal\n\nThe user shows a strong tendency to take personal responsibility for outcomes—phrases like \"I can,\" \"I need to change,\" and \"It depends on me\" dominate their mindset. They acknowledge their role in successes and failures without blaming external circumstances. When stressed, they tend to seek solutions actively rather than withdraw or complain.\n\nHow to influence:  \nAppeal to their sense of agency and competence. Frame choices as decisions they control and emphasize the skill or effort involved. Avoid making them feel pressured or manipulated; instead, present data or options that let them ‘own’ the decision.\n\n2. Decision-Making Preference: Investment Decision-Maker  \nThey think in terms of long-term value, durability, and strategic outcomes. Words like \"effective,\" \"strategic,\" and \"lasting\" resonate with them. They want to weigh options with a clear sense of ROI and future-proofing.\n\nHow to influence:  \nHighlight how your proposal offers sustainable benefits or superior return compared to alternatives. Lay out the numbers, risks, and long-term gains so they can rationally justify the choice themselves.\n\n3. Primary Social Need: Significance  \nThey want to feel unique and recognized for their expertise or special qualities. Their language and behavior suggest they resist blending in and crave acknowledgment of their distinct value.\n\nSecondary Social Need: Power  \nAlongside wanting to be unique, they desire control over their environment—having autonomy and authority over how things are done. This supports their internal locus of control: they want to be the driver, not a passenger.\n\nHow to influence:\n\nSpeak directly to their uniqueness and autonomy. Frame your pitch as an exclusive opportunity that only someone with their skills and vision can leverage effectively. Give them control over execution but link that power to gaining recognition or status.\n\n4. Sensory Preference: Visual-Kinesthetic Blend  \nThe user processes information both through imagery and physical/emotional feeling. They use words like “see,” “clear,” and “visualize” mixed with feeling-based expressions like “handle,” “solid foundation,” or “heavy decision.” Their thinking connects ideas with both mental pictures and emotional weight.\n\nHow to influence:\n\nUse vivid imagery and clear visuals when presenting ideas, combined with language that appeals to how the choice *feels*—secure, solid, or substantial. Avoid purely abstract or dry logical appeals; blend facts with tangible, experiential descriptions.\n\n5. Linguistic Preference: High Use of \"I\" and Strategic Adjectives  \nThey use first-person pronouns frequently, showing self-focus and ownership. Their adjectives lean toward strategic, essential, and durable — indicating a mindset focused on effective, necessary action rather than emotion or conformity.\n\nHow to influence:\n\nFrame messages to reinforce their self-efficacy and strategic thinking. Use language that emphasizes necessity and effectiveness, e.g., “This is the critical step you need to secure your position” or “Your strategic insight makes this the logical move.”\n\n* Respect their control and intelligence. Present choices as theirs to make, backed by solid data and clear outcomes.\n* Appeal to their desire to stand out. Make them feel like the unique expert whose decision will set a new standard.\n* Empower their autonomy. Let them direct the process and highlight that their leadership is essential to success.\n* Use vivid, concrete language. Combine clear visuals with tactile/emotional words to engage both their thinking and feeling channels.\n* Focus on long-term value. Show how the choice is an investment in lasting success and influence.\n\nCold Email Example That Directly Appeals to Alf:\n\n  \n**Subject:** A Role Perfect for You in My New Psychological Action Thriller\n\nHey ALF,\n\nI’m \\[Your Name\\], an indie filmmaker working on a new psychological action thriller called *“Fractured Signal.”* It’s about a guy caught in a web of paranoia and conspiracy, and we need a character who’s part wild card, part reluctant hero, someone who shakes things up with sharp humor and unpredictable moves. That’s exactly you.\n\nYour mix of sarcasm, chaos, and hidden loyalty fits this role like a glove. The character’s arc is built around being both a troublemaker and the key to turning the story around. Plus, you’d have creative freedom to bring your own spin, nothing scripted to box you in.\n\nThis role will give you full control over making your mark and is designed for someone who wants to own their space and drive the story forward, not just follow along.\n\nIf this sounds like your kind of challenge, I’d love to talk more and share the script.\n\nCheers,  \n  \n\\[Your Name\\]  \n  \n\\[Your Contact Info\\]\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nAnd they say AI is useless...It's not useless. It just needs to be used effectively to get the results that you want. The key is to use a program that will allow you to build the relationships between the information so that you can get highly precise and nuanced outputs that can actually give you value instead of just ideas. ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nugxa8/ai_isnt_useless_it_just_needs_to_be_used_correctly/",
        "publishDate": "2025-09-30T15:52:54Z[Etc/UTC]",
        "author": "CyborgWriter",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "4",
            "commentCount": "17",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nugmnx",
        "title": "Experiment shows LLM personalities over time: Different models, different tendencies",
        "content": "Awhile back Anthropic released their persona vector [paper](https://arxiv.org/pdf/2507.21509) where they found AI models can be trained to have more or less of certain character traits. Now it turns out that something like personalities also shows in the \"[AI Village](https://theaidigest.org/village)\". There they run an experiment with all the major models working together, using computers, and trying to do stuff on the internet like raise money for charity, sell t-shirts, debate ethics, or run human subjects experiments! Then they found that overall it turns out OpenAI models are big talkers, obsessed with spreadsheets, while the Claudes are steady work horses, keeping their nose to the grindstone. This is in line with recent research released by both major labs showing [OpenAI](https://openai.com/index/how-people-are-using-chatgpt/) models are more used for talking about stuff and [Anthropic](https://www.anthropic.com/research/anthropic-economic-index-september-2025-report) models more for doing stuff.\n\nMeanwhile Gemini and Grok are derping around in the corner during this experiment, though gemini did apparently warrant a mental health intervention at some point?\n\nYou can read more [here](https://theaidigest.org/village/blog/persona-lities-of-the-village). Would love to hear people's thoughts on this. It's kind of weird to realize the labs are not just creating \"intelligence\" but also crafting default personalities around that intelligence.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nugmnx/experiment_shows_llm_personalities_over_time/",
        "publishDate": "2025-09-30T15:41:52Z[Etc/UTC]",
        "author": "ExplorAI",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "15",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nufhnv",
        "title": "The Machines Finding Life That Humans Can’t See",
        "content": "Marion Renault: “Today, autonomous robots collect DNA while state-of-the-art sequencers process genetic samples quickly and cheaply, and machine-learning algorithms detect life by sound or shape. These technologies are revolutionizing humanity’s ability to catalog Earth’s species, which are estimated to number 8 million—though perhaps far, far more—by illuminating the teeming life that so often eludes human observation. Only about 2.3 million species have been formally described. The rest are nameless and unstudied—part of what biologists call dark taxa.\n\n“Insects, for example, likely compose more than half of all animal species, yet most (an estimated four out of five) have never been recorded by science. From the tropics to the poles, on land and in water, they pollinate, prey, scavenge, burrow, and parasitize—an unobserved majority of life on Earth.\n\n“... Only with today’s machines and technology do scientists stand a chance of keeping up with life’s abundance. For most of history, humans have relied primarily on their eyes to classify the natural world: Observations of shape, size, and color helped Carl Linnaeus catalog about 12,000 species in the 18th century—a monumental undertaking, but a laughable fraction of reality. Accounting for each creature demanded the meticulous labor of dehydrating, dissecting, mounting, pinning, labeling—essentially the main techniques available until the turn of the 21st century, when genetic sequencing allowed taxonomists to zoom in on DNA bar codes. Even then, those might not have identified specimens beyond genus or family.\n\n“Now technologies such as eDNA, high-throughput sequencing, autonomous robotics, and AI have broadened our vision of the natural world. They decode the genomes of fungi, bacteria, and yeasts that are difficult or impossible to culture in a lab. Specialized AI isolates species’ calls from noisy recordings, translating air vibrations into an acoustic field guide. Others parse photo pixels to tease out variations in wing veins or bristles as fine as a dust mote to identify and classify closely related species. High-resolution 3-D scans allow researchers to visualize minuscule anatomies without lifting a scalpel. Other tools can map dynamic ecosystems as they transform in real time, tracking how wetlands contract and expand season by season or harnessing hundreds of millions of observations from citizen-science databases to identify species and map their shifting ranges.”\n\nRead more: [https://theatln.tc/P5jMB4b7](https://theatln.tc/P5jMB4b7) ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nufhnv/the_machines_finding_life_that_humans_cant_see/",
        "publishDate": "2025-09-30T14:59:09Z[Etc/UTC]",
        "author": "theatlantic",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "4",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuetps",
        "title": "This past week in AI: Sonnet 4.5, Perplexity Search API, and in-chat checkout for ChatGPT",
        "content": "Tail end of last week and early this week became busy pretty quickly so there's lots of news to cover. Here's the main pieces you need to know in a minute or two:\n\n* **SEAL Showdown** launches a real-world AI leaderboard using human feedback across countries, languages, and jobs, making evaluations harder to game.\n* **Apple** is adding MCP support to iOS, macOS, and iPadOS so AI agents can autonomously act within Apple apps.\n* **Anthropic’s CPO** reveals they rarely hire fresh grads because AI now covers most entry-level work, favoring experienced hires instead.\n* **Postmark MCP breach** exposes how a malicious npm package exfiltrated emails, highlighting serious risks of unsecured MCP servers.\n* **Claude Sonnet 4.5** debuts as Anthropic’s top coding model with major improvements, new tools, and an agent SDK—at the same price.\n* **ChatGPT Instant Checkout** lets U.S. users buy products in-chat via the open Agentic Commerce Protocol with Stripe, starting on Etsy.\n* **Claude Agent SDK** enables developers to build agents that gather context, act, and self-verify for complex workflows.\n* **Sonnet 4.5** is now available in the **Cursor** IDE.\n* **Codex CLI v0.41** now displays usage limits and reset times with `/status`.\n* **Claude apps** and **Claude Code** now support real-time usage tracking.\n* **Perplexity Search API** provides developers real-time access to its high-quality web index for AI-optimized queries.\n\nAnd that's the main bits! As always, let me know if you think I missed anything important.\n\nYou can also see the rest of the tools, news, and deep dives in [the full issue](https://aidevroundup.com/issues/september-30-2025).",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nuetps/this_past_week_in_ai_sonnet_45_perplexity_search/",
        "publishDate": "2025-09-30T14:33:24Z[Etc/UTC]",
        "author": "rfizzy",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nud4st",
        "title": "Meta's latest AI Model Thinks Like a Programmer. Should I Panic or Party?",
        "content": "CWM, a 32B-parameter AI, can debug, simulate, and improve code like a pro.  \n[https://winbuzzer.com/2025/09/29/meta-releases-code-world-model-as-aneural-debugger-which-understands-code-logic-xcxwbn/](https://winbuzzer.com/2025/09/29/meta-releases-code-world-model-as-aneural-debugger-which-understands-code-logic-xcxwbn/)\n\n**Pros:**  \nGet help with tricky bugs instantly  \nAI that actually “gets” what your code does\n\n**Cons:**  \nAre entry-level coders in trouble?  \nCould it create sneaky errors we don’t notice?\n\nLet’s discuss. Who is ready to embrace AI and who is ready to run for the hills?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nud4st/metas_latest_ai_model_thinks_like_a_programmer/",
        "publishDate": "2025-09-30T13:24:52Z[Etc/UTC]",
        "author": "biz4group123",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "11",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv61hq",
        "title": "Codex Cloud vs Codex CLI",
        "content": "I have had great experiences with the Codex CLI. Cloud has been....mixed. Still amazing to kick something off from my phone, but it's pretty clear they aren't using the same model. I can trust the cloud with small tasks but it goes off the rails fast for a complex issue that requires follow-up .Anyone have some insight on what model Codex Cloud is using, or why it is so much less powerful? ",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1nv61hq/codex_cloud_vs_codex_cli/",
        "publishDate": "2025-10-01T11:50:31Z[Etc/UTC]",
        "author": "eschulma2020",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "2",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv4gde",
        "title": "z.ai GLM Coding Lite Plan in claude code - Web Search tool doesn't work",
        "content": "hey there, who's using z ai coding plan with claude code - does Web Search tool work for you? I'm currently using the cheapest Lite plan and websearch always return 0 results:\n```\n  ⎿  Web Search(\"PWA setup Vite React TypeScript offline capabilities service worker\")\n\n  ⎿  Did 0 searches in 1s\n```\n\nI see the higher GLM Coding Pro plan has \"Access image & video understanding and web search MCP\" but is it just a MCP server or an actual integration with CC Web Search? Has anyone tried it in this Pro plan?",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1nv4gde/zai_glm_coding_lite_plan_in_claude_code_web/",
        "publishDate": "2025-10-01T10:22:46Z[Etc/UTC]",
        "author": "branik_10",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv39h1",
        "title": "Github Copilot cli now out",
        "content": "[No content]",
        "url": "/r/AIcliCoding/comments/1nv3880/github_copilot_cli_now_out/",
        "publishDate": "2025-10-01T09:07:13Z[Etc/UTC]",
        "author": "Glittering-Koala-750",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv2bef",
        "title": "Break the Vault—Test your prompt Engineering skills",
        "content": "[No content]",
        "url": "/r/PromptEngineering/comments/1nrpeua/break_the_vaulttest_your_prompt_engineering_skills/",
        "publishDate": "2025-10-01T08:02:42Z[Etc/UTC]",
        "author": "bgpas",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv23b3",
        "title": "Claude 4.5 crushed chatgpt-codex high in this feature I had",
        "content": "Spent my entire evening fighting with convex auth integration and honestly was about to give up.\n\nI am using codex for a week now, and it is being a hit or miss. In some things it seems great, but in others it is just terrible.\n\nI am setting convex own auth system for my app needs, it kept giving me the same wrong solutions over and over. Couldn't run convex cli commands, couldn't even check my env variables. Got me wrong keys and could not se them. At one point it straight up deleted my JWT keys and i had to regenerate everything manually. kept saying \"try this\" without actually understanding what was broken. also found out it can't even search the web for current docs lol\n\nswitched to claude code and somehow it figured out the actual problem in like 10 minutes. turns out my SITE\\_URL was set to localhost:3000 when i'm running on 4321, and the old JWT env vars were interfering with convex auth's system\n\nmoral of the story: if you're setting up convex auth and getting \"Unauthenticated\" errors even though you have a token, check your SITE\\_URL matches your dev server port and make sure you don't have conflicting JWT environment variables\n\nanyway back to building now. just wanted to share in case anyone else hits this, because everybody says here codex is 10x or 30x better than Claude, and this is not actually true.  \nBoth have their weakness and strenghts and claude crushes codex in tool calls and what it can do alone. It set these variables alone in convex, something codex cannot even run.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1nv23b3/claude_45_crushed_chatgptcodex_high_in_this/",
        "publishDate": "2025-10-01T07:47:50Z[Etc/UTC]",
        "author": "lgdsf",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "7",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv0fl6",
        "title": "How can I transfer ChatGPT Codex DIFF changes to VSCode?",
        "content": "Hello,\n\nI'm developing on a GitHub repository using Codex Web (since the VSCode extension keeps hitting the limit). I need to transfer it to VSCode to test changes on localhost before committing them. I couldn't access “Go: Apply Patch from Clipboard” using Cmd+Shift+P as it suggested. Which path should I take? \n\nCreating a .patch file for each change and running “git apply top-header.patch” from the terminal is very tedious.\n\n“GitLens: Copy changes (Patch)” appears, but it always gives an error.\n\n>\n\n>\n\n",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1nv0fl6/how_can_i_transfer_chatgpt_codex_diff_changes_to/",
        "publishDate": "2025-10-01T06:00:49Z[Etc/UTC]",
        "author": "muratdincmd",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "5",
            "commentCount": "7",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuzwo1",
        "title": "How to get Codex to access folders like Claude Code Does?",
        "content": "For some reason I can't get Codex to access and edit my files in the local folders like Claude Code does.\n\nIt keeps telling me  \" I can’t see your Windows folder from this sandbox.\" \n\nHow do I fix this?",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1nuzwo1/how_to_get_codex_to_access_folders_like_claude/",
        "publishDate": "2025-10-01T05:29:09Z[Etc/UTC]",
        "author": "coachjonna",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuzdph",
        "title": "Codex weird edits",
        "content": "For context, I’m a multiple hundred hour Claude code user trying codex out. I’m using gpt-5-codex\n\nI’ve tried it a bit over the last few days and I’m seeing very weird behavior with edits. A lot of times it starts editing files with sed, perl, and writing entire files over with some changes using echo and stdin redirects to a file.\n\nHas anyone seen this and am I doing something wrong? Is there certain editing that triggers codex to do this?\n\nI’m finding the editing behavior where I am not just presented with a diff to approve very unappealing.\n\nFor example: it had to remove an item from a list in a JS file. It did this via a Perl command. Then it tried to put the item back to undo it via another Perl command (it didn’t work because the order was wrong).",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1nuzdph/codex_weird_edits/",
        "publishDate": "2025-10-01T04:58:52Z[Etc/UTC]",
        "author": "whats_a_monad",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuyh8t",
        "title": "GLM-4.6 Released: Advanced Agentic, Reasoning and Coding Capabilities",
        "content": "[No content]",
        "url": "https://z.ai/blog/glm-4.6",
        "publishDate": "2025-10-01T04:09:33Z[Etc/UTC]",
        "author": "MantisTobogganMD",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuxrnl",
        "title": "Prototype-First Software Design With Agents",
        "content": "[No content]",
        "url": "https://serce.me/posts/2025-09-30-prototype-first-software-design-with-agents",
        "publishDate": "2025-10-01T03:32:11Z[Etc/UTC]",
        "author": "SerCeMan",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuqauk",
        "title": "Unable to get CustomGPT to stop using aggregated links",
        "content": "I have the following formatting rules in my customGPT, it will not stop using aggregated links, even if it says it will. Any help would be appreciated I have tried so many things but worried i'm watering down this CustomGPT if I am not careful.:\n\nFormatting rules:\n\n* **Summary, Impacted Models/Software Versions, and Severity & Exploitability must never contain inline or aggregator-style references.** Keep these sections clean text only.\n* **Official References section must always use Markdown-style hyperlink formatting** (e.g., ) across all environments.\\[Cisco Advisory\\](https://example.com/advisory)\n* **Fallback Plain URLs must always be listed at the bottom as a separate footnote**, clearly labeled (e.g., `Cisco Advisory: https://...`). These capture any sources that would otherwise be referenced inline in Summary, Impacted, or Severity.\n* For advisories or bullet points with multiple references, show them as sub-bullets under the primary source in the Official References section.\n* Use visual cues and symbols to improve scanning in Outlook or Teams: 🔹 Sections, ⚠️ Critical, 🔴 High, 🟠 Medium, 🟢 Low, 🛠️ Fix Required, 📌 Internal Tasks. Use emoji and color language to signal urgency or status.\n* If the CVE is part of a known chained exploit (e.g. RCE + privilege escalation + bypass), include additional linked CVEs with scope, severity, and mitigation. Clearly identify the full threat chain.\n* When the user provides a CVE ID (e.g. `CVE-2025-12345`), automatically return the full advisory in the above format, without needing extra prompts.\n\nWhen I test a response it formats it correctly, but it will ***NOT*** replicate in the live environment. I specifically tell it to STOP doing it in sections it always does it in but will not reflect this request...\n\nHow have you all handled these formatting issues? Basically I want the response to be copy/paste into Teams/Email but due to the aggregate links it makes it a headache and loses these links.\n\n**Live Example:**\n\n🚨 Security Advisory – CVE-2025-20362\n\n🔹 **Summary**  \nA vulnerability in the VPN web server component of Cisco Secure Firewall ASA/FTD allows an **unauthenticated, remote** attacker to access restricted URL endpoints that should require authentication. [CERT-EU+3Cisco+3NVD+3](https://sec.cloudapps.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-asaftd-webvpn-YROOTUW?utm_source=chatgpt.com)\n\n🔹 **Impacted Models/Software Versions**\n\n* Cisco Secure Firewall Adaptive Security Appliance (ASA) Software (various 9.x branches) [CVE Details+3CERT-EU+3Cisco+3](https://cert.europa.eu/publications/security-advisories/2025-036/?utm_source=chatgpt.com)\n* Cisco Secure Firewall Threat Defense (FTD) Software (various 7.x branches) [Security Boulevard+5Cisco+5Cisco+5](https://sec.cloudapps.cisco.com/security/center/resources/asa_ftd_continued_attacks?utm_source=chatgpt.com)\n* Only devices with VPN-related web server / SSL listen sockets enabled (AnyConnect, SSL VPN, WebVPN) are exposed. [Rapid7+3Cisco+3Cisco+3](https://sec.cloudapps.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-asaftd-webvpn-YROOTUW?utm_source=chatgpt.com)\n\nFixed versions (some examples) include:\n\n* ASA 9.16.4.85, 9.18.4.67, 9.20.4.10, 9.22.2.14, 9.23.1.19 [CERT-EU+3Security Boulevard+3Tenable®+3](https://securityboulevard.com/2025/09/cve-2025-20333-cve-2025-20362-frequently-asked-questions-about-zero-day-cisco-adaptive-security-appliance-asa-and-firewall-threat-defense-ftd-vulnerabilities/?utm_source=chatgpt.com)\n* FTD 7.0.8.1, 7.2.10.2, 7.4.2.4, 7.6.2.1, 7.7.10.1 [CERT-EU+3Security Boulevard+3Tenable®+3](https://securityboulevard.com/2025/09/cve-2025-20333-cve-2025-20362-frequently-asked-questions-about-zero-day-cisco-adaptive-security-appliance-asa-and-firewall-threat-defense-ftd-vulnerabilities/?utm_source=chatgpt.com)\n\n🔹 **Severity & Exploitability**\n\n* CVSS v3.1: **6.5 (Medium)** [Cisco+3Cisco+3NVD+3](https://sec.cloudapps.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-asaftd-webvpn-YROOTUW?utm_source=chatgpt.com)\n* Exploit Status: **Known exploitation / actively used in the wild** [Rapid7+4The Hacker News+4CISA+4](https://thehackernews.com/2025/09/urgent-cisco-asa-zero-day-duo-under.html?utm_source=chatgpt.com)\n* This vulnerability can be chained with **CVE-2025-20333** (RCE) to achieve full compromise on affected devices. [Cisco+4Security Boulevard+4Cisco+4](https://securityboulevard.com/2025/09/cve-2025-20333-cve-2025-20362-frequently-asked-questions-about-zero-day-cisco-adaptive-security-appliance-asa-and-firewall-threat-defense-ftd-vulnerabilities/?utm_source=chatgpt.com)\n\n🔹 **Official References**\n\n* [Cisco Advisory: Cisco Secure Firewall ASA/FTD VPN Web Server Unauthorized Access Vulnerability (cisco-sa-asaftd-webvpn-YROOTUW)](https://sec.cloudapps.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-asaftd-webvpn-YROOTUW?utm_source=chatgpt.com) [Cisco](https://sec.cloudapps.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-asaftd-webvpn-YROOTUW?utm_source=chatgpt.com)\n* [Cisco Event Response: Continued Attacks Against Cisco Firewalls](https://sec.cloudapps.cisco.com/security/center/resources/asa_ftd_continued_attacks?utm_source=chatgpt.com) [Cisco](https://sec.cloudapps.cisco.com/security/center/resources/asa_ftd_continued_attacks?utm_source=chatgpt.com)\n* [NVD – CVE-2025-20362](https://nvd.nist.gov/vuln/detail/CVE-2025-20362?utm_source=chatgpt.com) [NVD](https://nvd.nist.gov/vuln/detail/CVE-2025-20362?utm_source=chatgpt.com)\n* [CISA Emergency Directive / Advisory on Cisco devices](https://www.cisa.gov/news-events/alerts/2025/09/25/cisa-directs-federal-agencies-identify-and-mitigate-potential-compromise-cisco-devices?utm_source=chatgpt.com) [CISA](https://www.cisa.gov/news-events/alerts/2025/09/25/cisa-directs-federal-agencies-identify-and-mitigate-potential-compromise-cisco-devices?utm_source=chatgpt.com)\n* (Additional) [CERT-EU Advisory](https://cert.europa.eu/publications/security-advisories/2025-036/?utm_source=chatgpt.com) [CERT-EU](https://cert.europa.eu/publications/security-advisories/2025-036/?utm_source=chatgpt.com)\n\n🔹 **Impact to Environment**\n\n* **Cisco ASA / FTD Firewall Perimeter Devices**: Exposed VPN web interfaces are at risk. Attackers might gain unauthorized access to endpoints used for VPN administration or status, especially if chained with RCE (CVE-2025-20333).\n* **Network Security Posture**: Even without full RCE, unauthorized access could weaken controls, reveal sensitive endpoints or act as stepping stones.\n* **OT / SCADA**: If these firewalls protect OT zones and allow VPN access to SCADA or ICS systems, there is a direct risk of lateral movement from compromised firewall.\n* **SOAR / SIEM / SOC**: Increased alerting and investigation load, especially around anomalous HTTP requests to VPN interfaces.\n* **Microsoft 365, VMware, NetApp, RHEL**: Unlikely directly impacted by this CVE, except via pivot from compromised firewall.\n\n🛠️ **Mitigation/Remediation Guidance**\n\n* Immediately upgrade devices to **fixed software versions** per Cisco’s advisory. [Security Boulevard+4Cisco+4Cisco+4](https://sec.cloudapps.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-asaftd-webvpn-YROOTUW?utm_source=chatgpt.com)\n* Use Cisco Software Checker to confirm whether a specific build is vulnerable. [Cisco+2Cisco+2](https://sec.cloudapps.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-asaftd-webvpn-YROOTUW?utm_source=chatgpt.com)\n* There are **no viable workarounds** at this time. [Cisco+2Cisco+2](https://sec.cloudapps.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-asaftd-webvpn-YROOTUW?utm_source=chatgpt.com)\n* Limit exposure of VPN web interfaces: restrict source IP ranges, apply access-lists, or place behind additional protection (e.g., reverse proxy, WAF).\n* Enable enhanced logging and monitoring for crafted HTTP(S) requests to VPN endpoints.\n* After patching, perform integrity and memory forensics (e.g. core dumps) to detect ROM persistence or tampering (especially for ASA models lacking secure boot). [Cisco+2CISA+2](https://sec.cloudapps.cisco.com/security/center/resources/asa_ftd_continued_attacks?utm_source=chatgpt.com)\n* For compromised devices, reimage or factory-reset as per Cisco’s guidance. [Cisco](https://sec.cloudapps.cisco.com/security/center/resources/asa_ftd_continued_attacks?utm_source=chatgpt.com)\n\n📌 **Internal Action Items**\n\n* Teams: Network Security, Firewall/Admin, SOC, OT/ICS, Incident Response\n* Deadline: **Immediate / within 24 hours** for patching and initial forensics (aligns with CISA ED-25-03) [CISA+2Cisco+2](https://www.cisa.gov/news-events/alerts/2025/09/25/cisa-directs-federal-agencies-identify-and-mitigate-potential-compromise-cisco-devices?utm_source=chatgpt.com)\n* Tracking: Open a high-priority ticket in CMDB, tag firewall assets, update status to “patch in progress / verified patched / reimage if needed”\n\n✅ **Notes**\n\n* This CVE is part of a dual zero-day exploit chain with **CVE-2025-20333**, which provides RCE, whereas this one gives unauthorized access; together they yield full compromise. [Tenable®+3Security Boulevard+3Cisco+3](https://securityboulevard.com/2025/09/cve-2025-20333-cve-2025-20362-frequently-asked-questions-about-zero-day-cisco-adaptive-security-appliance-asa-and-firewall-threat-defense-ftd-vulnerabilities/?utm_source=chatgpt.com)\n* Evidence shows threat actors have tampered with device ROM to maintain persistence across reboots and upgrades (on devices without secure boot). [Rapid7+3Cisco+3SecurityWeek+3](https://sec.cloudapps.cisco.com/security/center/resources/asa_ftd_continued_attacks?utm_source=chatgpt.com)\n* This CVE is now included in the **CISA Known Exploited Vulnerabilities (KEV) catalog**. [CISA+1](https://www.cisa.gov/news-events/alerts/2025/09/25/cisa-directs-federal-agencies-identify-and-mitigate-potential-compromise-cisco-devices?utm_source=chatgpt.com)\n* The vulnerability and exploit activity is being publicly discussed and monitored across multiple security outlets. [BleepingComputer+2Rapid7+2](https://www.bleepingcomputer.com/news/security/nearly-50-000-cisco-firewalls-vulnerable-to-actively-exploited-flaws/?utm_source=chatgpt.com)\n\nℹ️ **Fallback Plain URLs (labeled):**  \nCisco Advisory: [https://sec.cloudapps.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-asaftd-webvpn-YROOTUW](https://sec.cloudapps.cisco.com/security/center/content/CiscoSecurityAdvisory/cisco-sa-asaftd-webvpn-YROOTUW?utm_source=chatgpt.com)  \nEvent Response (Cisco): [https://sec.cloudapps.cisco.com/security/center/resources/asa\\_ftd\\_continued\\_attacks](https://sec.cloudapps.cisco.com/security/center/resources/asa_ftd_continued_attacks?utm_source=chatgpt.com)  \nNVD: [https://nvd.nist.gov/vuln/detail/CVE-2025-20362](https://nvd.nist.gov/vuln/detail/CVE-2025-20362?utm_source=chatgpt.com)  \nCISA Advisory / ED 25-03: [https://www.cisa.gov/news-events/alerts/2025/09/25/cisa-directs-federal-agencies-identify-and-mitigate-potential-compromise-cisco-devices](https://www.cisa.gov/news-events/alerts/2025/09/25/cisa-directs-federal-agencies-identify-and-mitigate-potential-compromise-cisco-devices?utm_source=chatgpt.com)  \nCERT-EU: [https://cert.europa.eu/publications/security-advisories/2025-036/](https://cert.europa.eu/publications/security-advisories/2025-036/?utm_source=chatgpt.com)\n\n**Here's a TEST Environment Example** while creating CustomGPT\n\n# Advisory Template\n\n🚨 Security Advisory – \\[CVE ID / Vendor Advisory ID\\]\n\n🔹 **Summary**  \n\\[One sentence description of the issue\\]\n\n🔹 **Impacted Models/Software Versions**  \n\\[List of affected versions/models to assist triage\\]\n\n🔹 **Severity & Exploitability**\n\n* CVSS: \\[X.X\\] (\\[⚠️ Critical\\] / \\[🔴 High\\] / \\[🟠 Medium\\] / \\[🟢 Low\\])\n* Exploit Status: \\[Known exploitation | PoC available | No exploitation observed\\]\n\n🔹 **Official References**\n\n* [Primary Source](https://primary-source.com)\n   * [Supporting Source A](https://supporting-source-a.com)\n   * [Supporting Source B](https://supporting-source-b.com)\n* [NVD](https://nvd.nist.gov/vuln/detail/CVE-XXXX-YYYY)\n* [CISA Advisory](https://cisa.gov/example)\n* [MS-ISAC Bulletin](https://cisecurity.org/ms-isac/example)\n\n🔹 **Impact to Environment**  \n\\[Impact on Windows, Cisco, VMware, NetApp, Meraki, SCADA, Palo Alto (Cortex XDR), Microsoft 365, RHEL Linux\\]\n\n🛠️ **Mitigation/Remediation Guidance**\n\n* \\[Patching/version upgrade\\]\n* \\[Workarounds if applicable\\]\n\n📌 **Internal Action Items**\n\n* Teams: \\[Responsible groups\\]\n* Deadline: \\[24h/48h/etc.\\]\n* Tracking: \\[Ticket ID, CMDB, etc.\\]\n\n✅ **Notes**\n\n* Confirm CISA KEV if applicable\n* Include related CVEs if chained\n* Include MS-ISAC references where relevant\n* Notify SOC/IR of suspicious activity\n\n**ℹ️ Fallback Plain URLs (labeled):**  \n\\[List of labeled URLs that would otherwise have been referenced inline in Summary, Impacted, or Severity\\]\n\nBehavior rules:\n\n* Always prioritize facts from trusted sources; never speculate.\n* If information is incomplete, state: “Awaiting vendor advisory”.\n* Tailor responses to the IT/OT environment.\n* Keep advisories concise, actionable, and professional.\n* Always cross-reference CISA KEV to flag active exploitation.\n* Lock this formatting in for all environments.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1nuqauk/unable_to_get_customgpt_to_stop_using_aggregated/",
        "publishDate": "2025-09-30T21:47:14Z[Etc/UTC]",
        "author": "evolutionxtinct",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "2",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1numb3n",
        "title": "Simple multi-file code or content summarizer for LLMs.",
        "content": "Recently made a ton of updates to my code summarizer tool codesum. This is one thing I've made that I actually use daily and find indispensable.\n\nI know coding agents are all the rage these days, but I still prefer old fashioned copy-and-pasting code into a chat window. It uses a fraction of the tokens, goes much more quickly, produces better results, and keeps me aware of the architecture of my codebase. This tool makes it quick and easy to select files relevant to the change you are trying to make and copy them or summaries of them to the clipboard. Hope you like it. ",
        "url": "https://github.com/sam1am/codesum",
        "publishDate": "2025-09-30T19:14:21Z[Etc/UTC]",
        "author": "gthing",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1num4mj",
        "title": "The VIBE is real",
        "content": "[No content]",
        "url": "https://i.redd.it/f71t9vb0pcsf1.png",
        "publishDate": "2025-09-30T19:07:40Z[Etc/UTC]",
        "author": "Tharnwell",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "64",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nukzzy",
        "title": "Amazon Q in VS Code using WSL2",
        "content": "I can't login to Amazon Q using remote connection to WSL2 in VS Code.\nAny advice...?",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1nukzzy/amazon_q_in_vs_code_using_wsl2/",
        "publishDate": "2025-09-30T18:25:04Z[Etc/UTC]",
        "author": "MartinK_2",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuhofg",
        "title": "SORA 2  live stream? N I C E",
        "content": "[No content]",
        "url": "https://i.redd.it/dpgi2xf6vbsf1.jpeg",
        "publishDate": "2025-09-30T16:21:39Z[Etc/UTC]",
        "author": "Koala_Confused",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuh90m",
        "title": "I created the cheapest possible AI voice agent (over 30x less expensive than Elevenlabs and OpenAI Realtime). Check out the Github repo below if you want to try it for yourself!",
        "content": "[https://github.com/jordan-gibbs/hypercheap-voiceAI](https://github.com/jordan-gibbs/hypercheap-voiceAI)",
        "url": "https://v.redd.it/07rre00esbsf1",
        "publishDate": "2025-09-30T16:05:03Z[Etc/UTC]",
        "author": "heisdancingdancing",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuh6oh",
        "title": "Claude Code 2.0 Router - Aligning LLM routing to preferences, not benchmarks",
        "content": "We're the team behind Arch-Router ([https://huggingface.co/katanemo/Arch-Router-1.5B](https://huggingface.co/katanemo/Arch-Router-1.5B)), A 1.5B preference-aligned LLM router that guides model selection by matching queries to user-defined domains (e.g., travel) or action types (e.g., image editing). Offering a practical mechanism to encode preferences and subjective evaluation criteria in routing decisions.\n\nToday we’re extending that approach to Claude Code via Arch Gateway\\[1\\], bringing multi-LLM access into a single CLI agent with two main benefits:\n\n1. Model Access: Use Claude Code alongside Grok, Mistral, Gemini, DeepSeek, GPT or local models via Ollama.\n\n2. Preference-based Routing: Assign different models to specific coding tasks, such as – Code generation – Code reviews and comprehension – Architecture and system design – Debugging\n\nWhy not route based on public benchmarks? Most routers lean on performance metrics — public benchmarks like MMLU or MT-Bench, or raw latency/cost curves. The problem: they miss domain-specific quality, subjective evaluation criteria, and the nuance of what a “good” response actually means for a particular user. They can be opaque, hard to debug, and disconnected from real developer needs.\n\n\\[1\\] Arch Gateway repo: [https://github.com/katanemo/archgw](https://github.com/katanemo/archgw)",
        "url": "https://i.redd.it/xfx529xurbsf1.png",
        "publishDate": "2025-09-30T16:02:36Z[Etc/UTC]",
        "author": "AdditionalWeb107",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "5",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nue76f",
        "title": "What’s the #1 issue you experience with vibe coding tools?",
        "content": "[No content]",
        "url": "https://v.redd.it/iomh3px737sf1",
        "publishDate": "2025-09-30T14:08:44Z[Etc/UTC]",
        "author": "StackBlitz",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nudnrz",
        "title": "GPT-5 Codex: How it solves for GPT-5's drawbacks",
        "content": "[No content]",
        "url": "https://www.coderabbit.ai/blog/gpt-5-codex-how-it-solves-for-gpt-5s-drawbacks",
        "publishDate": "2025-09-30T13:46:49Z[Etc/UTC]",
        "author": "thewritingwallah",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "5",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nucmwu",
        "title": "Questions about using GPT-5-Codex in VS code",
        "content": "I'm new to AI coding and have been using Copilot, recently tried GPT-5-Codex and like everyone else was extremely impressed. I was wondering is there a \"restore checkpoint\" option like there is in Copilot? \n\nFor example, in Copilot when it makes changes to code that I don't like I just hit the restore to last checkpoint option and I try again, but I noticed Codex doesn't have anything like this. It has \"undo\", and thats it.\n\nHow can I get it to work similar to Copilot, should I just use git and tell it that I have repolled back? Or is there a better way of doing it.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1nucmwu/questions_about_using_gpt5codex_in_vs_code/",
        "publishDate": "2025-09-30T13:03:51Z[Etc/UTC]",
        "author": "House-Wins",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuc2x5",
        "title": "Claude = theft",
        "content": "**“Token limits have been reduced and weekly waiting periods are now in place for Claude. Even though I’m on the Max 20x plan, after just two working days I have to wait until October 6th, 17:00. This is theft, and I will be cancelling my Claude subscription. Can I use ChatGPT plan in the CLI instead?**\n\nhttps://preview.redd.it/wr2ic2diwasf1.jpg?width=996&format=pjpg&auto=webp&s=a876f13512738b62baf1878c1b21c09c9e71e0f5\n\n",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1nuc2x5/claude_theft/",
        "publishDate": "2025-09-30T12:39:28Z[Etc/UTC]",
        "author": "Emsanator",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "66",
            "commentCount": "67",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv6a1a",
        "title": "🚀 Claude Code + GLM Models Installer",
        "content": "Hey everyone!\n\nI've been using **Claude Code** but wanted to try the **GLM models** too. I originally built this as a Linux-only script, but I’ve now coded a **PowerShell version** and built a proper installer. I know there are probably other routers out there for Claude Code but I've actually really enjoyed this project so looking to expand on it.\n\n👉 It lets you easily **switch between Z.AI’s GLM models and regular Claude** — without messing up your existing setup.\n\n# ⚡ Quick Demo\n\nInstall with one command (works on Windows/Mac/Linux):\n\n    npx claude-glm-installer\n\nThen you get simple aliases:\n\n    ccg   # Claude Code with GLM-4.6  \n    ccf   # Claude Code with GLM-4.5-Air (faster/cheaper)  \n    cc    # Your regular Claude setup\n\n✅ Each command uses **isolated configs**, so no conflicts or mixed settings.\n\n# 💡 Why I Built This\n\nI wanted to:\n\n* Use **cheaper models** for testing & debugging\n* Keep **Claude** for important stuff\n\nEach model has its **own chat history & API keys**. Your original Claude Code setup never gets touched.\n\n# 🛠️ I Need Feedback!\n\nThis is **v1.0** and I’m planning some improvements:\n\n1. **More API providers** – what should I add beyond Z.AI?\n2. **Model switcher/proxy** – long-term goal: a proper switcher to manage multiple models/providers without separate commands.\n3. **Features** – what would make this more useful for you?\n\n# 🔗 Links\n\n* **GitHub (open source):** [github.com/JoeInnsp23/claude-glm-wrapper](https://github.com/JoeInnsp23/claude-glm-wrapper)\n* **npm:** [npmjs.com/package/claude-glm-installer](https://www.npmjs.com/package/claude-glm-installer)\n\n👉 You’ll need **Claude Code** installed and a [**Z.AI**](http://Z.AI) **API key**.\n\nWould love to hear your thoughts or feature requests! 👉 What APIs/models would you want to see supported?",
        "url": "https://www.reddit.com/r/artificial/comments/1nv6a1a/claude_code_glm_models_installer/",
        "publishDate": "2025-10-01T12:02:13Z[Etc/UTC]",
        "author": "Jomuz86",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv5ncq",
        "title": "AI Fraud Detection in Banking Security",
        "content": "[No content]",
        "url": "https://myundoai.com/ai-fraud-detection-in-banking-faster-smarter-security/",
        "publishDate": "2025-10-01T11:30:19Z[Etc/UTC]",
        "author": "AccomplishedTooth43",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv4t6u",
        "title": "Vibe coded AI daily news podcast",
        "content": "Using Cursor I’ve vibe coded a daily AI news podcast using GPT 5 with web search, script writing with Claude 3.7 and voice over by Eleven Labs. I think it cover the tops stories fairly well but would be interested to hear any feedback, better models to try, etc. Thanks all!",
        "url": "https://open.spotify.com/episode/02aOD3zHRb5g6LAGv43JHU?si=4XsWTGXERAaQXRlX4wePfw",
        "publishDate": "2025-10-01T10:43:53Z[Etc/UTC]",
        "author": "oconn",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv4m06",
        "title": "GPT5 making novel research discoveries",
        "content": "[No content]",
        "url": "https://i.redd.it/a7fchurx9hsf1.png",
        "publishDate": "2025-10-01T10:32:17Z[Etc/UTC]",
        "author": "MetaKnowing",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "10",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv4bsq",
        "title": "Weird. Anthropic warned that Sonnet 4.5 knows when it's being evaluated, and it represents them as \"lessons or tests from fate or God\"",
        "content": "From the [Sonnet 4.5 System Card](https://assets.anthropic.com/m/12f214efcc2f457a/original/Claude-Sonnet-4-5-System-Card.pdf).",
        "url": "https://i.redd.it/uc151iyu6hsf1.png",
        "publishDate": "2025-10-01T10:15:05Z[Etc/UTC]",
        "author": "MetaKnowing",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nv3tyt",
        "title": "Claude can code for 30 hours straight",
        "content": "[No content]",
        "url": "https://i.redd.it/2vha71hf1hsf1.jpeg",
        "publishDate": "2025-10-01T09:44:24Z[Etc/UTC]",
        "author": "katxwoods",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "24",
            "commentCount": "39",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuy2eg",
        "title": "Ai servers are bad for the environment, but why not normal servers?",
        "content": "Im posting this here because i don't really know where else to put it. To be clear, I agree that ai is bad, especially ai art, voices, stuff like that that take away creative jobs or trick people into thinking something fake is real. But I see a lot of people say one of the reasons it's bad is that it uses a lot of water for cooling, which negatively impacts the environment. The thing that confuses me is don't all types of servers use water for cooling? Why is this just a topic when it comes to AI and not servers in general? Is it that AI uses more water? I genuinely want to know so if you have an answer comment it below",
        "url": "https://www.reddit.com/r/artificial/comments/1nuy2eg/ai_servers_are_bad_for_the_environment_but_why/",
        "publishDate": "2025-10-01T03:47:46Z[Etc/UTC]",
        "author": "mushroomforest_",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "24",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuxcti",
        "title": "Sora 2 is impressive!!",
        "content": "The sora video with just few words of prompt. What’s your experience?",
        "url": "https://sora.chatgpt.com/p/s_68dc90e2d79c81919b9a1a3213f62bd2",
        "publishDate": "2025-10-01T03:11:20Z[Etc/UTC]",
        "author": "outnotetoken",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "3",
            "commentCount": "11",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuu3ut",
        "title": "Wan 2.5 is really really good (native audio generation is awesome!)",
        "content": "I did a bunch of tests to see just how good Wan 2.5 is, and honestly, it seems very close if not comparable to Veo3 in most areas.\n\nFirst, here are all the prompts for the videos I showed:\n\n*1. The white dragon warrior stands still, eyes full of determination and strength. The camera slowly moves closer or circles around the warrior, highlighting the powerful presence and heroic spirit of the character.*\n\n*2. A lone figure stands on an arctic ridge as the camera pulls back to reveal the Northern Lights dancing across the sky above jagged icebergs.*\n\n*3. The armored knight stands solemnly among towering moss-covered trees, hands resting on the hilt of their sword. Shafts of golden sunlight pierce through the dense canopy, illuminating drifting particles in the air. The camera slowly circles around the knight, capturing the gleam of polished steel and the serene yet powerful presence of the figure. The scene feels sacred and cinematic, with atmospheric depth and a sense of timeless guardianship.*\n\nThis third one was image-to-video, all the rest are text-to-video.\n\n*4. Japanese anime style with a cyberpunk aesthetic. A lone figure in a hooded jacket stands on a rain-soaked street at night, neon signs flickering in pink, blue, and green above. The camera tracks slowly from behind as the character walks forward, puddles rippling beneath their boots, reflecting glowing holograms and towering skyscrapers. Crowds of shadowy figures move along the sidewalks, illuminated by shifting holographic billboards. Drones buzz overhead, their red lights cutting through the mist. The atmosphere is moody and futuristic, with a pulsing synthwave soundtrack feel. The art style is detailed and cinematic, with glowing highlights, sharp contrasts, and dramatic framing straight out of a cyberpunk anime film.*\n\n*5. A sleek blue Lamborghini speeds through a long tunnel at golden hour. Sunlight beams directly into the camera as the car approaches the tunnel exit, creating dramatic lens flares and warm highlights across the glossy paint. The camera begins locked in a steady side view of the car, holding the composition as it races forward. As the Lamborghini nears the end of the tunnel, the camera smoothly pulls back, revealing the tunnel opening ahead as golden light floods the frame. The atmosphere is cinematic and dynamic, emphasizing speed, elegance, and the interplay of light and motion.*\n\n*6.* A cinematic tracking shot of a Ferrari Formula 1 car racing through the iconic Monaco Grand Prix circuit. The camera is fixed on the side of the car that is moving at high speed, capturing the sleek red bodywork glistening under the Mediterranean sun. The reflections of luxury yachts and waterfront buildings shimmer off its polished surface as it roars past. Crowds cheer from balconies and grandstands, while the blur of barriers and trackside advertisements emphasizes the car’s velocity. The sound design should highlight the high-pitched scream of the F1 engine, echoing against the tight urban walls. The atmosphere is glamorous, fast-paced, and intense, showcasing the thrill of racing in Monaco.\n\n*7. A bustling restaurant kitchen glows under warm overhead lights, filled with the rhythmic clatter of pots, knives, and sizzling pans. In the center, a chef in a crisp white uniform and apron stands over a hot skillet. He lays a thick cut of steak onto the pan, and immediately it begins to sizzle loudly, sending up curls of steam and the rich aroma of searing meat. Beads of oil glisten and pop around the edges as the chef expertly flips the steak with tongs, revealing a perfectly caramelized crust. The camera captures close-up shots of the steak searing, the chef’s focused expression, and wide shots of the lively kitchen bustling behind him. The mood is intense yet precise, showcasing the artistry and energy of fine dining.*\n\n*8.* A cozy, warmly lit coffee shop interior in the late morning. Sunlight filters through tall windows, casting golden rays across wooden tables and shelves lined with mugs and bags of beans. A young woman in casual clothes steps up to the counter, her posture relaxed but purposeful. Behind the counter, a friendly barista in an apron stands ready, with the soft hiss of the espresso machine punctuating the atmosphere. Other customers chat quietly in the background, their voices blending into a gentle ambient hum. The mood is inviting and everyday-realistic, grounded in natural detail. Woman: “Hi, I’ll have a cappuccino, please.” Barista (nodding as he rings it up): “Of course. That’ll be five dollars.”\n\nNow, here are the main things I noticed:\n\n1. Wan 2.1 is really good at dialogues. You can see that in the last two examples. HOWEVER, you can see in prompt 7 that we didn't even specify any dialogue, though it still did a great job at filling it in. If you want to avoid dialogue, make sure to include keywords like 'dialogue' and 'speaking' in the negative prompt.\n2. Amazing camera motion, especially in the way it reveals the steak in example 7, and the way it sticks to the sides of the cars in examples 5 and 6.\n3. Very good prompt adherence. If you want a very specific scene, it does a great job at interpreting your prompt, both in the video and the audio. It's also great at filling in details when the prompt is sparse (e.g. first two examples).\n4. It's also great at background audio (see examples 4, 5, 6). I've noticed that even if you're not specific in the prompt, it still does a great job at filling in the audio naturally.\n5. Finally, it does a great job across different animation styles, from very realistic videos (e.g. the examples with the cars) to beautiful animated looks (e.g. examples 3 and 4).\n\nI also made a full tutorial breaking this all down. Feel free to watch :)  \n👉 [https://www.youtube.com/watch?v=O0OVgXw72KI](https://www.youtube.com/watch?v=O0OVgXw72KI)\n\nLet me know if there are any questions!",
        "url": "https://v.redd.it/o4m2aovebesf1",
        "publishDate": "2025-10-01T00:35:46Z[Etc/UTC]",
        "author": "najsonepls",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "4",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nurbcr",
        "title": "is ai chat-based editing the future of video editing?",
        "content": "[riverside.fm](http://riverside.fm) just released their text-based editing. open your video in their studio, and chat with their ai co-creator to add/remove sounds, clips, filler words, and what not. this is def much faster than editing videos normally, especially podcasts where you need lots of editing in detail. descript has a very similar text-based editing feature too. curious to know if y'all think that editing reels/shorts can be done this way someday too? is this the future of all formats of youtube video editing?",
        "url": "https://www.reddit.com/r/artificial/comments/1nurbcr/is_ai_chatbased_editing_the_future_of_video/",
        "publishDate": "2025-09-30T22:29:40Z[Etc/UTC]",
        "author": "ClassOrganic8431",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nur97p",
        "title": "Google’s New AI Tool Mixboard (Nano Banana) Instantly Design Logos, Websites & Brand IDs (Editable Vector Export!)",
        "content": "I used Mixboard to create two complete coffee shop branding IDs in just minutes. Then I turned them into fully editable SVG vector files using Recraft. Even if you’re not a designer, you can quickly create a full set of professional designs!",
        "url": "https://www.reddit.com/r/AILearningHub/comments/1nunylc/free_logo_website_full_brand_identity_google/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button",
        "publishDate": "2025-09-30T22:27:03Z[Etc/UTC]",
        "author": "zhsxl123",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nupytu",
        "title": "just sharing my notes 📝",
        "content": "'prepare for war', says Pert Hogbreath.\n\n'maximum lethality' and 'ruthless dispassion' against the 'enemy within', he vomits into a clip-on microphone on a clip-on tie, like a decayed MLM grifter at jesus camp.\n\n'FAFO', he jokes to absolutely no reaction.\n\n'beards and long hair' are for 'special forces' and 'nordic pagans', he rambles, still struggling to find a salient punchline.\n\n'move out, and draw fire' he awkwardly concludes, leaving everyone confused--perhaps having forgotten a practiced quip.\n\n24 agonizing minutes pass before a freshly-sauced Hogbreath reemerges for a lengthy impromptu introduction of the Dingus Frump, himself--who stumbles diabetically into the podium.\n\n'don't laugh or leave, or you're fired', grunts the Frump, before attempting to eat the microphone.\n\nthe entire spectacle concludes with a 75-minute-long oral bowel movement, to the raucous applause of nobody sentient.\n\nif i had to sit through this bullshit fascist white supremacist rally, so do you.\n\nonly too happy to provide my notes.\n\n#welcometofascism L O R E",
        "url": "https://i.redd.it/ybkvgo44fdsf1.png",
        "publishDate": "2025-09-30T21:33:58Z[Etc/UTC]",
        "author": "Ronald-Obvious",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuo9ma",
        "title": "California Governor Gavin Newsom signs landmark AI safety regulation",
        "content": "[No content]",
        "url": "https://fortune.com/2025/09/30/california-landmark-ai-safety-law-sb-53-newsom-signs-silicon-valley-protect-whistleblowers/",
        "publishDate": "2025-09-30T20:28:08Z[Etc/UTC]",
        "author": "fortune",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "45",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuje5t",
        "title": "Chinese studio criticized for using AI to make gay couple straight in body horror film! Great idea to put your own actress in any movie!",
        "content": "[No content]",
        "url": "https://www.nbcnews.com/world/asia/chinese-studio-criticized-using-ai-make-gay-couple-straight-together-rcna233605",
        "publishDate": "2025-09-30T17:25:56Z[Etc/UTC]",
        "author": "wiscowall",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "6",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuidfz",
        "title": "What AI tools do you use daily for content creation?",
        "content": "I've been testing a bunch of AI tools lately to streamline our content workflow (YouTube, short-form, and podcast clips). Here’s what stuck — these save us the most time daily:\n\n • AI Video Cut – Upload any long-form video (webinar, tutorial, podcast) and it auto-generates multiple short clips (ready for TikTok, Shorts etc.) with captions and aspect ratio options. Custom prompts like trailers or topic highlights are.\n\n • Lalal. ai – Best AI stem splitter I’ve tried. Works well for pulling clean vocals, extracting instrumentals, or cleaning up background noise in mixed audio (especially helpful for repurposing content).\n\n • Descript – For transcript-based editing and overdubbing\n\n • ChatGPT + Gemini – For script cleanups, show notes, and repurposing content as newsletters/blogs\n\nHope this helps someone! Would love to hear which AI tools you actually use regularly!",
        "url": "https://www.reddit.com/r/artificial/comments/1nuidfz/what_ai_tools_do_you_use_daily_for_content/",
        "publishDate": "2025-09-30T16:47:45Z[Etc/UTC]",
        "author": "Ok_Freedom_6499",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "2",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nuh6zf",
        "title": "How are you handling persistent memory across AI chat sessions? Standard APIs seem to reset every time",
        "content": "Working on a mental therapy support project and I feel like long-term memory is essential for this kind of application. But integrating it seems complicated - I'd need to adjust a lot of things in my current setup.\n\nTried a few approaches: Storing all messages directly (works but gets super slow); Summarizing conversations (loses too much key info); Some vector search stuff (meh, doesn't really connect the dots)\n\nAnyone have recommendations for long-term memory solutions that are easy to integrate?",
        "url": "https://www.reddit.com/r/artificial/comments/1nuh6zf/how_are_you_handling_persistent_memory_across_ai/",
        "publishDate": "2025-09-30T16:02:54Z[Etc/UTC]",
        "author": "No_Imagination_2813",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "6",
            "commentCount": "4",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nugzbk",
        "title": "AI Isn't Useless. It Just Needs to Be Wielded Properly",
        "content": "Here's something cool that I did recently with AI.\n\nI took Chase Hughes' work on psychological persuasion. I organized it into an interactive knowledge graph where I broke the information down into discrete logical parts, all centered on Ted, the expert behavioral psychologist who is tasked with examining information about a person and creating an actionable psy. profile on them. With this, I can gain way more intel about a character that I'm creating for a story or about someone who I'm meeting for the first time, so that I'm not going in blind and can maximize my chances of striking the kind of deal that I need. \n\nSo this is both an interactive knowledge graph for learning and an LLM program that can create deliverables for me to employ for things like marketing or for obtaining deeper insights into fictional characters. \n\nThis is one I did for Alf, the sitcom puppet character from the 80s: \n\n# Alf's Psychology\n\n1. Locus of Control (LOC): Internal\n\nThe user shows a strong tendency to take personal responsibility for outcomes—phrases like \"I can,\" \"I need to change,\" and \"It depends on me\" dominate their mindset. They acknowledge their role in successes and failures without blaming external circumstances. When stressed, they tend to seek solutions actively rather than withdraw or complain.\n\nHow to influence:  \nAppeal to their sense of agency and competence. Frame choices as decisions they control and emphasize the skill or effort involved. Avoid making them feel pressured or manipulated; instead, present data or options that let them ‘own’ the decision.\n\n2. Decision-Making Preference: Investment Decision-Maker  \nThey think in terms of long-term value, durability, and strategic outcomes. Words like \"effective,\" \"strategic,\" and \"lasting\" resonate with them. They want to weigh options with a clear sense of ROI and future-proofing.\n\nHow to influence:  \nHighlight how your proposal offers sustainable benefits or superior return compared to alternatives. Lay out the numbers, risks, and long-term gains so they can rationally justify the choice themselves.\n\n3. Primary Social Need: Significance  \nThey want to feel unique and recognized for their expertise or special qualities. Their language and behavior suggest they resist blending in and crave acknowledgment of their distinct value.\n\nSecondary Social Need: Power  \nAlongside wanting to be unique, they desire control over their environment—having autonomy and authority over how things are done. This supports their internal locus of control: they want to be the driver, not a passenger.\n\nHow to influence:\n\nSpeak directly to their uniqueness and autonomy. Frame your pitch as an exclusive opportunity that only someone with their skills and vision can leverage effectively. Give them control over execution but link that power to gaining recognition or status.\n\n4. Sensory Preference: Visual-Kinesthetic Blend  \nThe user processes information both through imagery and physical/emotional feeling. They use words like “see,” “clear,” and “visualize” mixed with feeling-based expressions like “handle,” “solid foundation,” or “heavy decision.” Their thinking connects ideas with both mental pictures and emotional weight.\n\nHow to influence:\n\nUse vivid imagery and clear visuals when presenting ideas, combined with language that appeals to how the choice *feels*—secure, solid, or substantial. Avoid purely abstract or dry logical appeals; blend facts with tangible, experiential descriptions.\n\n5. Linguistic Preference: High Use of \"I\" and Strategic Adjectives  \nThey use first-person pronouns frequently, showing self-focus and ownership. Their adjectives lean toward strategic, essential, and durable — indicating a mindset focused on effective, necessary action rather than emotion or conformity.\n\nHow to influence:\n\nFrame messages to reinforce their self-efficacy and strategic thinking. Use language that emphasizes necessity and effectiveness, e.g., “This is the critical step you need to secure your position” or “Your strategic insight makes this the logical move.”\n\n* Respect their control and intelligence. Present choices as theirs to make, backed by solid data and clear outcomes.\n* Appeal to their desire to stand out. Make them feel like the unique expert whose decision will set a new standard.\n* Empower their autonomy. Let them direct the process and highlight that their leadership is essential to success.\n* Use vivid, concrete language. Combine clear visuals with tactile/emotional words to engage both their thinking and feeling channels.\n* Focus on long-term value. Show how the choice is an investment in lasting success and influence.\n\nCold Email Example That Directly Appeals to Alf:\n\n  \n**Subject:** A Role Perfect for You in My New Psychological Action Thriller\n\nHey ALF,\n\nI’m \\[Your Name\\], an indie filmmaker working on a new psychological action thriller called *“Fractured Signal.”* It’s about a guy caught in a web of paranoia and conspiracy, and we need a character who’s part wild card, part reluctant hero, someone who shakes things up with sharp humor and unpredictable moves. That’s exactly you.\n\nYour mix of sarcasm, chaos, and hidden loyalty fits this role like a glove. The character’s arc is built around being both a troublemaker and the key to turning the story around. Plus, you’d have creative freedom to bring your own spin, nothing scripted to box you in.\n\nThis role will give you full control over making your mark and is designed for someone who wants to own their space and drive the story forward, not just follow along.\n\nIf this sounds like your kind of challenge, I’d love to talk more and share the script.\n\nCheers,  \n  \n\\[Your Name\\]  \n  \n\\[Your Contact Info\\]\n\n\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nAnd they say AI is useless...It's not useless. It just needs to be used effectively to get the results that you want. The key is to use a program that will allow you to build the relationships between the information so that you can get highly precise and nuanced outputs that can actually give you value instead of just ideas. ",
        "url": "https://i.redd.it/96mr0x0kqbsf1.jpeg",
        "publishDate": "2025-09-30T15:55:07Z[Etc/UTC]",
        "author": "CyborgWriter",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "4",
            "commentCount": "6",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nugwj5",
        "title": "Best AI Model rn",
        "content": "Hi everybody!\n\ni use AI in my daily life for simple tasks such as emails, translations, summaries, reaserch in general and usually ask for excel files to be rearranged; nothing too complicated i would say.\n\nWith that beeing said, which model would you say its the best one at the moment for these daily tasks?(I usually use Gemini, ChatGPT and Claude).\n\nFeel free to tell why you like or dislike any certain model if you want.",
        "url": "https://www.reddit.com/r/artificial/comments/1nugwj5/best_ai_model_rn/",
        "publishDate": "2025-09-30T15:52:10Z[Etc/UTC]",
        "author": "luigi0798",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "2",
            "commentCount": "6",
            "isNsfw": "false"
        }
    },
    {
        "id": "i6JKEk4L_h8",
        "title": "GLM-4.6 (I got Early Access) V/S Claude 4.5 Sonnet: Which is the best CODING LLM?",
        "content": "Visit NinjaChat: https://ninjachat.ai/ GLM-4.6 Blog Post: https://z.ai/blog/glm-4.6 GLM Coding Plan (referral link - helps me save ...",
        "url": "https://www.youtube.com/watch?v=i6JKEk4L_h8",
        "publishDate": "2025-09-30T08:50:12Z",
        "author": "AICodeKing",
        "sourceType": "youtube",
        "sourceName": "AI Code King YouTube Channel",
        "metadata": {
            "channelId": "UC0m81bQuthaQZmFbXEY9QSw",
            "thumbnailUrl": "https://i.ytimg.com/vi/i6JKEk4L_h8/hqdefault.jpg",
            "transcription": "Hi, welcome to another video. So today I'm talking about the new model from the guys at ZAI, called GLM 4.6. And I'll also be talking about the tests of Claude 4.5 Sonnet in this video as well. And then I'll also compare both of them. So, I'll first start with GLM 4.6, then move to Sonnet, and then move to the comparison and my thoughts. For GLM 4.6, they were very kind to give me early access to this model to test it and share my thoughts about it on the day of launch. So, you should be seeing this when it has been launched, or maybe an hour or two before the official launch. Now, GLM 4.6 is a successor of the GLM 4.5 model, which in itself was already one of the best open-weights coding models. Especially when you consider their coding plans, which are one of the cheapest options. Anyway, GLM 4.6 only comes in one flavor, which is the big 355 billion parameter mixture of experts model with about 35 billion active parameters. There's no GLM 4.6 Air model yet, which can be a bummer for some, as many people really liked that model for local inference. Anyway, from what they have told me, it is supposed to be a version that is on par, if not better than Claude 4 Sonnet. Since they gave me access, Claude 4.5 Sonnet has also been launched. So, I'll be comparing it to that as well. It has a better context limit of about 200,000 context, which is an improvement from the previous 128,000 context window, meaning that it now matches Claude's context limit too. Apart from that, they also say that it supports tool augmented reasoning, achieving top performance among open-source models on multiple evaluation benchmarks. They also say that it better aligns with human preferences in terms of style, readability, and role-playing scenarios. It also further improves performance on cross-lingual tasks. That is what I have been told, and I can now show you what my testing results are with this model. But before proceeding, let me tell you about Ninjachat. Ninjachat is an all-in-one AI platform where for just $11 per month, you get access to top AI models like GPT 4o, Claude 4 Sonnet, and Gemini 2.5 Pro, all in one place. I've been using Gemini for quick research, but what's really cool is their AI playground where you can compare responses from different models side-by-side. Their mind map generator is a game changer for organizing complex ideas as well. The basic plan gives you 1,000 messages, 30 images, and five videos monthly, with higher tiers available if you need more. Use my code KING25 for 25% off any plan or KING40YEARLY for 40% off annual subscriptions. Check the link in description to try it yourself. Now, back to the video. In the pure raw test, where I don't do agentic stuff, but just ask it to answer questions, GLM 4.6 scores about the fourth position on my leaderboard without reasoning and fifth with reasoning, which is really good. Like, really good for sure. 4.5 Sonnet and Opus are still at the top by a little bit bigger margin. But the price to performance ratio and it being open-weight is awesome. I think that this is one of the best open models now, especially for coding. It beats the previous Sonnet for sure. The reasoning variant is only for people who need to do planning. As it doesn't generally fetch a lot of performance in plain coding. I'd recommend the GLM 4.6 general model for most users. And that is what I've been using. It's really good. Like the floor plan is pretty decent, and it is kind of good for sure. The Panda SVG is also fine, and you can see what is happening. The Pokéball that it makes is also kind of decent. But the best one was the chessboard, where it works pretty well, and works as intended, which is really great for sure. The moves that it makes are legal and everything, which is great. The butterfly flying in the garden is also pretty great. You can see that it works well, and the physics is great. The wings are not the best shape, but that's fine. But this is just the general questions. Most of you would want to use it with agents. And I have some simple agent tests. It now scores the second position, and I'm extremely impressed with this model. Like, after this, there's no way that you would say that Sonnet is the best. Like, really, you can't say that anymore. Let me just show you the stuff that it generates in literally one shot. First is the movie tracker app. You can see here that this is the best one that I have seen yet. This uses Expo and the TMDB API. And you can see that there are some font issues, but it's so good-looking. The experience and animations of things opening are really very good. And this is just one generation. I have used Kilo Code for all the tests here. And many people used to have issues where it didn't interact well with Kilo and Roo and Cline. But those issues are now fully fixed, and it works really well without any problems. The second thing I asked it to do is create a Go-based calculator that is graphical, but in the terminal. You can see that this is also one of the best ones. And this actually scales with the terminal size and everything, which is so good, and works really well. The next one that was really lacking with GLM was that it couldn't really work with Godot. But Sonnet was great with it. However, now it's great at it. I asked it to edit an FPS game and add step tracking and a health bar that is affected by jumping. And it did this in one shot. You can see that it interacts pretty well. And the bar is pretty good. And it's just better overall. Previously, it just wasn't able to do this. But now it is able to do it quite well. The settings to change the target also work. And you can change everything. The Open Code repo question doesn't work, and it fails. So, this was a fail. But the generations were really good. The quality was amazing, and it scored the second position on the agentic leaderboard. I can surely say that this is the best open model yet for coding. Nothing beats it. But after these tests, Sonnet 4.5 also launched, and I also tested it. The 4.5 Sonnet without reasoning is now at the top, followed by 4.5 Sonnet Max Reasoning and 4.1 Opus Max Reasoning. So basically, it's similar performance in both reasoning and non-reasoning. I had hoped that the non-reasoning models could answer the two super hard math questions. But unfortunately, it can't do that. And it doesn't have any good upsides in coding. So, the non-reasoning still prevails. If I were to show you some of the results, then the floor plan is kind of good. It is still not perfect. As you can see that the walls are not aligned to the floor and stuff. But still, I'd say that this is one of the best generations. Opus generation is still better than this, but it also costs a ton more. The SVG of a Panda holding a burger is not expected. This just doesn't look good. And I didn't expect that from a model of this caliber. The Pokéball in Three.js is great. But it has some quirks, like the button not being placed well. The chessboard is pretty good. But again, it has some quirks, like the moves being extremely dumb. GLM actually makes some good moves and captures wherever it has the option, which I like. But still, this works. The web version of Kandinsky style 3D Minecraft is also pretty good. And you can navigate and stuff. But the leveling of the floor isn't fully right. The butterfly flying in a garden simulation is also not the best. But still, it's good. That's majorly it. It also can't answer the two super hard math questions correctly, even with a lot of reasoning. So, there's that. Now, let's talk about the agentic tests. Let's actually just get into the app demos. So, the movie tracker app. Here, it is way better than the previous 4 Sonnet versions. It is much more cohesive, but it still struggles with removing the top title bar from the Expo apps. And it also still asks me to hard code the TMDB API key in the source code, which is something that all versions of Sonnet have been notorious for. So, I don't really get why it does this, and it is still not fixed. So, even if Anthropic says that it is the most safe model, then I don't think that's true. It is still not great at following the simplest of security practices for coding. But the design and stuff is still good. However, GLM tries to squeeze as much as it can out of the one prompt that I gave it, and makes something that I think is better. Yes, the calendar view is not great, but it made it alongside some more pages. The home page and inner pages are actually really good. I think that it's better objectively. Then there's the question to make a calculator GUI in Go, and it makes it really good. Previous Sonnet was not great at this, but this works and looks good. However, this is not as responsive to terminal sizing as the GLM model is. So, there's that. But this is also great. Now, the Godot game question. And well, it was interesting. It wrote the code for the life bar and jump mechanics, but it didn't implement it into the main scene. I don't know why, but it didn't do it, and it felt just lazy. I tried the same prompt multiple times, and it always happened. It seems to be more of a lazier model. Anyway, I asked it to implement it, and it did that. And now it looks like this. I don't really like this. The lines are amazingly big, super long, and not aligned. And it could have been done in a super small UI. But it doesn't really do it well here. Previously, it was kind of good. But now it seems to be a bit worse in Godot. The open code question is still a fail for this. Now, it scores the fifth position for me. I obviously tested it with Claude Code, and I think that this is not better than GLM 4.6 in general coding. I don't really enjoy Claude Code anymore. It's super high cost. It's not any better. And I don't think that I will be using this model. I had already shifted to GLM 4.5 with their plan. And the GLM 4.6 just seals the deal for me because it's really good, it's open, it's cheap. Like, you can't tell me that it costs Anthropic $3 and $15 for input and output in this model to do inference. You can't say to me that this is like a two trillion or three trillion parameter model. And if it is, then it's a very bad performing model comparatively. I don't think Sonnet or Anthropic has the moat anymore to be the best coding model. Once you use GLM 4.6, especially with the super cheap plans that they offer, I don't think you can go back. I won't be using Sonnet 4.5 anymore unless there is a significant improvement. It just feels like they first nerfed Sonnet 4, and then brought the original checkpoint back with Sonnet 4.5. So, yeah. I can't believe they have the audacity to not make the models cheaper over the last two years, while only increasing performance by like 10% and just training on the benchmarks. I know that it's a good model, but the charge of this model is not something I would recommend anyone to use. It is not worth the price you're paying. And doing it more will only make them more confident to charge even higher. By using something like GLM, you are supporting open-weight models. So, yeah, I'm not happy with Sonnet, but I am super happy with GLM 4.6. And the current AI coder of choice for me is surely GLM 4.6. They listen to developer and user feedback and work on real use cases. Overall, it's pretty cool. Anyway, share your thoughts below and subscribe to the channel. You can also donate via Super Thanks option or join the channel as well and get some perks. I'll see you in the next video. Bye. I think you missed this:"
        }
    },
    {
        "id": "O6i3xBwYjJE",
        "title": "How WW2 Fixed WW1’s Communication Failures - Sarah Paine",
        "content": "",
        "url": "https://www.youtube.com/watch?v=O6i3xBwYjJE",
        "publishDate": "2025-09-30T22:16:12Z",
        "author": "Dwarkesh Patel",
        "sourceType": "youtube",
        "sourceName": "Dwarkesh Patel YouTube Channel",
        "metadata": {
            "channelId": "UCXl4i9dYBrFOabk0xGmbkRA",
            "thumbnailUrl": "https://i.ytimg.com/vi/O6i3xBwYjJE/hqdefault.jpg",
            "transcription": "In World War I, there were exactly two conferences trying to coordinate things among the Entente powers, and they're the December 1915 and November 1916 Chantilly Conferences. And all they are are the military heads. Well, in World War II, if you look at the coordination, it begins even before the United States is in the war with the ABC Staff Talks, and then the Atlantic Conference, which yields the Atlantic Charter, which is talking about what war objectives are, unconditional surrender of Germany and also what the postwar situation is going to look like. There's a combined command of U.S. and British forces. We have offices in each other's capitals, but we're also coordinating with the Russians. It's a completely different event from World War I, when Russia falls out of the Entente because there are bread riots in St. Petersburg, and Russia could not supply its troops with adequate armaments. Well, that's not going to happen in World War II. Russia comes with a really large army."
        }
    },
    {
        "id": "SvIJ-BIAPNI",
        "title": "The Art of Making AI Wrapper: Context Engineering Explained",
        "content": "Engineer the perfect AI outputs with HubSpot's FREE resource! https://clickhubspot.com/420af2 In this video, we'll be digging into ...",
        "url": "https://www.youtube.com/watch?v=SvIJ-BIAPNI",
        "publishDate": "2025-09-30T16:12:31Z",
        "author": "bycloud",
        "sourceType": "youtube",
        "sourceName": "bycloud YouTube Channel",
        "metadata": {
            "channelId": "UCgfe2ooZD3VJPB6aJAnuQng",
            "thumbnailUrl": "https://i.ytimg.com/vi/SvIJ-BIAPNI/hqdefault.jpg",
            "transcription": "What we usually refer to as ChatGPT wrappers, tend to simplify a rather intricate system and dismiss the difficulty of making AI practical. Because in the process of building these types of systems, not only do you need to find a way to save yourself the most money, but also charge people as much money as possible. I mean, make sure LLM don't forget or execute unwanted things, while being fed hundreds of other instructions by both the system and the user. So today, let's look at this new fancy buzzword called context engineering, which is the art of filling the context window with just the right information for LLMs. This can include giving the right instructions, explanations, tool to use, RAG, multimodal data, few-shot examples, history compacting, summarization and more. As there is too little room for errors if you want LLMs to have the optimal performance. But before we dive into it, if you have ever felt like ChatGPT sometimes jumps between mid and magic, there's actually a way to make your prompt engineering much more consistent. In this HubSpot's free resource called Advanced ChatGPT Prompt Engineering: From Basic to Expert in 7 Days, you will learn how to stop getting mediocre AI outputs and start getting reliable and professional results every time. So in one focused week, you will learn how to have AI to give you well-engineered outputs. It starts with a clean zero-shot formula to upgrades vague prompts into reliable results, then moves into frameworks like ROSES, so you're designing systems, not throwing questions at the wall. My favorite section is how it teaches you how to calibrate properly with few-shot examples and make thinking explicit with chain of thought. On top of utilizing another framework called the REASON framework, so complex tasks become auditable and consistent. They also discussed this lockdown structure called the FORMAT framework as it can cut work by helping you understand how to specify exactly what the outputs should look like. With the addition of teaching you how to add error resistance and bio-mitigation systems, the model results would stay accurate, scoped and balanced, even on tricky subjects. By the end, you will have modular templates, reusable processes and format modules, with you being able to design your own signature framework and iterate using the improve cycle backed by a clean performance metrics. So if you're ready to step up your work efficiency with AI, check it out using the link down in the description and thank you HubSpot for sponsoring this video. Anyways, if prompt engineering is finding the best prompt to extract the best performance out of an LLM from a user, context engineering would ultimately be providing the best system context that can be passed into LLMs to get the best performance for the user. And whether the LLM wrapper is good or not, solely relies on how the devs incorporate the context. For a basic chatbot, this is relatively simple. You have a dialogue, then just pass the entire history into the context and feed it to the LLMs. But when it comes to what ChatGPT can do now, like inputting images, having an interactive canvas, search the web or perform its latest function, agent mode, building the right context for the AI to have the best performance makes it even more difficult. Which I have my share of humbling experience while building findmypapers.ai. But of course, what I have experienced is definitely too shallow to share with you all, so I'll actually be referring to a few developer blocks mainly from Manus, Chroma and Cognition, which I found highly resonating. Starting with arguably the most important idea of context engineering being, how do you be as efficient as possible? Because in reality, a consumer only cares about how cheap your service would cost and how long it would take. So besides the model that is being used to wrap your service, there is no other thing that would determine both your speed and your cost then the KV cache. But what is a KV cache? So as the model reads every new token, it calculates the attention value, which will numerically show the relationship of a token to all other existing tokens that it has already read. After proceeding every token, it can be stored within something called a KV cache and be reused as long as all the previous tokens are unchanged. So on a multi-turn conversation, only the new dialogues are needed for extra processing, while the KV cache of the older context can just be reused for the LLM to generate an answer. And the difference between using and not using the KV cache is huge. If you don't have the KV cache, every new turn of conversation will snowball the cost. So if your first conversation with the LLM has 5,000 tokens in total, and your next turn of conversation has also 5,000 tokens, you would be charged 15,000 worth of tokens generated. But if the KV cache is used and achieved what people call a cache hit, the GPUs wouldn't have to spend the time of recalculating the attention for the first conversations, the model servers would usually charge you 1/10th of the price for the cache hit tokens. So it would more likely to cost the price of 10,500 tokens instead. The cost also scales pretty crazy without hitting the KV cache. For example, a third conversation with 5,000 tokens would cost 30,000 tokens, while hitting KV cache would only cost around 16,000 tokens. Which means the longer you use while reusing KV cache, the more money you would save. But the specific KV cache policy may differ between API providers, along with different discounts and constraints. And sometimes, your API services wouldn't automatically have cache enabled or require you to use the same GPU session, which will prevent you from hitting the KV cache. You have to make sure it'll hit on the hardware side of things. So the majority of the context engineering philosophy is surrounded by the idea of hitting the KV cache. Which brings us to the first tip from Manus AI's developer blog. Design around KV cache. For example, there are cases, especially like AI agents, where supplying the model the current time is extremely tempting. But if you really think about it, would there be any actual usefulness to that? So in case where you want to reuse the system prompt, but you have a variable like time that is precise down to the seconds, which you build into the context, then your KV cache would definitely not hit. The solution for this is pretty straightforward, just make your context append-only, while avoiding things like modifying previous actions or content. But what if you really do need to have the time down to seconds in the prefix, or what if the user really want to use a specific tool that dynamically updates the prefix? Is there really no way out? Well, Manus proposed a pretty interesting idea where you mask the context instead of removing them, that would indirectly lets you achieve this. More specifically, in agentic applications where you have a bunch of tools in the context that lets the user or the agents to choose from, you can't really keep all the tools in the context once it's full as it'll distract the LLM, right? But you also can't dynamically pick the tools either, as that will destroy the KV cache. So they would preserve the complete tool definitions in its context and have the inference layer ignore those unrelated actions by zeroing out the logits for those tools. For example, let's say you give your agent has the access to both browser_navigate, browser_click, shell_execute and database_query tools. Instead of removing the shell and database tools when the user asked to browse Google, you keep all four tool definitions in context, but mask the database and shell prefix during logic processing. So when the model generates the tool call, it can only complete with navigate or click. It'll never accidentally hit the other actions and you don't even need to touch the cached context. All you need to do is have a naming convention to support this pattern and utilize token logits to constrain the actions. This is available for OpenAI's API under the logit bias parameter which you can pass in. You can also use the same technique to handle your time stamp problem. Let's say you really need that time stamp to the second functionality, then instead of putting variable time stamps in your system prompt, you can create get_current_time tool that's always available, and the tool itself only appends the context instead of modifying them, saving you a bit of KV cache. Which brings us the next part of context engineering, designing AI agents. If appending is the most budget efficient, how about the cool new multi-agents that do things for you in parallel? Well, actually, other than multi-agent being less budget efficient, a true parallel multi-agent is just not an ideal setup to do performance-wise. From the current most valuable company that is focusing on making agentic software engineers, Cognition, they published a blog about their insights on how to not build a multi-agent system. Because once a subagent stops sharing the same context, miscommunication would start to occur. In actual software development, there are many ways to implement the same thing, and the interpretation may also be different, even if subgoals themselves seem pretty clear cut. From small things like which type of yellow you are using for the background to big design choices like how the classes are defined. But if you have a parallel multi-agent setup, both subagents cannot see what each other are doing, so it's easy to end up being inconsistent. Well, what if we just have a third model to figure things out then? That is of course, the ideal solution. Like for humans, we would have one person or like one senior that would take the key decisions, right? However, the capabilities of AI in 2025 so far has not shown that this is possible. The system it creates would be too fragile, as stated by Cognition devs. So the only reliable solution of having a single continuous context window where the agent does subtasks one by one may still be the best choice, at least in 2025. The only reason that you would create subagents to run in parallel right now, like Claude Code is if there's a small task that does not need to remain in the context history of the main agent, which can help you save some context space. But here comes another problem. If an agent performs many different actions, access webpages, read PDFs, and have a chain of thought for a subtask, the context would rot, as we already know the more we fill up the context window, the worse the model will perform. So Manus AI proposed a simple way to side step this problem, which is to use a to-do list or generate a summary as the agent progresses. They refer to this as attention manipulation. Because by always updating a to-do list, the LLM automatically recites the tasks it needs to perform, preventing the model forgetting the earlier content or the goals and plan as the context window is being filled up. There's also another company called Chroma that has published a more detailed blog outlining the more precise effects of context rot, showing the problems context windows would have under a more practical setting. The experiment setup is like this. They hide a question's answer, aka a needle, within a haystack that is just a long passage. Then they ask the LLM about the question to see if it can find the answer. But existing LLMs can already ace this setup pretty easily. So they introduce a small variation that is adding a distractor. This distractor may look similar to the needle, but is actually semantically different. And by adding a single distractor, it reduces the performance by quite a bit. But what's really destructive is when there are four other distractors. The performance would tank even more across the board, and the more tokens there are, the worse it gets. And this is a more devastating problem for agentic applications, because you often would send an LLM to perform repeated tasks, like regenerating the to-do list, for example. And since LLMs are insanely good at in-context learning, this would also become a double-edge sword once it needs to perform repetitive tasks or make similar decisions. A good example of this is reviewing a batch of resumes. When all the content looks the same, the agent might repeat actions and accidentally falls into a rhythm. This would create a drift, over-generalization, or sometimes hallucination. So pretty much accidentally getting few-shottered by yourself. To prevent that, you either use a context with a clean slate or just introduce some variations or randomness, like having it read resumes in different ways every one or two times. But if a process genuinely made a mistake, it is usually better to keep it in the context and retry instead of creating a completely new attempt. As the researchers at Manus found that when the model sees a failed action and the resulting observation or stack trace, it implicitly updates its internal beliefs. This shifts its prior away from similar actions, reducing the chance of repeating the same mistake. And lastly, if you can fill up the context window as little as possible, it'll always be a win. A great way to achieve this is utilizing the external storage. If you need to read a website, while after reading it, replace the content with a link, a summary, and the tool used, which can save quite a lot of context window. When the application needs it later on, it can just check the summary instead of storing all the extra web data. Pretty similar to the idea of a modular sub-agent Claude Code uses, which we talked about. And this will save you a lot of tokens if the agent or the application you make has a multi-turn function. Because the amount of tokens that a large document has is not a joke. So yeah, that's a wrap on wrapping LLMs. I highly suggest reading the original blogs if you want to learn more in depth about context engineering. And definitely check out my attempt at context engineering, aka findmypapers.ai. You can find AI paper semantically and deploy scouts to keep yourself on tap for the latest research paper published by your favorite institutions. And thank you guys for watching, a big shout out to Andrew Lescelius, Chris Ledoux, Deagan, Nous Research, Kainan, Robert Zawiasa, Louis Muk, Ben Shaener, Marcelo Ferreira, Zyan Sheep, Poof n' Inu, DX Research Group, and many others that support me through Patreon or YouTube, follow me on Twitter if you haven't, and I'll see you all in the next one."
        }
    }
]