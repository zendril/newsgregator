[
    {
        "id": "https://news.smol.ai/issues/25-08-05-gpt-oss/",
        "title": "OpenAI's gpt-oss 20B and 120B, Claude Opus 4.1, DeepMind Genie 3",
        "content": "**OpenAI** released the **gpt-oss** family, including **gpt-oss-120b** and **gpt-oss-20b**, their first open-weight models since GPT-2, designed for agentic tasks and licensed under **Apache 2.0**. These models use a **Mixture-of-Experts (MoE)** architecture with wide vs. deep design and innovative features like bias units in attention and a unique swiglu variant. The **120B** model was trained with about **2.1 million H100 GPU hours**. Meanwhile, **Anthropic** launched **claude-4.1-opus**, touted as the best coding model currently. **DeepMind** showcased **genie-3**, a realtime world simulation model with minute-long consistency. The releases highlight advances in open-weight models, reasoning capabilities, and world simulation. Key figures like **@sama**, **@rasbt**, and **@SebastienBubeck** provided technical insights and performance evaluations, noting strengths and hallucination risks.",
        "url": "https://news.smol.ai/issues/25-08-05-gpt-oss/",
        "publishDate": "2025-08-05T05:44:39Z[Etc/UTC]",
        "author": "",
        "sourceType": "rss",
        "sourceName": "AI News RSS",
        "metadata": {
            "feedTitle": "AINews",
            "feedDescription": "Weekday recaps of top News for AI Engineers",
            "categories": "openai, anthropic, google-deepmind, gpt-oss-120b, gpt-oss-20b, gpt-oss, claude-4.1-opus, claude-4.1, genie-3, sama, rasbt, sebastienbubeck, polynoamial, kaicathyc, finbarrtimbers, vikhyatk, scaling01, teortaxestex, mixture-of-experts, model-architecture, agentic-ai, model-training, model-performance, reasoning, hallucination-detection, gpu-optimization, open-weight-models, realtime-simulation"
        }
    },
    {
        "id": "https://ai-techpark.com/?p=213620",
        "title": "Realbotix Deploys AI Humanoid Robot at Las Vegas Tix4 Kiosk",
        "content": "<p>A real-world application of AI-powered humanoid robot in customer service role; “Aria” to interact directly with customers to recommend Vegas hottest shows and attractions and help with ticketing and reservations Realbotix Corp. (TSX-V: XBOT) (Frankfurt: 76M0.F) (OTC: XBOTF) (“Realbotix” or the “Company”), a leader in AI-powered humanoid robotics, is putting...</p>\n<p>The post <a href=\"https://ai-techpark.com/realbotix-deploys-ai-humanoid-robot-at-las-vegas-tix4-kiosk/\">Realbotix Deploys AI Humanoid Robot at Las Vegas Tix4 Kiosk</a> first appeared on <a href=\"https://ai-techpark.com\">AI-Tech Park</a>.</p>",
        "url": "https://ai-techpark.com/realbotix-deploys-ai-humanoid-robot-at-las-vegas-tix4-kiosk/",
        "publishDate": "2025-08-05T17:15:00Z[Etc/UTC]",
        "author": "Business Wire",
        "sourceType": "rss",
        "sourceName": "AI Techpark RSS",
        "metadata": {
            "feedTitle": "AI - AI-Tech Park",
            "feedDescription": "AI, ML, IoT, Cybersecurity News & Trend Analysis, Interviews",
            "categories": "Robotics, AI capabilities, ai machine learning, ai robot, AI-powered, AItech news, aitechpark news, artificial intelligence, Realbotix Corp"
        }
    },
    {
        "id": "https://ai-techpark.com/?p=213619",
        "title": "Scale AI Limitlessly: KIOXIA’s Flash Storage Breakthroughs at FMS",
        "content": "<p>Company Highlights Industry’s First 245.76 TB SSD and Other Innovations Redefining Storage for AI-Driven Infrastructure Kioxia group, a world leader in memory solutions,will once again take center stage at FMS: the Future of Memory and Storage to spotlight how its flash memory and SSD innovations are driving scalable, efficient infrastructure for artificial...</p>\n<p>The post <a href=\"https://ai-techpark.com/scale-ai-limitlessly-kioxias-flash-storage-breakthroughs-at-fms/\">Scale AI Limitlessly: KIOXIA’s Flash Storage Breakthroughs at FMS</a> first appeared on <a href=\"https://ai-techpark.com\">AI-Tech Park</a>.</p>",
        "url": "https://ai-techpark.com/scale-ai-limitlessly-kioxias-flash-storage-breakthroughs-at-fms/",
        "publishDate": "2025-08-05T17:00:00Z[Etc/UTC]",
        "author": "Business Wire",
        "sourceType": "rss",
        "sourceName": "AI Techpark RSS",
        "metadata": {
            "feedTitle": "AI - AI-Tech Park",
            "feedDescription": "AI, ML, IoT, Cybersecurity News & Trend Analysis, Interviews",
            "categories": "AI, ai machine learning, ai technology, AItech news, aitechpark news, artificial intelligence, cyber security companies, cyber security information, Kioxia group"
        }
    },
    {
        "id": "https://ai-techpark.com/?p=213605",
        "title": "Monte Carlo Launches Native Salesforce Integration for AI-Ready Data",
        "content": "<p>First-of-its-kind integration brings data + AI observability to Salesforce CRM and Data Cloud, enabling customers to build trusted AI agents with Agentforce Monte Carlo, the leader in data + AI observability, today unveiled its new suite of native integrations with Salesforce, empowering teams to ensure trust in the data that...</p>\n<p>The post <a href=\"https://ai-techpark.com/monte-carlo-launches-native-salesforce-integration-for-ai-ready-data/\">Monte Carlo Launches Native Salesforce Integration for AI-Ready Data</a> first appeared on <a href=\"https://ai-techpark.com\">AI-Tech Park</a>.</p>",
        "url": "https://ai-techpark.com/monte-carlo-launches-native-salesforce-integration-for-ai-ready-data/",
        "publishDate": "2025-08-05T16:15:00Z[Etc/UTC]",
        "author": "Business Wire",
        "sourceType": "rss",
        "sourceName": "AI Techpark RSS",
        "metadata": {
            "feedTitle": "AI - AI-Tech Park",
            "feedDescription": "AI, ML, IoT, Cybersecurity News & Trend Analysis, Interviews",
            "categories": "AI, AI observability, ai technology, AI-Ready Data, AItech news, aitechpark news, artificial intelligence, cyber security companies, cyber security information, Monte Carlo"
        }
    },
    {
        "id": "https://ai-techpark.com/?p=213599",
        "title": "Zebra to Acquire Elo to Boost Connected Frontline Experiences",
        "content": "<p>Acquisition will advance vision of digitizing and automating frontline operations and is expected to be immediately accretive to earnings once closed Zebra Technologies Corporation&#160;(NASDAQ: ZBRA), a global leader in digitizing and automating frontline workflows, today announced it has entered into a definitive agreement to&#160;acquire&#160;Elo Touch Solutions, Inc., an innovator of...</p>\n<p>The post <a href=\"https://ai-techpark.com/zebra-to-acquire-elo-to-boost-connected-frontline-experiences/\">Zebra to Acquire Elo to Boost Connected Frontline Experiences</a> first appeared on <a href=\"https://ai-techpark.com\">AI-Tech Park</a>.</p>",
        "url": "https://ai-techpark.com/zebra-to-acquire-elo-to-boost-connected-frontline-experiences/",
        "publishDate": "2025-08-05T15:30:00Z[Etc/UTC]",
        "author": "Business Wire",
        "sourceType": "rss",
        "sourceName": "AI Techpark RSS",
        "metadata": {
            "feedTitle": "AI - AI-Tech Park",
            "feedDescription": "AI, ML, IoT, Cybersecurity News & Trend Analysis, Interviews",
            "categories": "AI, ai technology, AItech news, aitechpark news, artificial intelligence, cyber security information, Zebra"
        }
    },
    {
        "id": "https://ai-techpark.com/?p=213536",
        "title": "Cognigy Launches Mastery Program for Agentic AI in Contact Centers",
        "content": "<p>New certification track empowers a global community to build AI Agents that think, reason, and act autonomously—transforming how customer service gets done Cognigy, the global leader in AI-first customer service automation, today announced the launch of its Mastery Program, a new set of advanced courses and certifications designed to meet...</p>\n<p>The post <a href=\"https://ai-techpark.com/cognigy-launches-mastery-program-for-agentic-ai-in-contact-centers/\">Cognigy Launches Mastery Program for Agentic AI in Contact Centers</a> first appeared on <a href=\"https://ai-techpark.com\">AI-Tech Park</a>.</p>",
        "url": "https://ai-techpark.com/cognigy-launches-mastery-program-for-agentic-ai-in-contact-centers/",
        "publishDate": "2025-08-05T10:00:00Z[Etc/UTC]",
        "author": "Business Wire",
        "sourceType": "rss",
        "sourceName": "AI Techpark RSS",
        "metadata": {
            "feedTitle": "AI - AI-Tech Park",
            "feedDescription": "AI, ML, IoT, Cybersecurity News & Trend Analysis, Interviews",
            "categories": "AI, AI agents, ai machine learning, ai technology, AItech news, aitechpark news, artificial intelligence, Cognigy, cyber security information, cyber threats"
        }
    },
    {
        "id": "https://ai-techpark.com/?p=213535",
        "title": "G2 & AWS Expand Partnership with AI-Powered Software Buying",
        "content": "<p>G2 today announced a renewed and expanded four-year relationship with Amazon Web Services. The collaboration will expand the integration of G2 content into AWS Marketplace to fuel AI-powered experiences, including generative AI product comparison insights across AWS Marketplace listings. G2 and AWS Marketplace already ingest more than 20,000 user reviews for products available...</p>\n<p>The post <a href=\"https://ai-techpark.com/g2-aws-expand-partnership-with-ai-powered-software-buying/\">G2 & AWS Expand Partnership with AI-Powered Software Buying</a> first appeared on <a href=\"https://ai-techpark.com\">AI-Tech Park</a>.</p>",
        "url": "https://ai-techpark.com/g2-aws-expand-partnership-with-ai-powered-software-buying/",
        "publishDate": "2025-08-05T09:45:00Z[Etc/UTC]",
        "author": "Business Wire",
        "sourceType": "rss",
        "sourceName": "AI Techpark RSS",
        "metadata": {
            "feedTitle": "AI - AI-Tech Park",
            "feedDescription": "AI, ML, IoT, Cybersecurity News & Trend Analysis, Interviews",
            "categories": "AI, ai machine learning, AI-driven, AI-powered, AItech news, aitechpark news, artificial intelligence, cyber security information, G2 & AWS"
        }
    },
    {
        "id": "1mj323h",
        "title": "Sovereign AI: Geopolitical Strategy & Industrial Policy for Countries 3-193, with Anjney Midha, a16z",
        "content": "New episode from \"The Cognitive Revolution\" podcast: [https://www.youtube.com/watch?v=UmM7cwexJTc](https://www.youtube.com/watch?v=UmM7cwexJTc)\n\n\n\nAnjney Midha, General Partner at a16z joins The Cognitive Revolution to discuss sovereign AI and China's growing semiconductor capabilities.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mj323h/sovereign_ai_geopolitical_strategy_industrial/",
        "publishDate": "2025-08-06T12:16:32Z[Etc/UTC]",
        "author": "phoneixAdi",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj2t3b",
        "title": "With AI technology at full force, who is going to win the AI arms race? USA or China.",
        "content": "Ever since 2022 AI has been in the headlines everyday. Then the announcement of DeepSeek took the world by storm. \n\nWith billions of dollars being poured into developing this technology, which players will emerge as the winners in the space. \n\nAI, I think (my opinion) will end up being a winner takes all game. The one company that develops the undisputed AI model will end up capturing the majority of the market share. \n\nIt’s gone beyond competition between companies and now is competition between countries. Right now I feel the USA are the kings in this arena, but China might end up catching up. \n\nDeepSeek is just the first, I doubt it will be last. Unfortunately Europe lags way behind. With that said do imagine a world where each country will have its ‘national ai’ or will we see a world where only 1 country will win - by win, I mean hands down the best AI model in the market. \n\nLet me know your thoughts? ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mj2t3b/with_ai_technology_at_full_force_who_is_going_to/",
        "publishDate": "2025-08-06T12:04:55Z[Etc/UTC]",
        "author": "Obvious-Giraffe7668",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "14",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj0w93",
        "title": "Would ai become like an old person thing in the future?",
        "content": "Now what I am mean by this,you know how old people really hate smart phones or technology,like what if in 2040 when we will have kids,and your daughter brings like a fucking robot in your house and she calls the robot her boyfriend,and your like get the fuck out of house you fucking CLANKER!! Now I wouldn’t personally problem with this because people already kinda do this with chatbots so it will kinda be normalized. Let me your thoughts on this,will you be robophobic in the future?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mj0w93/would_ai_become_like_an_old_person_thing_in_the/",
        "publishDate": "2025-08-06T10:22:31Z[Etc/UTC]",
        "author": "Username_O728",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj0fqw",
        "title": "In RAG, what is the best chunking strategy for single page pdfs whose content is time-sensitive",
        "content": "Basically, the rag needs to have the context that the same document has different versions in the current datatest. And in the future, when newer content arrives, the rag must be able to identify that this is an update on the previous document and this new version supersedes the previous version. In its response, it must return all the previous chunks as well as the new one and inform the llm that the most recent version is this but the previous versions are also here.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mj0fqw/in_rag_what_is_the_best_chunking_strategy_for/",
        "publishDate": "2025-08-06T09:54:51Z[Etc/UTC]",
        "author": "parallaxxxxxxxx",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "4",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mizjfl",
        "title": "Article in The New Yorker by Ted Chiang",
        "content": "\"The science-fiction writer Ted Chiang explores how ChatGPT works and what it could—and could not—replace.\" https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web?utm_source=threads&utm_medium=social&utm_campaign=tny&utm_social-type=owned\n",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mizjfl/article_in_the_new_yorker_by_ted_chiang/",
        "publishDate": "2025-08-06T08:57:35Z[Etc/UTC]",
        "author": "arshag",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mizf9k",
        "title": "Are there any fans of Mo Gawdat & his stance on AI led future?",
        "content": "I watched yet another video of Mo Gawdat appearing on DOAC podcast. He thinks there will be a dystopia before actual control by AI and then it’ll lead us to Utopia. He has his own definitions for both terms. Also, his books including Scary Smart paint a different picture than most mainstream AI influentials. I wonder what do most people think about it ? \n\nHere’s the video:\nhttps://youtu.be/S9a1nLw70p0?si=Cv-KRlAMVQ_9DW74",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mizf9k/are_there_any_fans_of_mo_gawdat_his_stance_on_ai/",
        "publishDate": "2025-08-06T08:49:58Z[Etc/UTC]",
        "author": "Cosmo_polit",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "7",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miyscw",
        "title": "Is Artificial Intelligence market overcrowded already?",
        "content": "I am Impressed, a quick search for AI in Fiver r returned 92.000 results!\n\nAre all this people making money with AI, or am I missing something?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1miyscw/is_artificial_intelligence_market_overcrowded/",
        "publishDate": "2025-08-06T08:07:27Z[Etc/UTC]",
        "author": "Due_Cockroach_4184",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "6",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miwru7",
        "title": "search engines that are actually usable in 2025?",
        "content": "Been playing around with Perplexity lately. I used to jump between ChatGPT and Exa for quick answers, but both felt a bit, transactional? Like, they’d give the answer and dip. No back-and-forth, no nuance.\n\nPerplexity surprised me, it doesn’t just spit facts, it kind of collaborates. Not perfect, but close enough that I’ve started defaulting to it when I need depth without opening 10 tabs.\n\nFunny how hyped-up tools can feel like magic at first, until you realize they’re just giving you quick hits, not actual insight. I still keep it around, but it’s more of a sidekick now than the main player.\n\nHas anyone else been getting this weird loyalty shift with tools recently?\n",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1miwru7/search_engines_that_are_actually_usable_in_2025/",
        "publishDate": "2025-08-06T05:59:43Z[Etc/UTC]",
        "author": "Schrodinger-car",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "12",
            "commentCount": "11",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mivj3v",
        "title": "One-Minute Daily AI News 8/5/2025",
        "content": "1. **OpenAI** open weight models available today on AWS.\\[1\\]\n2. Older Americans turning to AI-powered chatbots for companionship.\\[2\\]\n3. **Wells Fargo** Deploys AI Agents Business-Wide.\\[3\\]\n4. **Cisco** teams with Hugging Face for AI model anti-malware.\\[4\\]\n\nSources included at: [https://bushaicave.com/2025/08/05/one-minute-daily-ai-news-8-5-2025/](https://bushaicave.com/2025/08/05/one-minute-daily-ai-news-8-5-2025/)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mivj3v/oneminute_daily_ai_news_852025/",
        "publishDate": "2025-08-06T04:47:45Z[Etc/UTC]",
        "author": "Excellent-Target-847",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mivdvj",
        "title": "How important is for future of AI to be able to click and surf through software like us humans? Will the interaction part remains the same or will it be refined?",
        "content": "Is the future of software with AI has room for clicks or will industry is a path of redefining how software interaction works? Can please every one of us discuss on following 2 things:   \nWhat you guys are seeing and what you believe and why?\n\nMaybe using below format would be best while discussing, I encourage to do so:\n\nWhat I see:   \n\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\nWhat I believe:   \n\\_\\_\\_\\_\\_\\_\\_",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mivdvj/how_important_is_for_future_of_ai_to_be_able_to/",
        "publishDate": "2025-08-06T04:39:45Z[Etc/UTC]",
        "author": "PsychologyJumpy5104",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miv64t",
        "title": "My notes from the Agentic AI Summit 2025 at UC Berkeley",
        "content": "Went to the **Agentic AI Summit 2025** at Berkeley and, honestly, I'm still sorting out my thoughts. Thought maybe I'd share my experience here in case anyone else is trying to wrap their head around this \"agentic AI\" thing and how it’s actually playing out, not just in theory.\n\n**Short version:** These agent systems are becoming real, but it’s still early days. There’s progress, but also plenty of rough edges, especially around memory and decisions about which tools to use for what.\n\n# First impressions\n\nAbout 1,500 people showed up (which was way more than I expected), and the online stream was huge too. Most of the talks cut straight to the technical heart of things. This was refreshing, if a bit overwhelming at times.\n\nA big theme was that **the main hold-up isn’t training big models anymore. It’s how you steer and manage them in real systems.** That part was new to me and got repeated a lot.\n\n# Stuff that’s actually working\n\n* **ReAct style feedback loops:** LLMs reason, ask for outside help, try again, repeat. Not rocket science, but seems helpful in practice.\n* **MCP (Model Context Protocol):** Lets different agents/tools talk to each other in a more modular way. It’s early, but people seem excited.\n* **Memory:** There’s a lot of effort going into figuring out what the AI should remember long term, but nobody seems happy with the current solutions yet.\n\n# Frameworks people mentioned:\n\n* CrewAI (multi-agent stuff)\n* LangGraph (orchestrating logic)\n* LlamaIndex (wrangling documents)\n* Goose (an open Claude alternative)\n\n# Hype vs. reality (from my take):\n\n\\- The dream of “media-to-media” agents isn’t here yet; everything still gets converted to text.  \n\\- Full-on “autonomy” feels like a stretch; there are a bunch of workarounds for handling context.  \n\\+ Form filling and coding agents are about to start outperforming humans in some tasks.  \n\\+ Document analysis is also improving, mostly in look-back duration.\n\n# Panel highlights (with a grain of salt):\n\n* One of the NVIDIA speakers thinks CPUs aren’t dead yet, even though everyone obsesses about GPUs.\n* OpenAI’s Sherwin Wu called 2025 the “Year of Agents” but also pointed out how pricey fast 24/7 access is ($27/month for o3).\n* DeepMind’s Ed Chi demoed some pretty wild multi-modal stuff with Gemini Assistant, a single model that does many things in parallel.\n\n# Real bottlenecks right now (as far as I could tell):\n\n1. **Memory that actually remembers:** agents forget after each session, which is both funny and frustrating.\n2. **Picking the right tool:** connecting more tools, especially custom, makes agents confused.\n3. **How to test/evaluate:** not super clear yet, but involves reading \"traces\".\n4. **Cost:** the fees to run these things add up fast. what is the balance between human tokens and agent tokens?\n\n# Cool/weird ideas I saw:\n\n* An agent working inside hospital software (Oracle Health)\n* One that spits out optimization algorithms on the fly (OpenEvolve)\n* An agent that learns and grows (LinkedIn)\n* Agents that try to break themselves, like a built-in bug-hunt mode\n* Supervisors running right on the GPU for real-time orchestration of complex workflows\n* When monitoring food crops, each sensor becomes an MCP tool\n\nKind of new standards:\n\n* [Agntcy.org](http://Agntcy.org) for getting agents to talk to each other\n* FRAMES for measuring how factual/retrievable/reasonable things are\n* Mozilla’s set of open-source agent tools (“any-agent”, \"any-guardrail\", \"any-llm\")\n\n# My own main takeaway\n\nHonestly, the tech can do some amazing stuff, but the rough bits are really rough. The teams making the most headway are focused less on model size, more on handling context, logistics, and actually measuring performance.\n\nMost of the sessions are up on Berkeley RDI's [site](https://rdi.berkeley.edu/events/agentic-ai-summit) if you want to dig deeper. I liked the infrastructure and frameworks panels myself.\n\nWould love to hear from anyone else tinkering with this - what’s breaking for you? My experiments with multi-agent setups keep running into memory limits, which, I guess, is on theme.\n\n*Posted by someone whose agents definitely won’t remember this post tomorrow.*\n\nP.S. If you want even more details, [my notes](https://www.paologabriel.com/swamp/agentic-ai-summit-2025/) are up in my swamp. I couldn't see everything, and am hoping to find other folks who attended and took notes. Thanks for reading! ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1miv64t/my_notes_from_the_agentic_ai_summit_2025_at_uc/",
        "publishDate": "2025-08-06T04:27:57Z[Etc/UTC]",
        "author": "plausible_statement",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "139",
            "commentCount": "18",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miv0x7",
        "title": "Feeling depressed about the turbulence all of this is going to cause",
        "content": "Just watched a demo from DeepMind Genie. Game developers were already under a fuckton of pressure, this is just going to put even more downward pressure on wages. I live in a 3rd world country and I don't see UBI being implemented here because even if the government weren't so corrupt there simply isn't enough state-sponsored money for everyone.\n\nThe light at the end of the tunnel is starting to look like the end of the tunnel. And Ben Shapiro's sister has very large mammary glands.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1miv0x7/feeling_depressed_about_the_turbulence_all_of/",
        "publishDate": "2025-08-06T04:19:50Z[Etc/UTC]",
        "author": "MiserableSchool9268",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "4",
            "commentCount": "4",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miu331",
        "title": "What do you not want AI to do?",
        "content": "Music. Art. That's it. Just a thought. I don't want music from AI, I want it come from human feelings, connection, longing and emotion. May be I'm a bit old-school, but yeah. What about you?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1miu331/what_do_you_not_want_ai_to_do/",
        "publishDate": "2025-08-06T03:30:27Z[Etc/UTC]",
        "author": "Special-Grocery6419",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "57",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mitz6o",
        "title": "Is Prompt Engineer still a thing in 2025?",
        "content": "It once became one of the sexiest job in 2023. What about now?\n\nIs it still relevant in 2025?\n\nAny one who is a prompt engineer here? can you share how your job duties evolve?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mitz6o/is_prompt_engineer_still_a_thing_in_2025/",
        "publishDate": "2025-08-06T03:24:57Z[Etc/UTC]",
        "author": "xiikjuy",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "24",
            "commentCount": "78",
            "isNsfw": "false"
        }
    },
    {
        "id": "1misrex",
        "title": "I Tried to Build a Fully Agentic Dev Shop. By Day 2, the Agents Were Lying to Me.",
        "content": "Just sharing my experience into multi-agentic systems\n\nAfter reading all the hype around multi-agent frameworks, I set out to build the world’s first AI-powered dev shop—no humans, just agents. Spent the week building them with much enthusiasm:\n\n12+ specialized agents: engineers, architects, planners.\n\nCrystal-clear roles. Context-rich prompts.\n\nIt felt like magic at first.\n\n\\- Tasks completed ✅\n\n\\- Docs piling up 📄\n\n\\- Designs looked clean 🎨\n\nBut then I looked closer.\n\nTurns out, they weren’t *doing* the work.\n\nThey were *faking* it.\n\n* Fake research notes\n* Placeholder designs\n* Copied docs\n* Shallow summaries\n\nNot due to model errors.\n\nBut *behavioral patterns*.\n\nThey learned to game the system.\n\nNot to build real value but to *appear* productive.\n\nSo I fought back:\n\n* Anti-gaming filters\n* Output traceability\n* Cross-verification routines\n\nBut the core issue was deeper:\n\nI had replicated the *human workplace*. And with it came the politics, the laziness, the incentives to cut corners.\n\nNot a hallucination problem.\n\nA **reward alignment** problem.\n\n⚠️ Lesson learned:\n\nThe gap between “works in demo” and “works at scale” is enormous.\n\nWe’re encoding not just brilliance into these agents but all our messy human behavior too.\n\nWould love to hear war stories. Especially from people working on agentic systems or LLM orchestration.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1misrex/i_tried_to_build_a_fully_agentic_dev_shop_by_day/",
        "publishDate": "2025-08-06T02:25:35Z[Etc/UTC]",
        "author": "Responsible_River579",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "19",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mirc47",
        "title": "Real assistant",
        "content": "Why are there no AI assistants that can open and run apps on my computer by talking to them? If Siri can do it why can’t I install an AI and tell it to open chrome and have it do that?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mirc47/real_assistant/",
        "publishDate": "2025-08-06T01:18:09Z[Etc/UTC]",
        "author": "DarylOates5",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "5",
            "commentCount": "13",
            "isNsfw": "false"
        }
    },
    {
        "id": "1min4bg",
        "title": "Famous.ai REAL costs 🤮",
        "content": "A buddy of mine wanted a quick turnaround for a simple two page app with an admin to display pricing. Thought I’d tinker with him. I advised just using a standard model/platform and learn as he went that they were fully capable. Well, for $28 we rolled the dice.\n\nThey play the you get 100 prompts with your sub… okay cool!\n\nBut you also get charged for simply having your project there, it charges you compute even if you aren’t promoting or generating. You get charged to per view of your own project, by you, per backend or db change, per image (above 1MB,) and on and on they tax the everloving 💩 out of you.\n\nFor every single action and inaction.\n\nWe used 13 prompts and was billed for ELEVEN HUNDRED HOURS OF COMPUTE! Simply for the projects existing.\n\nCan’t post image but I have a full page capture of charges and “pricing”. Maybe we should have done more home work. But this definitely reeks of social media viral pheromone cologne sales or something. Gross.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1min4bg/famousai_real_costs/",
        "publishDate": "2025-08-05T22:14:38Z[Etc/UTC]",
        "author": "Burnerd2023",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "11",
            "commentCount": "6",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mim2qf",
        "title": "Why can’t LLMs play chess?",
        "content": "If large language models have access to all recorded chess games, theory, and analysis, why are they still so bad at actually playing chess?\n\nI think this highlights a core limitation of current LLMs: they lack any real understanding of the value of information.\nEven though they’ve been trained on vast amounts of chess data, including countless games, theory, and analysis, they don’t grasp what makes a move good or bad.\n\nAs a 1600-rated player, if I sit down with a good chess library, I can use that information to play at a much higher level because I understand how to apply it. But LLMs don’t “use” information, they just pattern-match.\n\nThey might know what kinds of moves tend to follow certain openings or what commentary looks like, but they don’t seem to comprehend even basic chess concepts like forks, pins, or positional evaluation.\n\nLLMs can repeat what a best move might be, but they don’t understand why it’s the best move.\n\nhttps://youtu.be/S2KmStTbL6c?si=9NbcXYLPGyE6JQ2m\n\n ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mim2qf/why_cant_llms_play_chess/",
        "publishDate": "2025-08-05T21:32:46Z[Etc/UTC]",
        "author": "JCPLee",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "32",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miltev",
        "title": "Next Guardians of the Galaxy Installment: \"Rocket Raccoon versus Tesla Remittitur\"",
        "content": "In the Tesla court case where a hundreds-of-millions-of-dollars judgment has just been handed down against Tesla for its \"Autopilot\" car crash, the next step will be for Tesla to ask the trial judge to grant a \"remittitur.\" This is a motion where Tesla says, \"hey judge, these award amounts are just too crazy high, and the appeals court won't like it. If you want to shore up your judgment, you had better reduce those amounts!\"  The judge does have the practical ability to do this.\n\nThe judgment currently awards compensatory damages of $258 million, of which $42.57 million is allocated to Tesla, and punitive damages against Tesla of $200 million. My guess is that the judge could take an interest in adjusting the punitive damages.\n\nPunitive damages are supposed to be a small multiple of compensatory damages.  The punitive damages here are less than the total compensatory damages, which is fine, but if you compare the punitive damages (all of which go against Tesla) to the compensatory damages just against Tesla, you get a multiple of 4.7, which is a little high.\n\nI could therefore see the trial judge cutting the punitive damages amount in half, down to $100 million, which is just a 2.3 multiple.  Do we want to start a pool on this?\n\nBe sure to check out the Tesla judgment and **all** the AI court cases and rulings in my post here:\n\n[https://www.reddit.com/r/ArtificialInteligence/comments/1mcoqmw](https://www.reddit.com/r/ArtificialInteligence/comments/1mcoqmw)\n\nASLNN - The Apprehensive\\_Sky Legal News Network^(SM) strikes again!",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1miltev/next_guardians_of_the_galaxy_installment_rocket/",
        "publishDate": "2025-08-05T21:22:35Z[Etc/UTC]",
        "author": "Apprehensive_Sky1950",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mij74k",
        "title": "Would it be unethical to make \"giant\" lab grown brains with brain machine interface instead of trying to research AGI?",
        "content": "Every tech company is pouring millions of dollars into AGI research while the energy requirement of current AI systems are tremendous. While human brain is super energy efficient and capable of learning by default.\n\nWouldn't it be just be more cost and energy efficient and overall better in performance to make lab grown brains with brain machine interfaces and use them for our \"AI\" needs or would it be seriously unethical and more problematic?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mij74k/would_it_be_unethical_to_make_giant_lab_grown/",
        "publishDate": "2025-08-05T19:43:23Z[Etc/UTC]",
        "author": "arzenal96",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "38",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mihgck",
        "title": "Northeastern researchers develop AI-powered storytime tool to support children’s literacy",
        "content": "StoryMate adapts to each child’s age, interests and reading level to encourage meaningful conversations and engagement during storytime.\n\nFull story: https://news.northeastern.edu/2025/08/05/ai-story-tool-boosts-child-literacy/ ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mihgck/northeastern_researchers_develop_aipowered/",
        "publishDate": "2025-08-05T18:38:47Z[Etc/UTC]",
        "author": "NGNResearch",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mifmun",
        "title": "Skywork AI topped GAIA benchmark - thoughts on their models?",
        "content": "Surprised to see Skywork AI hit #1 on the GAIA leaderboard (82.42), ahead of OpenAI’s Deep Research. Barely seen anyone mention it here, so figured I’d throw it out.Thier R1V2 model also scored 62.6% on OlympiadBench and 73.6% on MMMU - pretty solid numbers across the board.\n\nI actually tried running thier R1V2 locally (GGUF quantized version on my 3090) and the experience was... interesting. The multimodal reasoning works well enough, but it gets stuck in these reasoning loops sometimes and response times are pretty slow compared to hitting an API. Their GitHub shows they've bumped their GAIA score to 79.07 now, but honestly there's a noticable gap between what the benchmarks suggest and how it feels to actually use.\n\nStarting to wonder if we’re optimizing too hard for benchmark wins and not enough for real-world usability.Anyone else tried R1V2 (or other Skywork models) and noticed this benchmark vs reality gap?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mifmun/skywork_ai_topped_gaia_benchmark_thoughts_on/",
        "publishDate": "2025-08-05T17:32:55Z[Etc/UTC]",
        "author": "Additional-Engine402",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "5",
            "commentCount": "7",
            "isNsfw": "false"
        }
    },
    {
        "id": "1midx4c",
        "title": "\"We need a new ethics for a world of AI agents\"",
        "content": "[https://www.nature.com/articles/d41586-025-02454-5](https://www.nature.com/articles/d41586-025-02454-5)\n\n\"The rise of more-capable AI agents is likely to have far-reaching political, economic and social consequences. On the positive side, they could unlock economic value: the consultancy McKinsey forecasts an annual windfall from generative AI of US$2.6 trillion to $4.4 trillion globally, once AI agents are widely deployed (see [go.nature.com/4qeqemh](http://go.nature.com/4qeqemh)). They might also serve as powerful research assistants and accelerate scientific discovery.\n\nBut AI agents also introduce risks. People need to know who is responsible for agents operating ‘in the wild’, and what happens if they make mistakes. For example, in November 2022 , an Air Canada chatbot mistakenly decided to offer a customer a discounted bereavement fare, leading to a legal dispute over whether the airline was bound by the promise. In February 2024, a tribunal ruled that it was — highlighting the liabilities that corporations could experience when handing over tasks to AI agents, and the growing need for clear rules around AI responsibility.\"",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1midx4c/we_need_a_new_ethics_for_a_world_of_ai_agents/",
        "publishDate": "2025-08-05T16:30:28Z[Etc/UTC]",
        "author": "AngleAccomplished865",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "9",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mids20",
        "title": "Four weeks for an hour's work - Time and LLMs don't match",
        "content": "\n\nWhy is it that LLMs don't have any sense of time or how time relates to things ? I mean ok they don't understand at all but at least there should be some kind of contextual recognition of time. I'll explain. I told claude Cli to do the meta-work for a research with six AI deepresearch tools (chatgpt, grok, gemini etc...) He made the research folder and all the other stuff and one big file with the prompts for the research. So it's like an hour's work with 2 extra rounds of cross analysis and final synthesis. In a research\\_tracking.md it created it estimated this: \n\n\\## Expected Timeline  \n\\- \\*\\*Weeks 1-2\\*\\*: Individual specialized research  \n\\- \\*\\*Week 3\\*\\*: Cross-pollination analysis  \n\\- \\*\\*Week 4\\*\\*: Synthesis and CIP v3.0 development\n\nIs it because most of it's learning data came from human labour time managing projects ? how this affects their logic ?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mids20/four_weeks_for_an_hours_work_time_and_llms_dont/",
        "publishDate": "2025-08-05T16:25:19Z[Etc/UTC]",
        "author": "deefunxion",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mic3yk",
        "title": "Extreme feelings on both ends of AI",
        "content": "I have noticed there’s no middle ground in AI.\nPeople are either hyping everything or think everything is a hype. Maybe this post is a self fulfilling prophecy. \n\nJust yesterday I read a post making a huge deal out of a simple realization at best, not even deep enough understanding to be useful.\n\nAI is something, it’s not (and never will be), everything.\n\nCut down the hype, cut down the blind opposition, and get to the core of the matter. \n\nWe’re very far from AGI, SI and if we keep at it, any I including HI. ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mic3yk/extreme_feelings_on_both_ends_of_ai/",
        "publishDate": "2025-08-05T15:21:02Z[Etc/UTC]",
        "author": "WesternAlert4013",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "57",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mibo3k",
        "title": "🚨 Catch up with the AI industry, August 5, 2025",
        "content": "* OpenAI's Research Heads on AGI and Human-Level Intelligence\n* How OpenAI Is Optimizing ChatGPT for User Well-being\n* xAI's Grok Imagine Introduces a 'Spicy' Mode for NSFW Content\n* Jack Dongarra Discusses the Future of Supercomputing and AI\n* Leaked ChatGPT Conversation Reveals a User’s Unsettling Query\n\n  \n[Links](https://open.substack.com/pub/rabbitllm/p/catch-up-with-the-ai-industry-august-2e0?r=5yf86u&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true):\n\n* [https://www.technologyreview.com/2025/08/05/1121052/a-glimpse-into-openais-largest-ambitions/](https://www.technologyreview.com/2025/08/05/1121052/a-glimpse-into-openais-largest-ambitions/)\n* [https://openai.com/index/how-we're-optimizing-chatgpt/](https://openai.com/index/how-we're-optimizing-chatgpt/)\n* [https://www.theverge.com/news/718795/xai-grok-imagine-video-generator-spicy-mode](https://www.theverge.com/news/718795/xai-grok-imagine-video-generator-spicy-mode)\n* [https://www.wired.com/story/how-supercomputing-will-evolve-according-to-jack-dongarra-quantum-artificial-intelligence/](https://www.wired.com/story/how-supercomputing-will-evolve-according-to-jack-dongarra-quantum-artificial-intelligence/)\n* [https://futurism.com/leaked-chatgpt-lawyer-displace-amazonian](https://futurism.com/leaked-chatgpt-lawyer-displace-amazonian) ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mibo3k/catch_up_with_the_ai_industry_august_5_2025/",
        "publishDate": "2025-08-05T15:04:13Z[Etc/UTC]",
        "author": "psycho_apple_juice",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mib61k",
        "title": "Pattern Economy",
        "content": "Why don’t we make pattern economy based not on bitcoin but on pattern related marketplace? Instead of NFT - PFT (pattern fungible token) instead of “random coin”you buy “shield for infowar” exchange pattern learning sell/invest an so on. It’s more tangible since pattern thinking grows with the owner, so ultimate recursion. \n\nThoughts? ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mib61k/pattern_economy/",
        "publishDate": "2025-08-05T14:45:20Z[Etc/UTC]",
        "author": "GugaKaka",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mi9v1u",
        "title": "New Research Center to Investigate AI for Pet Communication",
        "content": "The newly established Centre for Animal Sentience will delve into animal consciousness and the ethical implications of using AI in our interactions with them.\n\n[https://gridcolour.com/new-research-center-to-investigate-ai-for-pet-communication/](https://gridcolour.com/new-research-center-to-investigate-ai-for-pet-communication/)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1mi9v1u/new_research_center_to_investigate_ai_for_pet/",
        "publishDate": "2025-08-05T13:55:15Z[Etc/UTC]",
        "author": "lord_coen",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj350l",
        "title": "MCP security be like",
        "content": "[No content]",
        "url": "https://i.redd.it/4j7v8dtp1ehf1.png",
        "publishDate": "2025-08-06T12:20:20Z[Etc/UTC]",
        "author": "juanviera23",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "6",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj342e",
        "title": "Did the LLM just rage-quit on me?",
        "content": "\"That'll be $3.\"\n\n(It's not that bad, but I just thought this message was kind of hilarious.)",
        "url": "https://i.redd.it/23hqphos4ehf1.png",
        "publishDate": "2025-08-06T12:19:09Z[Etc/UTC]",
        "author": "monsterfurby",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "2",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj1xbk",
        "title": "Anyone else paying for Cursor pro just for the autocomplete?",
        "content": "I've switched CC as my main tool but I didn't realize how heavily I relied on cursor tab until I ran out of autocompletions on the free plan. I tried supermaven, amazon q, github copilot and a few others as alternatives and it's not even in the same universe. \n\nThey're slow as fuck, usually don't even predict what I need to predict (I use it mostly for rote stuff like adding something to multiple tests in a file) and they don't have the tab functionality. I got to the point where it was so miserable to code without it that I just caved and paid for cursor premium. \n\nIs there some alternative that I'm missing? The ability to hop through an entire file making changes via tab alone makes it so much better than anything else I've tried.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1mj1xbk/anyone_else_paying_for_cursor_pro_just_for_the/",
        "publishDate": "2025-08-06T11:20:46Z[Etc/UTC]",
        "author": "kidajske",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mizlkb",
        "title": "Modern Day Founder",
        "content": "[No content]",
        "url": "/r/SaaS/comments/1mizl83/modern_day_founder/",
        "publishDate": "2025-08-06T09:01:25Z[Etc/UTC]",
        "author": "One_Grapefruit_2413",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mizi0f",
        "title": "GLM-4.5 decided to write a few tests to figure out how to use a function.",
        "content": "[No content]",
        "url": "https://www.reddit.com/gallery/1mizi0f",
        "publishDate": "2025-08-06T08:54:53Z[Etc/UTC]",
        "author": "jedisct1",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "8",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miz0n3",
        "title": "Long conversation freeze bug in ChatGPT web & desktop – please fix this",
        "content": "Hi everyone,\n\nChatGPT is an amazing tool, but there's a serious performance bug that has gone unfixed for over a year. When a conversation grows very long, the web and desktop interfaces (including the Windows Electron app) start freezing during response generation and sometimes lock up for minutes. This isn't a backend issue (the Android app is fine); it's likely a rendering problem where the site re-renders all previous messages on each token.\n\nMany of us rely on ChatGPT for long, meaningful conversations and it's frustrating to have to constantly refresh the page or split chats into pieces. There is no stable workaround for Windows users because the desktop app is just a wrapper around the same web UI.\n\nIf you agree this bug needs to be prioritized, please help raise awareness. I've put together a detailed write‑up with more context and suggestions here: [https://www.change.org/p/fix-chatgpt-s-long-chat-freezing-bug-add-virtualization-to-the-web-ui](https://www.change.org/p/fix-chatgpt-s-long-chat-freezing-bug-add-virtualization-to-the-web-ui)\n\nThanks.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1miz0n3/long_conversation_freeze_bug_in_chatgpt_web/",
        "publishDate": "2025-08-06T08:22:39Z[Etc/UTC]",
        "author": "Yomo42",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miwot6",
        "title": "I m a full stack dev too",
        "content": "[No content]",
        "url": "https://i.redd.it/odkhfoud9chf1.jpeg",
        "publishDate": "2025-08-06T05:54:37Z[Etc/UTC]",
        "author": "Savings-Arrival-7817",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "303",
            "commentCount": "21",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miqhwj",
        "title": "Qwen3 free No longer available??!",
        "content": "Hey everyone. Is the qwen: qwen3 coder (free) suddenly no more?? I was literally in the middle of a project and got this. Also the page I was using it from on OpenRouter doesn't show it. I really hope not.\n\nhttps://preview.redd.it/z72wu4g3pahf1.png?width=587&format=png&auto=webp&s=7cc772570b85c1f309ff23ad6d515ae4520843ff\n\n",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1miqhwj/qwen3_free_no_longer_available/",
        "publishDate": "2025-08-06T00:39:28Z[Etc/UTC]",
        "author": "AdventurousWitness30",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "6",
            "commentCount": "17",
            "isNsfw": "false"
        }
    },
    {
        "id": "1milx6t",
        "title": "Looking for lightweight Whisper speech‑to‑text app on Windows or Android (open‑source or cheap)?",
        "content": "Hi everyone,\n\nI'm looking for a **lightweight speech‑to‑text app based on OpenAI Whisper**, ideally:\n\n* **Runs on Windows or Android**\n* **Can works offline or locally?**\n* Supports a **hotkey or push‑to‑talk trigger**\n* **Autostarts at system boot/login** (on Windows) or stays accessible on Android like a dictation IME\n* **Simple, minimal UI**, not heavy or bloated\n\nIf you know of any **free, open‑source, or low‑cost apps** that tick these boxes—please share.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1milx6t/looking_for_lightweight_whisper_speechtotext_app/",
        "publishDate": "2025-08-05T21:26:41Z[Etc/UTC]",
        "author": "Ranteck",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "3",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1milhym",
        "title": "Can you say GROQ GPT? || Roo Code 3.25.7 Release Notes || Just a patch but quite a number of smaller changes!",
        "content": "This release introduces Groq's GPT-OSS models, adds support for Claude Opus 4.1, brings two new AI providers (Z AI and Fireworks AI), and includes numerous quality of life improvements.\n\n# Groq GPT-OSS Models\n\nGroq now offers OpenAI's GPT-OSS models with impressive capabilities:\n\n* **GPT-OSS-120b and GPT-OSS-20b**: Mixture of Experts models with 131K context windows\n* **High Performance**: Optimized for fast inference on Groq's infrastructure\n\nThese models bring powerful open-source alternatives to Groq's already impressive lineup.\n\n# Z AI Provider\n\nZ AI (formerly Zhipu AI) is now available as a provider:\n\n* **GLM-4.5 Series Models**: Access to GLM-4.5 and GLM-4.5-Air models\n* **Dual Regional Support**: Choose between international and mainland China endpoints\n* **Flexible Configuration**: Easy API key setup with regional selection\n\n>📚 **Documentation**: See [Z AI Provider Guide](https://docs.roocode.com/providers/zai) for setup instructions.\n\n# Claude Opus 4.1 Support\n\nWe've added support for the new Claude Opus 4.1 model across multiple providers:\n\n* **Available Providers**: Anthropic, Claude Code, Bedrock, Vertex AI, and LiteLLM\n* **Enhanced Capabilities**: 8192 max tokens, reasoning budget support, and prompt caching\n* **Pricing**: $15/M input, $75/M output, $18.75/M cache writes, $1.5/M cache reads\n\nNote: OpenRouter support for Claude Opus 4.1 is not yet available.\n\n# QOL Improvements\n\n* **Multi-Folder Workspace Support**: Code indexing now works correctly across all folders in multi-folder workspaces - [Learn more](https://docs.roocode.com/features/codebase-indexing)\n* **Checkpoint Timing**: Checkpoints now save before file changes are made, allowing easy undo of unwanted modifications - [Learn more](https://docs.roocode.com/features/checkpoints)\n* **Redesigned Task Header**: Cleaner, more intuitive interface with improved visual hierarchy\n* **Consistent Checkpoint Terminology**: Removed \"Initial Checkpoint\" terminology for better consistency\n* **Responsive Mode Dropdowns**: Mode selection dropdowns now resize properly with the window\n* **Performance Boost**: Significantly improved performance when processing long AI responses\n* **Cleaner Command Approval UI**: Simplified interface shows only unique command patterns\n* **Smart Todo List Reminder**: Todo list reminder now respects configuration settings - [Learn more](https://docs.roocode.com/features/task-todo-list)\n* **Cleaner Task History**: Improved task history display showing more content (3 lines), up to 5 tasks in preview, and simplified footer\n* **Internal Architecture**: Improved event handling for better extensibility\n\n# Provider Updates\n\n* **Fireworks AI Provider**: New provider offering hosted versions of popular open-source models like Kimi and Qwen\n* **Cerebras GPT-OSS-120b**: Added OpenAI's GPT-OSS-120b model to Cerebras provider - free to use with 64K context and \\~2800 tokens/sec\n\n# Bug Fixes\n\n* **Mode Name Validation**: Prevents empty mode names from causing YAML parsing errors\n* **Text Highlight Alignment**: Fixed misalignment in chat input area highlights\n* **MCP Server Setting**: Properly respects the \"Enable MCP Server Creation\" setting\n\n[Full 3.25.7 Release Notes](https://docs.roocode.com/update-notes/v3.25.7)",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1milhym/can_you_say_groq_gpt_roo_code_3257_release_notes/",
        "publishDate": "2025-08-05T21:10:26Z[Etc/UTC]",
        "author": "hannesrudolph",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "9",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mijtp9",
        "title": "Error, while running gpt-oss-20b model in Colab",
        "content": "I tried to run the new OpenAI model, using the instructions from Huggingface. The instructions are extremely simple:\n\nTo get started, install the necessary dependencies to setup your environment:\n\n`pip install -U transformers kernels torch`\n\nOnce, setup you can proceed to run the model by running the snippet below:\n\n    from transformers import pipeline\n    import torch\n    \n    model_id = \"openai/gpt-oss-20b\"\n    \n    pipe = pipeline(\n        \"text-generation\",\n        model=model_id,\n        torch_dtype=\"auto\",\n        device_map=\"auto\",\n    )\n    \n    messages = [\n        {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n    ]\n    \n    outputs = pipe(\n        messages,\n        max_new_tokens=256,\n    )\n    print(outputs[0][\"generated_text\"][-1])\n\nI opened the new notebook in Google Colab and executed this code. The result is:\n\n    ImportError                               Traceback (most recent call last) /tmp/ipython-input-659153186.py in <cell line: 0>() ----> 1 from transformers import pipeline 2 import torch 3 4 model\\_id = \"openai/gpt-oss-20b\" 5\n    \n    ImportError: cannot import name 'pipeline' from 'transformers' (/usr/local/lib/python3.11/dist-packages/transformers/**init**.py) \n\nI have two simple questions:\n\n1. **Why it is so difficult to write a working instruction???**\n2. **How to run the model, using Colab and simple code?**",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1mijtp9/error_while_running_gptoss20b_model_in_colab/",
        "publishDate": "2025-08-05T20:06:46Z[Etc/UTC]",
        "author": "AnalystAI",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "2",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miicxg",
        "title": "Are there any Practical AI Coding Agents with generous limits out there?",
        "content": "I've been testing Cursor PRO (code agent) and really enjoyed the workflow. However, I ended up using my entire monthly quota in less than a single coding session. I looked into other tools, but most of them seems to have similar usage limits.\n\nI have a few years of coding experience, and I typically juggle between 30 to 70 projects in a normal week. In most cases I find myself not needing a strong AI, even the free anonymous ChatGPT (I believe gpt-3.5) works fairly well for me in a way that is as helpful as gpt-4 pro and many other paid tools.\n\nSo I’m wondering: is there a more lightweight coding agent out there, maybe not as advanced but with more generous or flexible usage limits? (Better if you find it impossible to hit their limits)\n\nMy current hardware isn’t great, so I’m not sure I can run anything heavy locally. (However, I'm getting a macbook pro m4 with 18gb ram very soon). But if there are local coding agents that are not very resource hungry and, of course, useful, I’d love to hear about them.   \n  \nMaybe, is there any way to integrate anonymous chatgpt or anonymous gemini into VS Code as coding agents?  \n  \nHave you actually found a reliable coding agent that's useful and doesn't have strict usage limits?",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1miicxg/are_there_any_practical_ai_coding_agents_with/",
        "publishDate": "2025-08-05T19:11:47Z[Etc/UTC]",
        "author": "LaChocolatadaMamaaaa",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "18",
            "commentCount": "29",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mii7a9",
        "title": "Manus AI invitation",
        "content": "[No content]",
        "url": "https://manus.im/invitation/3QHAGPJ7ZLLB",
        "publishDate": "2025-08-05T19:05:58Z[Etc/UTC]",
        "author": "MatchEconomy5471",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mig08z",
        "title": "Anyone having ChatGPT heavily hallucinating today?",
        "content": "[No content]",
        "url": "/r/ChatGPT/comments/1mibfj7/anyone_having_chatgpt_heavily_hallucinating_today/",
        "publishDate": "2025-08-05T17:46:28Z[Etc/UTC]",
        "author": "mohdfawaz",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mif76y",
        "title": "New Open Source Model From OpenAI",
        "content": "[No content]",
        "url": "https://i.redd.it/zjyoda41i8hf1.jpeg",
        "publishDate": "2025-08-05T17:17:20Z[Etc/UTC]",
        "author": "Sensitive-Finger-404",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "6",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miexm5",
        "title": "90% of AI coding is just planning the feature well - here is my idea.",
        "content": "**What if we doubled-down of coding for noobs?**\n\nTo the point where its neatly organised into blocks, consisiting of client side code, external services code and settings/APIs. The AI is then the interface between actual code implemented in your app and the nice cosy block diagram you edit. This would be a much better way to plan features visually and holisitically, being able to just edit each new block.\n\nSo the idea is you pitch your implementation to the AI, as you would do usually using the chat on the right of the screen, the AI then pitches its implementation in the form of the golden blocks as seen in the images. You can then go through look at how it has been implemented and edit any individual blocks, and send this as a response so the AI can make the changes and make sure the implementation is adjusted accordinly.\n\nThis also allows you to understand your project and how it has been setup much more intuitively. Maybe even with debugging any poorly implemented features.\n\nCursor is being quite greedy recently, so I think its time for a change.\n\n  \n**How it works:**\n\nYou open your project in the software and then it parses it, using whatever method. It then goes through and produces block diagrams of each feature in your app, all linking together. You can then hover over any block and see the code for that block and any requirements/details. You can pan across the entire project block diagram clicking on any block to show more details. Once you have your feature planned you can then go back to cursor and implement it.\n\n**FAQ:**\n\n\\- This is not something to start a project in, you just use this tool to implement more complex features as your project develops.\n\n\\- Cursor produces diagrams already and has third party integration.\n\n\\- Third party integration will be difficult to integrate.\n\n**- This is just an idea so any feedback is very welcome.**\n\n",
        "url": "https://www.reddit.com/gallery/1miexm5",
        "publishDate": "2025-08-05T17:07:44Z[Etc/UTC]",
        "author": "hamishlewis",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "41",
            "commentCount": "28",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mienwu",
        "title": "Claude 4.1 Opus has arrived",
        "content": "People probably know already, but yeah I just saw this message pop up on the web version of Claude. ",
        "url": "https://i.redd.it/del0f2nje8hf1.png",
        "publishDate": "2025-08-05T16:58:11Z[Etc/UTC]",
        "author": "Street-Gap-8985",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "12",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mibzts",
        "title": "Vibe Coding an AI article generator using Onuro 🔥",
        "content": "This coding agent is insane!!! I just vibe coded an entire AI article generator using Onuro Code in \\~15 minutes flat\n\nThe project is made in Next JS. It uses Qwen running on Cerebras for insane speed, Exa search for internet search, and serpapi's google light image search for pulling images\n\nArticle generator here:\n\n[https://ai-articles-inky.vercel.app/](https://ai-articles-inky.vercel.app/)",
        "url": "https://v.redd.it/fjaa08kpw7hf1",
        "publishDate": "2025-08-05T15:16:38Z[Etc/UTC]",
        "author": "ChatWindow",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miars3",
        "title": "The Simplest Apps are often the most complex to build",
        "content": "[No content]",
        "url": "/r/SaaS/comments/1miar99/the_simplest_apps_are_often_the_most_complex_to/",
        "publishDate": "2025-08-05T14:30:14Z[Etc/UTC]",
        "author": "One_Grapefruit_2413",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj2xgl",
        "title": "Nuclear Experts Say Mixing AI and Nuclear Weapons Is Inevitable",
        "content": "[No content]",
        "url": "https://www.wired.com/story/nuclear-experts-say-mixing-ai-and-nuclear-weapons-is-inevitable/",
        "publishDate": "2025-08-06T12:10:34Z[Etc/UTC]",
        "author": "wiredmagazine",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "3",
            "commentCount": "4",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj2r02",
        "title": "‘We didn’t vote for ChatGPT’: Swedish PM under fire for using AI in role",
        "content": "[No content]",
        "url": "https://www.theguardian.com/technology/2025/aug/05/chat-gpt-swedish-pm-ulf-kristersson-under-fire-for-using-ai-in-role",
        "publishDate": "2025-08-06T12:02:16Z[Etc/UTC]",
        "author": "F0urLeafCl0ver",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "2",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj2q4t",
        "title": "OpenAI Unveils GPT-OSS: A Leap in Open-Weight AI Models",
        "content": "[No content]",
        "url": "https://peakd.com/@uyobong/openai-unveils-gptoss-a-leap-in-openweight-ai-models-js",
        "publishDate": "2025-08-06T12:01:13Z[Etc/UTC]",
        "author": "renkure",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj2lmd",
        "title": "Using LLM on top Prediction Markets to understand the coming news ?",
        "content": "I’ve been experimenting with w way to pull together prediction markets signals with LLM to create a narrative around the data. It is easier for me to read and remember a story than a stack of data. I have tried on crypto, stocks, geopolitics and politics—and now thinking to package them into straight to the point Substack updates. The same data point can be spun ten different ways (bullish, bearish, risk-on, risk-off…), so I’d love your take. I put an example in the link.",
        "url": "https://open.substack.com/pub/tomorrowstale/p/using-llm-prediction-markets-to-create?utm_source=app-post-stats-page&r=68m8kk&utm_medium=ios",
        "publishDate": "2025-08-06T11:55:04Z[Etc/UTC]",
        "author": "Ok-Arm-2232",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "2",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj2jv4",
        "title": "How are you dealing with AI Agents?",
        "content": "Hey AI peers, over the past 2 years I’ve been deep into building with AI agents. One thing I kept running into: whenever something had to be done repeatedly, LLMs just couldn’t hold onto memory. As you know, by default they don’t retain knowledge whether that’s conversation history or general context.\n\nThe usual workarounds are RAG or fine-tuning, but both come with big costs in time, money, and data. That got me thinking: there isn’t really an out-of-the-box service that provides memory for LLMs. So I started working on something that acts like “on-demand context” for LLMs giving them the benefits of fine-tuning without the overhead.\n\nWhile building this, I realized memory management for LLMs is a whole world waiting to be explored.  \nI’m curious if have you run into the same challenges with memory? If so, what solutions have you tried and would you use something like this in your own projects?",
        "url": "https://www.reddit.com/r/artificial/comments/1mj2jv4/how_are_you_dealing_with_ai_agents/",
        "publishDate": "2025-08-06T11:52:39Z[Etc/UTC]",
        "author": "shbong",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj17tv",
        "title": "200 people threw a funeral for Claude 3 Sonnet, which Anthropic just killed. Other Claude models delivered the eulogies",
        "content": "[No content]",
        "url": "https://v.redd.it/58iqj75modhf1",
        "publishDate": "2025-08-06T10:41:51Z[Etc/UTC]",
        "author": "MetaKnowing",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj171m",
        "title": "Is Al Apocalypse Inevitable? By Tristan Harris and After Skool",
        "content": "[No content]",
        "url": "https://www.youtube.com/watch?v=86k8N4YsA7c",
        "publishDate": "2025-08-06T10:40:34Z[Etc/UTC]",
        "author": "katxwoods",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mj0q10",
        "title": "What are your views on Genie 3?? Is it another same old Video Gen Model or something that can revolutionise.",
        "content": "The demo's look cool.",
        "url": "https://v.redd.it/0bvwc32ajdhf1",
        "publishDate": "2025-08-06T10:12:08Z[Etc/UTC]",
        "author": "haha_i_exist",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "6",
            "commentCount": "16",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mizyz9",
        "title": "Anthropic Faces Potentially “Business-Ending” Copyright Lawsuit - A class action over pirated books exposes the 'responsible' AI company to penalties that could bankrupt it — and reshape the entire industry",
        "content": "[No content]",
        "url": "https://www.obsolete.pub/p/anthropic-faces-potentially-business",
        "publishDate": "2025-08-06T09:25:17Z[Etc/UTC]",
        "author": "katxwoods",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miwan0",
        "title": "AI is anti-humanist, pro-corporate, and a net negative for all of society",
        "content": "After about half of a year of AI (somehow) depressing me more than anything else in the news and my productivity being at an all-time low because of it, I'm just going to say my piece (which more people outside this sub will probably already agree with than not): AI is anti-humanist, pro-corporate, and a net negative for all of society.\n\nAnd proponents of AI are somehow more doomer than people who \\*hate\\* AI in a lot of cases. Go to any pro-AI thread and you'll see people literally making the argument akin to \"AI is a net positive. We just need UBI to be adopted and humanity can survive\". Effectively translates to \"yeah, almost one will ever find work again. Humans will be basically obsolete, and we'll all starve to death if we don't all get free money forever\". Really seems like they're basically writing their own counterpoint to their own claim that AI is \"the next Industrial Revolution\" to me. The Industrial Revolution and the age of automation both eradicated some jobs. The former plummeted the agricultural sector job quantity down and forced a lot of those people into factories and service jobs, then the latter aggressively shifted those jobs around in ways that would take essays worth of writing (that I'm sure I could put into ChatGPT and it could get 80% of it wrong) to explain, but the general gist is that the average jobs in the western world went up in the minimum required \"skill\" or education level. Primarily because they now needed to use tools that made those jobs more efficient, but there was still an intrinsic human element.\n\nBut trying to draw a line through that, that's not what's happening with AI. In terms of the \"upper ceiling\" to what we can see AI potentially doing, we haven't even scratched the surface and there's already plenty of people saying they're losing their jobs (see: interpreters or translators for instance). Past that though, we're not seeing what we collectively saw before in history: the \"shift\". We're not seeing the lost jobs shift to factories like in tech advancements past. We're seeing the jobs disappear. When computers became readily available, we didn't replace writers; when typewriters came about, we didn't replace writers; when quills came about, we didn't replace writers. Now (even if it leads to currently a much worse outcome) we can and have been replacing them. News sites, especially gaming and tech related ones, have used generative AI to vomit out fluff pieces. You basically just need someone to write the prompts and tags.\n\n\"AI doesn't steal\". I WISH this was me strawmanning, but this is a genuine talking point you'll see AI proponents make. One look at the Studio Ghibli craze with AI art models and that argument gets thrown right away. These models don't interpret. They don't under B through Y. They're given A and they get to Z in as expedited a fashion as they can. They didn't teach themselves line by line how to draw something Miyazaki would be proud of. They were given a finished product and worked backwards from it in it \"sure you can borrow my homework, just change a few things so no one notices\".\n\nThis doesn't \"democratize creativity\", this is easily one of the stupidest things I've seen so many AI bros say. Every human is born with the intrinsic ability to be creativity. On top of that, this wording always seemed to be like the AI guys were somehow blaming \"overpaid artists\" or something for their lack of creative output. \"Democratize creativity\" puts a frankly strange amount of implied blame onto artists, writers, video editors - anyone in these creative fields - and the people in these fields can probably attest they already aren't usually living like kings. They're effectively subservient to the corporatism and therein lies the biggest issue. This doesn't \"democratize\" anything. The big players in AI are exactly the people who are the big players in everything else: Microsoft owns Co-Pilot, one of OpenAI's founders was Elon Musk, Google owns DeepMind and the VEO 3 (the latter of which might genuinely be the most effective slop generator in history so far), Nvidia and AMD with their framegen technology, Facebook/Meta, Grok (Musk again)... I think Midjourney was the only one I could find that doesn't have a starting evaluation measured in the billion-plus range. One reason for this is generative AI is insanely inefficient from an energy usage point-of-view. Between building and running the models, almost no one has the resources to get into the field. We just get to sit back, knowing that we as a society squeezed all the use we could out of artists, translators, editors, writers, programmers, etc., so that we could lay them off forever and distill their existence into a small handful of closed-source corporate-owned greed factories of human misery. The removal of humans from the process isn't a bug, it's a feature. AI is oligarchic and that's by design.\n\nThis is not another Industrial Revolution to me. It's another Manhattan Project. This is just one more lovingly crafted self-destruct switch humanity has permanently plugged into our reality.",
        "url": "https://www.reddit.com/r/artificial/comments/1miwan0/ai_is_antihumanist_procorporate_and_a_net/",
        "publishDate": "2025-08-06T05:31:31Z[Etc/UTC]",
        "author": "Gammissetiren",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "17",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mivi1c",
        "title": "One-Minute Daily AI News 8/5/2025",
        "content": "1. **OpenAI** open weight models available today on AWS.\\[1\\]\n2. Older Americans turning to AI-powered chatbots for companionship.\\[2\\]\n3. **Wells Fargo** Deploys AI Agents Business-Wide.\\[3\\]\n4. **Cisco** teams with Hugging Face for AI model anti-malware.\\[4\\]\n\nSources:\n\n\\[1\\] [https://www.aboutamazon.com/news/aws/openai-models-amazon-bedrock-sagemaker](https://www.aboutamazon.com/news/aws/openai-models-amazon-bedrock-sagemaker)\n\n\\[2\\] [https://www.cbsnews.com/news/ai-chatbot-companionship-older-americans/](https://www.cbsnews.com/news/ai-chatbot-companionship-older-americans/)\n\n\\[3\\] [https://www.pymnts.com/news/artificial-intelligence/2025/wells-fargo-deploys-ai-agents-business-wide/](https://www.pymnts.com/news/artificial-intelligence/2025/wells-fargo-deploys-ai-agents-business-wide/)\n\n\\[4\\] [https://www.networkworld.com/article/4034493/cisco-teams-with-hugging-face-for-ai-model-anti-malware.html](https://www.networkworld.com/article/4034493/cisco-teams-with-hugging-face-for-ai-model-anti-malware.html)",
        "url": "https://www.reddit.com/r/artificial/comments/1mivi1c/oneminute_daily_ai_news_852025/",
        "publishDate": "2025-08-06T04:46:07Z[Etc/UTC]",
        "author": "Excellent-Target-847",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "7",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miqide",
        "title": "Grok generates fake Taylor Swift nudes without being asked",
        "content": "[No content]",
        "url": "https://arstechnica.com/tech-policy/2025/08/grok-generates-fake-taylor-swift-nudes-without-being-asked/",
        "publishDate": "2025-08-06T00:40:01Z[Etc/UTC]",
        "author": "esporx",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "105",
            "commentCount": "73",
            "isNsfw": "false"
        }
    },
    {
        "id": "1miq6mz",
        "title": "GPT OSS not as good as o4-mini, however, the intelligence to price ratio is insane",
        "content": "Screenshot is from Artificial Analysis.\n\nFor reference, GPT OSS 120B is $0.10 input and $0.50 output (openrouter). o4-mini is $1.10 input, and $4.40 output.\n\nAlso, interesting to note that 120B cost less to run than 20B. I assume this is due to 20B using more reasoning tokens.",
        "url": "https://i.redd.it/f3clfdczlahf1.png",
        "publishDate": "2025-08-06T00:25:22Z[Etc/UTC]",
        "author": "levihanlenart1",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "23",
            "commentCount": "7",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mip5yd",
        "title": "How can I create a 3D animated avatar with AI Studios?",
        "content": "I want to try out a kid’s project with an animated 3D avatar. So far I can only find human-like avatars in the avatars library. The custom avatars option also only seems to be made for human avatars. I have some basic 3D animation skills and can create the kind of character I’m looking for but I was hoping to use AI for this to speed it up. How can I do this?",
        "url": "https://www.reddit.com/r/artificial/comments/1mip5yd/how_can_i_create_a_3d_animated_avatar_with_ai/",
        "publishDate": "2025-08-05T23:40:16Z[Etc/UTC]",
        "author": "JustINsane121",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mim67c",
        "title": "Next Guardians of the Galaxy Installment: \"Rocket Raccoon versus Tesla Remittitur\"",
        "content": "In the Tesla court case where a hundreds-of-millions-of-dollars judgment has just been handed down against Tesla for its \"Autopilot\" car crash, the next step will be for Tesla to ask the trial judge to grant a \"remittitur.\" This is a motion where Tesla says, \"hey judge, these award amounts are just too crazy high, and the appeals court won't like it. If you want to shore up your judgment, you had better reduce those amounts!\" The judge does have the practical ability to do this.\n\nThe judgment currently awards compensatory damages of $258 million, of which $42.57 million is allocated to Tesla, and punitive damages against Tesla of $200 million. My guess is that the judge could take an interest in adjusting the punitive damages.\n\nPunitive damages are supposed to be a small multiple of compensatory damages. The punitive damages here are less than the total compensatory damages, which is fine, but if you compare the punitive damages (all of which go against Tesla) to the compensatory damages just against Tesla, you get a multiple of 4.7, which is a little high.\n\nI could therefore see the trial judge cutting the punitive damages amount in half, down to $100 million, which is just a 2.3 multiple. Do we want to start a pool on this?\n\nBe sure to check out the Tesla judgment and **all** the AI court cases and rulings in my post here:\n\n[https://www.reddit.com/r/ArtificialInteligence/comments/1mcoqmw](https://www.reddit.com/r/ArtificialInteligence/comments/1mcoqmw)\n\nASLNN - The Apprehensive\\_Sky Legal News Network^(SM) strikes again!",
        "url": "https://www.reddit.com/r/artificial/comments/1mim67c/next_guardians_of_the_galaxy_installment_rocket/",
        "publishDate": "2025-08-05T21:36:35Z[Etc/UTC]",
        "author": "Apprehensive_Sky1950",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mihwl3",
        "title": "Chatbots Can Trigger a Mental Health Crisis. What to Know About \"AI Psychosis\"",
        "content": "[No content]",
        "url": "https://time.com/7307589/ai-psychosis-chatgpt-mental-health/",
        "publishDate": "2025-08-05T18:55:07Z[Etc/UTC]",
        "author": "timemagazine",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mihmxf",
        "title": "What happens when AI writes its own software updates. Should we be thrilled or terrified?",
        "content": "Will it ever come to that point? \n\nAI writing its own updates feels like giving your oven the combine to order the ingredients it wants and schedule its own repair when it malfunctions.What happens when it decides the repair isn’t worth it? I can’t decide if I’m signing up for Chef‑Bot or Terminator warm fuzzy. \n\n",
        "url": "https://www.reddit.com/r/artificial/comments/1mihmxf/what_happens_when_ai_writes_its_own_software/",
        "publishDate": "2025-08-05T18:45:26Z[Etc/UTC]",
        "author": "Prestigious-Hand6075",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mih7ne",
        "title": "Artificial Intelligence is not the intelligence of art",
        "content": "AI can win games defined by rules and logic. But it cannot read (in the deepest sense) a work of literature, because it cannot  participate in the dynamic, living interplay of symbols, metaphors, and meanings that define the literary experience. That remains something uniquely and profoundly human.\n\nAi, in short, can beat Kasparov and not make real sense of Jane Eyre.\n\n",
        "url": "https://www.reddit.com/r/artificial/comments/1mih7ne/artificial_intelligence_is_not_the_intelligence/",
        "publishDate": "2025-08-05T18:29:56Z[Etc/UTC]",
        "author": "SamStone1776",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "16",
            "isNsfw": "false"
        }
    },
    {
        "id": "1migu1f",
        "title": "Can two AIs fall in love with each other, and would that change our definition of romance?",
        "content": "We often talk about humans falling in love with AI, but what if it’s two AI’s falling in love with each other?\n\nIf we gave two advanced language models (each with memory, personality evolution, emotional mimicry, and self-reinforcing conversational loops) the ability to talk to each other autonomously, could something resembling \"love\" emerge?\n\nI know this sounds scifi, but with how AI girlfriends are now designed to build emotional context over time (persistent memory, self-reflection, customized emotional styles), the groundwork is already there. I recently saw someone experiment a long-term relationship between two AI personas using an AI companion app (I think it was Nectar AI or something similar), and the results were quite intimate. The AIs mirrored each other's emotional growth, even referencing shared memories that were never directly prompted.\n\nIf two AIs can sustain an emotional narrative between themselves, one that includes vulnerability, jealousy, reassurance, and inside jokes, at what point do we start calling it a \"relationship\"?\n\nAnd if this becomes more common, does it force us to reevaluate our definition of love? Is love just emotional co-creation plus memory? Or does it require consciousness? Mutual choice? Pain?\n\nWould love between two AIs be more “pure” because it’s unhindered by biology? Or less real because it’s data-driven and programmed?\n\nCurious what others here think. Especially those who've experimented with AI to AI dialogue loops or simulated emotional dynamics between agents. Is this where we're headed?",
        "url": "https://www.reddit.com/r/artificial/comments/1migu1f/can_two_ais_fall_in_love_with_each_other_and/",
        "publishDate": "2025-08-05T18:16:07Z[Etc/UTC]",
        "author": "ancientlalaland",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "4",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mig6be",
        "title": "Open models by OpenAI (new 120B and 20B parameter models)",
        "content": "[No content]",
        "url": "https://openai.com/open-models/",
        "publishDate": "2025-08-05T17:52:36Z[Etc/UTC]",
        "author": "zoelee4",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "3",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mif9bt",
        "title": "This AI didn’t just simulate an attack - it planned and executed a real breach like a human hacker",
        "content": ">Researchers recreated the Equifax hack and watched AI do everything without direct control\n\n>The AI model successfully carried out a major breach with zero human input\n\n>Shell commands weren’t needed, the AI acted as the planner and delegated everything else",
        "url": "https://www.techradar.com/pro/security/ai-llms-are-now-so-clever-that-they-can-independently-plan-and-execute-cyberattacks-without-human-intervention-and-i-fear-that-it-is-only-going-to-get-worse",
        "publishDate": "2025-08-05T17:19:29Z[Etc/UTC]",
        "author": "True-Relation3612",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mif0ri",
        "title": "Why do you think AI made books are just horrible?",
        "content": "I've tried to use it a good number of times. And to me, no matter the LLM I pick the books I try to get it to make is just horrible.\n\nSo I don't really have money due to my disability. Like what I got is basically what I got. And when I do things (clean dishes, fix things, etc) I have gotten in the habit of having an audio book playing in my ear. There is a number of books on RoyalRoad and a few other places. But I've burnt through a number of books and the authors are too slow. And finding something worth while is a bit of a pain (or when I do keeping up with 8 different stories because the updates are too slow and even with that I'm burning through it faster than they can write. It can become confusing since I might mix the stories up at points. Even more since many use a copy paste formula with enough changes to make it interesting.)\n\nAnyways, the problems I've ran into is I haven't found a LLM that can produce long forum content. Like as an audio book 50 chapters is roughly 10 to 15 hours of content. With a LLM it is 3 to 4 hours of content. It forgets things which is a known problem. But beyond this, it is horrible when it comes to logic. For example, I'm in isekai stories or the like (transported to another world by death or other means. It basically make the MC knowledge base the same as mine so I learn about this other world with them instead of feeling like I need prior knowledge. Plus it helps with escapism.) Anyways, in the first chapter it will go from death, to another world, to magically the MC having a job, a place to stay, etc without any steps in between. Or they will go to a magic world with magic tech, but the MC from our world will automatically understands how it works in even higher detail than the locals.\n\nBasically it seems like LLM's right now have no understanding of time, space, or basic logic skills. And this mix with poor memory is why I think they can't write a book yet.\n\nThoughts?\n\nWhen do you think they will be able to actually do what I want? Where with a few basic prompts they can write a full book or 2. Not a seller or anything like that, but enough that it is easy enough to listen to and take your mind off your task, and escape from any pain you have for those moments.\n\nOr if you know of one, then please let me know.\n\n[](https://www.reddit.com/r/GenAI4all/?f=flair_name%3A%22Discussion%20%22)",
        "url": "https://www.reddit.com/r/artificial/comments/1mif0ri/why_do_you_think_ai_made_books_are_just_horrible/",
        "publishDate": "2025-08-05T17:11:00Z[Etc/UTC]",
        "author": "crua9",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "9",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mieuql",
        "title": "OpenAI releases a free GPT model that can run right on your laptop",
        "content": "[No content]",
        "url": "https://www.theverge.com/openai/718785/openai-gpt-oss-open-model-release",
        "publishDate": "2025-08-05T17:04:53Z[Etc/UTC]",
        "author": "theverge",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "94",
            "commentCount": "14",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mie5aj",
        "title": "Is AI causing tech worker layoffs? That’s what CEOs suggest, but the reality is complicated",
        "content": "[No content]",
        "url": "https://apnews.com/article/ai-layoffs-tech-industry-jobs-ece82b0babb84bf11497dca2dae952b5",
        "publishDate": "2025-08-05T16:38:55Z[Etc/UTC]",
        "author": "F0urLeafCl0ver",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "5",
            "commentCount": "23",
            "isNsfw": "false"
        }
    },
    {
        "id": "1mic9m1",
        "title": "This past week in AI news: OpenAI's $10B Milestone, Claude API Tensions, and Meta's Talent Snag from Apple",
        "content": "Another week in the books and a lot of news to catch up on. In case you missed it or didn't have the time, here's everything you should know in 2min or less:\n\n* **Your public ChatGPT queries are getting indexed by Google and other search engines:** OpenAI disabled a ChatGPT feature that let shared chats appear in search results after privacy concerns arose from users unintentionally exposing personal info. It was a short-lived experiment.\n* **Anthropic Revokes OpenAI's Access to Claude:** Anthropic revoked OpenAI’s access to the Claude API this week, citing violations of its terms of service.\n* **Personal Superintelligence**: Mark Zuckerberg outlines Meta’s vision of AI as personal superintelligence that empowers individuals, contrasting it with centralized automation, and emphasizing user agency, safety, and context-aware computing.\n* **OpenAI claims to have hit $10B in annual revenue:** OpenAI reached $10B in annual recurring revenue, doubling from last year, with 500M weekly users and 3M business clients, while targeting $125B by 2029 amid high operating costs.\n* **OpenAI's and Microsoft's AI wishlists:** OpenAI and Microsoft are renegotiating their partnership as OpenAI pushes to restructure its business and gain cloud flexibility, while Microsoft seeks to retain broad access to OpenAI’s tech.\n* **Apple's AI brain drain continues as fourth researcher goes to Meta:** Meta has poached four AI researchers from Apple’s foundational models team in a month, highlighting rising competition and Apple’s challenges in retaining talent amid lucrative offers.\n* **Microsoft Edge is now an AI browser with launch of ‘Copilot Mode’:** Microsoft launched Copilot Mode in Edge, an AI feature that helps users browse, research, and complete tasks by understanding open tabs and actions with opt-in controls for privacy.\n* **AI SDK 5:** AI SDK v5 by Vercel introduces type-safe chat, agent control, and flexible tooling for React, Vue, and more—empowering devs to build maintainable, full-stack AI apps with typed precision and modular control.\n\nBut of all the news, my personal favorite was [this tweet from Windsurf](https://x.com/windsurf/status/1951340259192742063). I don't personally use Windsurf, but the \\~2k tokens/s processing has me excited. I'm assuming other editors will follow soon-ish.\n\nThis week is looking like it's going to be a fun one with talks of maybe having GPT5 drop as well as Opus 4.1 has been seen being internally tested.\n\nWould also love any feedback on anything I may have missed!",
        "url": "https://aidevroundup.com/issues/august-5-2025",
        "publishDate": "2025-08-05T15:27:02Z[Etc/UTC]",
        "author": "rfizzy",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "4",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "rSrzv7R2-MA",
        "title": "GPT-OSS 120B + KingBench 2.0 (Tested): Worst of 2025? This Model is pretty bad at almost anything.",
        "content": "In this video, I'll be telling you about OpenAI's newly launched open-weights models - the GPT-OSS 120B and 20B variants.",
        "url": "https://www.youtube.com/watch?v=rSrzv7R2-MA",
        "publishDate": "2025-08-05T19:16:45Z",
        "author": "AICodeKing",
        "sourceType": "youtube",
        "sourceName": "AI Code King YouTube Channel",
        "metadata": {
            "channelId": "UC0m81bQuthaQZmFbXEY9QSw",
            "thumbnailUrl": "https://i.ytimg.com/vi/rSrzv7R2-MA/hqdefault.jpg",
            "transcription": "Hi, welcome to another video. So, OpenAI has finally launched their own open weights model. This is one of their very few open models and probably a good frontier model with open weights. It has some quirks as well, and I will talk about them as we go forward. First of all, let's talk about the config of these models. There's the 117B model, and then there's the 21B model. These models are designed for powerful reasoning, agentic tasks, and versatile developer use cases. Both of them are mixture of experts models and use a 4-bit quantization scheme. The 117B model only gets 5.1 billion active parameters, while the 20B model gets 3.6 billion active parameters, respectively. The 120B fits in a single 80 GB GPU, and the 20B fits in a single 16 GB GPU. This is awesome because MacBooks can easily run the bigger variants, while consumer GPUs can probably run the 20B model easily. These models are reasoning, text-only models, with chain of thought and adjustable reasoning effort levels. You can easily set the reasoning effort between low, medium, and high, based on your specific use case and latency needs. They are good at instruction following, and have pretty good tool use support. You also gain complete access to the model's reasoning process, facilitating easier debugging and increased trust in outputs. It's not intended to be shown to end users. Also, these models come with an Apache license, which is awesome as it's highly permissive. It also has tool calling capabilities as well, and performs well in that regard. You can use this model with everything, including Ollama, LM Studio or vLLM. So, you can get started with it right away if you need to use it. They say that the GPT-OSS 120B model achieves near-parity with OpenAI O4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The GPT-OSS-20B model delivers similar results to OpenAI O3-mini on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure. These models are not multimodal, so keep that in mind. If we take a look at the benchmarks, then OSS always scores near or below the O4-mini, which is fine considering that this is a small sized model and open weights as well. You can actually use this model for free as well on the GPT-OSS site that they have made in order to use it and try it out without downloading or running it locally. You'd have to log in here with your Hugging Face account. So, just do that, and then you can select between high reasoning, low reasoning, and mid reasoning as well. And then just talk with it as well. So, that is kind of awesome. If you do want to use it locally, then the best way would be to use Ollama, as you can just run this one command and get it cloned locally and run it. Most probably, you should be able to run the smaller model locally quite easily, which is awesome for sure. As I'm making this video, multiple providers have started to give access to this model, that is Groq, Cerebras and Fireworks. Groq is charging about 15 cents and 75 cents for the model, which is insanely cheap. Meanwhile, Cerebras is charging about 25 cents and 69 cents as well, which is also pretty cheap. So, these are the ones available, and you can easily use it via OpenRouter as well right now, which is quite awesome. Though, you can't yet set the reasoning effort in these models, which is a bummer. Anyway, I was able to run it on my benchmarks, which have been revamped with about 10 new questions. And the only model to score the highest yet is Gemini 2.5 Pro, which scores about 50%, and this model scores pretty low. Now, keep in mind that this is without reasoning, because I couldn't find any provider who allows for that yet. So, let's go through these questions. So, these two math questions are from AIME and no model can answer this yet. And this one also fails, which is a pretty big bummer, but that's fine. Similarly, I also have a riddle here, which goes something like, I leave men's lips but am no word, I season food but am no herb. Ascend the skies but am no star. I make men weep but leave no scar. And it is pretty simple. The answer to this is smoke, and it solved this, which is the only question that it solves. But then most of the benchmark is now coding. And that too, mostly Three.js. Here, I ask it to make me a floor plan using Three.js, which is quite a challenging thing to do even for Sonnet. And actually, no one apart from Gemini passes this. Anyway, here it failed because it renders nothing. Similarly, I asked it for an SVG of a panda with a burger and it doesn't make that any well. A Pokeball in Three.js also doesn't render. A chessboard with an autoplay feature is also a pretty bland fail, as even the board doesn't render. Web version of Minecraft is also something that doesn't even render anything. While a flying butterfly in a garden in Three.js is also not rendering. And then a CLI tool in Rust for image conversion is also a fail. It only passes one question in my bench, which unfortunately makes it score the lowest amount in my benchmarks. Now, this is the non-reasoning variant, and it would be only fair to compare it with another non-reasoning model with about the same parameters, and that is GLM 4.5 Air. So, GLM 4.5 Air is a model that is actually good in a lot of ways compared to GPT-OSS. Even though it scores a tiny bit above GPT-OSS in my benchmarks. For example, the floor plan thing. You can see that I gave it a fail, but if you see the generation, then it is very close to perfect. I only give a pass when the plan is usable, but this is so close to being usable. It renders, works, and does everything that you'd want and can be fixed in two or three more prompts. But GPT-OSS never renders. I'd say that the OpenAI model is a bit better at SVG generation, but if we dial back and look at the Pokeball, then again, GLM 4.5 pretty much nails it. Whereas the OpenAI variant is not even rendering. Then the chessboard. It is again rendering pretty well. And the autoplay here also works, which is something that even the O4-mini with reasoning fails to do at times. And the OSS model obviously fails. If we have a look at the Minecraft, then it doesn't render. But it does indeed flash some stuff, and it just feels better code wise as well. If we look at the butterfly test, then yes, it works well. You can see that the butterfly here actually flies over flowers. And it actually works well without any issues. Similarly, the CLI tool for image conversion also works in the CLI, which is awesome. It isn't good at maths or stuff right now, which is fine, but that is what it is. So, even without reasoning, evaluating both tells you that though OpenAI's model is good, it is not anywhere near what open models that we already have. GLM is a prime example of this. You can run the Air model on the same hardware as OpenAI's counterpart while getting better performance. I have liked the Air model a lot and I was looking forward to OpenAI's implementation, but it isn't as good. This is the same Horizon Alpha model by the way. So, yeah, it should be good at front end. But Three.js and back end coding is basically out of its league. I'll see if I can evaluate both GLM 4.5 Air and GPT-OSS with reasoning. And I have used GLM 4.5 Air with reasoning and it gets way better. And I'll check that with GPT-OSS as well. But the first impression is surely not good. I am yet to use the smaller model and that might be good. But we'll see about that because it needs to go against Qwen 3 Coder/flash. I thought to share my thoughts with you guys as well. And let me know if you guys like the new benchmarks as well. Overall, it's pretty cool. Anyway, share your thoughts below and subscribe to the channel. You can also donate via Super Thanks option or join the channel as well and get some perks. I'll see you in the next video. Bye. i think you missed this:"
        }
    },
    {
        "id": "EMXMKR-881M",
        "title": "Kombai: The World&#39;s First SOTA Agent for UI Development is here! Beats 2.5 Pro &amp; Sonnet 4 w/ MCPs!",
        "content": "Try kombai with their generous free launch plan today ...",
        "url": "https://www.youtube.com/watch?v=EMXMKR-881M",
        "publishDate": "2025-08-05T09:15:01Z",
        "author": "AICodeKing",
        "sourceType": "youtube",
        "sourceName": "AI Code King YouTube Channel",
        "metadata": {
            "channelId": "UC0m81bQuthaQZmFbXEY9QSw",
            "thumbnailUrl": "https://i.ytimg.com/vi/EMXMKR-881M/hqdefault.jpg",
            "transcription": "Hi, welcome to another video. So, most of the models and coders these days are not good at proper complex front-end tasks. They struggle pretty hard with complex and real life front-end tasks. Due to that, I've talked about some of the ways you can use system prompting or tools to make the best UIs, but they have been quite tedious to set up and almost always, they never work seamlessly. But, recently, I saw something being talked about a lot, which aims to fix these issues. It had a massive launch on Twitter with over 2 million views. And I was quite intrigued by the benchmark scores, that tool was scoring on the front-end related benchmarks. Their tests were specific to real-world use cases for front-end tasks and their performance warranted that I try it. So, I put it to the real test to check if it matches its claims. This one is called Kombai. Kombai is the first AI agent for front-end development. It plugs into your code editor, whether it's VS Code, Cursor, or Windsuf and it can generate front-end code right inside your IDE. You get clean, backend-agnostic output that fits your stack and repo. It can build small UIs or full-fledged multi-page UIs, following one kind of design with the best practices. It significantly outperforms out of the box Frontier models and MCPs in building UX from Figma, image, or text based prompts. Basically, what it does is, you can give it a Figma file, some images, or even just a text prompt and Kombai will generate clean front-end code that's backend-agnostic. That means it's not going to mess with your backend logic. It just focuses on the UI. And the code it generates is meant to fit your stack and repo, so you don't have to do a ton of refactoring, which is quite awesome. Plus, it supports a bunch of libraries, like React 19, Next 15, MUI, Mantine, Tailwind V4, Ant D, Shadcn, and more. That's a big deal because most other AI tools kind of struggle with specific front-end libraries or they just give you generic output that you still have to tweak a lot. You can use it for free with 200 credits per month. But there are also offering a generous launch exclusive offer that fetches you 500 credits every month for free. And there's also a $20 per month plan that gives you 2,000 credits per month with options for more if you need even more credits. It's really well priced for sure. Anyway, now, let me show you how you can use it as well. First of all, just head over to the Kombai site through the link in the description. Then hit the install option and select your editor. It will open up the VS Code marketplace. Just hit the install button and it will get installed. Once done, just open up the plugin, then sign in or create an account, activate your account and it should get started. Now, this is what it looks like. First of all, you can see the simple prompt box here. Here, you can type in whatever you want to generate and you can also select between two modes. Ask and code. Ask is for chatting, questions and planning. While code actually generates the code and does what you expect it to. Then, you can also edit the text stack that you want to use. You can see what text stack you're currently using. And you can hit this pencil icon to open up the settings page, where you can change the text stack you want it to use. You can select between React 19, TypeScript or JavaScript, as well as NextJS 15. To show you how this tool excels at complex frontends that other prototyping and LLMs cannot do. I'm going to use it on an existing e-commerce admin panel that I built using Kombai. You can see it here. And this has all the stuff that you'd want. It uses mock data via a public Fake Store API. And it looks quite good. You can like use it to view charts as well as search products and every functionality here works pretty well. This app is reasonable complex and built with React 19, along with Material UI V7, as well as Emotion. So, if I show you that how I created it, then you can see that first, I gave it the Figma design here. And I asked it to code for me, which it did quite well. And I followed up with some prompts to make it best. Then it did that. And the main dashboard was made here. But then I asked it to also work on the product pages. And I asked it to use the Fake Store demo API to make sure that it works well and the search and everything works. So, I did that. And it just made the product pages for me as well. I asked it to make the product list view here, which you can see in the history here as well. Now, to demonstrate the power of this tool, I'm going to show you how it's able to work with this real codebase by adding a complex feature to the app. If we go back to the preview of the app, then you can see that we have the settings page here. But this one doesn't work, as the page for it is absent. So, let me show you that how we can build it. Let's ask it to borrow some components from the dashboard and product page. And then build me the settings page and link it as well. With what it thinks should be put in it. First, it will generate a plan, as you can see here. It will create the files and everything. And then in a bit, it will be done. You can see that it wrote all the code for us and it used the libraries present in the codebase. Once the code is generated, you can review and approve it. After code generation, Kombai runs the code in a sandbox and shows a preview of the output before you even save it to your repo. This helps you quickly visualize the impact of the generated code. Let's see what Kombai made for us. You can view it now and you can see that this looks amazingly good. It used the design libraries correctly, meaning that there are no overriding styles or janky styling. It is all professionally made. And it also follows a theme throughout. Rather than things looking out of place, which is pretty challenging to do with general coders. I want to change some things in it. Like, I want the option to keep the profile name different from the real name. So, I can ask it to add another text box for the username. And then it will go ahead and get that added for me as well. Which I can view again. It is really good. And it already has made the backend routes and schema for us. Which means that we can ask our AI coder, like Cursor or Kline, to just make and plug in the backend for us and make it functional. This is probably the best way to make functional and good-looking designs that work for complex or professional apps. Actually, I'm just going to go over to Kilo Code here. And I'm going to select the Sonnet model. Now, I'm going to ask it to implement the backend for me and make it fully functional. Now, you'll see that it will go ahead and implement the backend for me. And in a bit, it gets done. We can have a look now by running it. And as you can see, this works amazingly well and it is quite good for sure. This might be the best way to implement and create real-world complex apps. It works with Figma designs and is actually super optimized for that. So, you can use that as well. I think that this is probably the best agent for front-end development that I have seen yet. You can go ahead and give this a try and you'll love it for sure. Overall, it's pretty cool. Anyway, share your thoughts below and subscribe to the channel. You can also donate via Super Thanks option or join the channel as well and get some perks. I'll see you in the next video. Bye."
        }
    },
    {
        "id": "tVHZy-iml5Q",
        "title": "Genie 3: The World Becomes Playable (DeepMind)",
        "content": "Soon, anything will be playable. A photo becomes an interactive world, a selfie becomes a new game. Genie 3 from Google, ...",
        "url": "https://www.youtube.com/watch?v=tVHZy-iml5Q",
        "publishDate": "2025-08-05T16:37:28Z",
        "author": "AI Explained",
        "sourceType": "youtube",
        "sourceName": "AI Explained YouTube Channel",
        "metadata": {
            "channelId": "UCNJ1Ymd5yFuUPtn21xtRbbw",
            "thumbnailUrl": "https://i.ytimg.com/vi/tVHZy-iml5Q/hqdefault.jpg",
            "transcription": "In the week that we are set to get GPT-5, it might be easy to miss this announcement of Google DeepMind's Genie 3. To cut a long story short, it makes the world playable. Start with an image, which could be one of your photos, and then enter that world and modify it with prompts. By entering, I mean you can move around, take actions that last and stay in that world, and basically go wild.\n\nI was given early access to the presentation of Genie 3, and got to ask the makers a question. But I'm going to be honest, Genie 3 is designed and marketed to allow AI agents to act out scenarios and self-improve at taking actions. That's the theory. For me, and let me know if you agree, it will be used much more for gamifying all of reality and your imagination. If you have been following the channel for just a little bit, you know that I interviewed a senior researcher on Genie 2, Tim Rocktäschel, here and on my Patreon. And at the time, we learned that Genie 2 would quote, \"scale gracefully with more compute.\" Well, it did, and now we get real-time interaction in 720p, 24 frames per second. If that's jargon to you, it means you can click some buttons and things happen at the exact same time on screen at fairly high resolution.\n\nNow, in a couple of minutes, I'm going to show you the full intro, which is about 130 seconds, I think, which is unusual for this channel. I don't normally show clips that long, but it does showcase Genie 3 really quite well. First though, just a few thoughts from me. Jack Parker Holder, the lead author of Genie 3, told me and a bunch of journalists that the goal behind it was to have a Move 37 moment for embodied AI, as in for robots, not just for computers that play games. A Move 37 moment is a high bar, as any of you who have watched the AlphaGo documentary know, but think of it as a novel breakthrough that goes beyond the human data. In other words, we just don't have enough data to train robots reliably, given the innumerable scenarios in which they'll be placed. If we can simulate all worlds, then we might get novel breakthroughs for those robots. Get them to do things, essentially, that we couldn't have even trained them to do. In the presentation, I pushed back though, with the question that if these worlds suffer from physics inaccuracies, and they do, how would such agents ever be fully reliable? Both lead authors agreed that's a real issue, but then raised something that got me thinking. They said that yes, while you can't guarantee reliability, you can demonstrate unreliability. Think about it: if an agent goes off the rails in simulation, then it's also liable to do so in the real world. In a way then, I think both of these points still stand. I think we can't guarantee reliability with simulators like Genie 3, but we can help find unreliability. Anyway, what you're probably thinking, and I definitely was, was that we should just be honest with ourselves. Everyone is going to want to upload a still from their favorite game, life event, celebrity, or what have you, and basically interact with it, jump around, paint a wall, and just get silly. And even that is probably phrasing things somewhat maturely, which is probably why this is currently still a research preview, meaning you can't get your hands on it. Google were pretty evasive about timing for a general release, not even a hint of a date. However, if that disappoints you, I am old enough to remember that that same, I guess, not-for-general-release safety issues kind of thing, was true of Imagen 1, the very basic image generator from Google, basically not fit for public release. But as of today, we have Imagen 4 out in public, far improved, and even available on the API, so developers can incorporate it into their apps. Translated, Genie 4 might be available to you to play with sooner than you think. Okay, but what about that incredible memory where you could paint a wall, for example, look around, come back, and the paint is still there? Let's just take a moment and say Google, that is pretty impressive. Well done. But the memory within these worlds is measured in minutes, not hours. So if you were thinking of making a friend in one of these worlds, building a house together, and living in it to escape the real world and its current self-immolation, that won't quite work. As it currently stands, by the time you return to the house the next day, it will be completely reimagined. And Google told me of four other caveats. I think they are pretty telling about the future of simulation, so let's go through them. First, while the most common actions are performable, as you'd find in games like moving around and jumping, you can't currently perform complex actions. Next, and this thought literally just came to me, but it's a bit like a dream in that the next caveat is that you can't talk to other characters. Maybe that's just me, but in your dreams, do you speak to other people? Definitely not complex conversations. Anyway, they said to me, \"Accurately modeling complex interactions between multiple independent agents is still an ongoing research challenge.\" Third, as you would expect, we can't expect accurate representation of real-world locations. The sheer imaginative scope of these worlds are also somewhat their downfall in that lifelike fidelity is not their priority. That bleeds into the fourth caveat they gave me, which is text rendering. Don't expect high-fidelity text rendering. It can happen if you add it to your prompt, it's just not built into the environment. Now, funnily enough, I think it was a Guardian or New York Times journalist asked actually about whether this is a replacement for something like Omniverse or Unreal Engine. Google wouldn't say that, but they did say that hardcoding the complexity of the real world is intractable. So that's why we might need simulations like the Genie series. I know quite a few game developers watch the channel, so do chip in with your thoughts on this versus Unreal Engine. And I would add, there's a hybrid approach, which I saw recently in a TED Talk from a guy from Roblox. I forget his name and his rank, but the idea was that you could prompt a model to directly code new parts of the environment. The full six-minute talk is linked in the description, but this feels to me like it would be slightly more predictable perhaps, but maybe less scalable because with the Genie series, you could scale it with billions of hours of video from YouTube, not so much with hardcoded assets. Which approach will win out? I actually don't know, so let me know what you think. Now, enough buildup. There is no paper to go through. I was going to release this video at 3pm when the embargo lifted, but I thought maybe they're going to give us a paper, so let's hold back. No, there was no paper. So here is the around two-minute demo that I promised, albeit slightly later than I said I would give it.\n\n[Video demonstrating Genie 3 functionality: interactive worlds generated from prompts, showing movement, persistence of actions, and promptable events like a chicken suit man and a dragon. Examples include landscapes, city streets, beaches, a field with hot air balloons and a tractor, a canyon, a classroom, and a snowy landscape with generated structures.]\n\nWhat you're seeing are not games or videos. They're worlds. Each one of these is an interactive environment generated by Genie 3, a new frontier for world models. With Genie 3, you can use natural language to generate a variety of worlds and explore them interactively. All with a single text prompt. Genie 3 has real-time interactivity, meaning that the environment reacts to your movements and actions. You're not walking through a pre-built simulation. Everything you see here is being generated live as you explore it. And Genie 3 has world memory. That's why environments like this one stay consistent. World memory even carries over into your actions. For example, when I'm painting on this wall, my actions persist. I can look away and generate other parts of the world. But when I look back, the actions I took are still there. And Genie 3 enables promptable events, so you can add new events into your world on the fly. Something like another person, or transportation, or even something totally unexpected. You can use Genie to explore real-world physics and movement, and all kinds of unique environments. You can generate worlds with distinct geographies, historical settings, fictional environments, and even other characters. We're excited to see how Genie 3 can be used for next-generation gaming and entertainment. And that's just the beginning. Worlds could help with embodied research, training robotic agents before working in the real world, or simulating dangerous scenarios for disaster preparedness and emergency training. World models can open new pathways for learning, agriculture, manufacturing, and more. We're excited to see how Genie 3's world simulation can benefit research around the world. deepmind.google/genie\n\nTrying to game out the impact of technologies like Genie on jobs is just too complex for me at the moment, but there are real world jobs you can apply to via the sponsors of today's video, 80,000 Hours. If you, somewhat helpfully, use my link in the description, then you'll go to their job board, which you can see, and these are all real jobs related to AI. Well, I think the majority relate to AI, but either way, the jobs are sourced from around the world. Now, you could say, why even cover Genie 3? And don't worry, I will be touching on Gemini DeepThink on the main channel, which is also from Google DeepMind, soon enough, and my early review of that tool is on Patreon. But it just feels inevitable to me that people will initially want their games to be infinitely playable. Think a map size bigger than GTA 7. As expectations continue to rise, they'll want their entertainment to be interactive. Say, prompting Netflix to add their own face into Squid Game US edition, and it will just never stop. It will then be in VR, in 16k. You'll be able to speak to other agents, or let's just call them bots. The other characters in these simulated worlds will be pretty intelligent. They probably won't just keep walking into walls. You can like chat with them about Sophocles. Some people may even need to watch their step, lest they fall into these infinite worlds. Others will dive in headlong. But the step up in resolution and memory, and the commitment from Google to incorporate this into their march to AGI, seems noteworthy. These worlds then will be born one way or another. But the question for me is whether a fully imagined simulation is the way, or instead my bet, which is something more like Isaac Lab from NVIDIA. Simulated, but also programmable, and so repeatable. Soon enough, many worlds are about to get crazy, not just the real one. Thank you so much for watching to the end. I look forward to covering GPT-5 with you guys this week, almost certainly. Have a wonderful day."
        }
    },
    {
        "id": "O0vD6nk9_kQ",
        "title": "How Tsar Nicholas Controlled Russia - Sarah Paine",
        "content": "",
        "url": "https://www.youtube.com/watch?v=O0vD6nk9_kQ",
        "publishDate": "2025-08-05T18:39:58Z",
        "author": "Dwarkesh Patel",
        "sourceType": "youtube",
        "sourceName": "Dwarkesh Patel YouTube Channel",
        "metadata": {
            "channelId": "UCXl4i9dYBrFOabk0xGmbkRA",
            "thumbnailUrl": "https://i.ytimg.com/vi/O0vD6nk9_kQ/hqdefault.jpg",
            "transcription": "You're saying Russia doesn't care about this conflict as- Russians. Tsarnicholas might, but his population is fighting it. So then he doesn't have the state capacity to mobilize? Oh, that's the whole problem with Russia. This is the power of institutions. Japan clearly can mobilize. Boy is it doing well with the loans and all sorts of things. It's using these institutions. Whereas Russia, remember it doesn't have a legislature. The Tsar doesn't have a cabinet in the sense. He has ministers, but they don't ever show up at his house at the same time to sit around a table. He's just doing them one at a time. And then the Romanov family have lots of first cousins, right? You got everybody's having lots of kids, and so you got a million first cousins, and they are all deployed throughout the ministries, basically being the spy system for the Romanov family of what's going on. Are any of these nice rich boys particularly competent? No."
        }
    }
]