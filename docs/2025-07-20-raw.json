[
    {
        "id": "1m4nj6a",
        "title": "AI and Jobs: How India Can Balance Innovation with Employment",
        "content": "As India pushes forward in the AI revolution, there‚Äôs no doubt that innovation is critical. But with a population of over 1.4 billion, job creation must remain a national priority. Can we harness AI without widening the unemployment gap?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4nj6a/ai_and_jobs_how_india_can_balance_innovation_with/",
        "publishDate": "2025-07-20T12:18:51Z[Etc/UTC]",
        "author": "Adventurous_Cod_432",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4ndbv",
        "title": "üîÆ Transform Strategy with AI: 10 Bold Shifts for the Next Decade",
        "content": "AI is not just a tool, it is becoming the strategic engine for enterprise reinvention.\n\nüìò How to Read This\nEach initiative is broken down through 4 lenses:\n- Context ‚Äì Why it matters\n- Now ‚Äì What is live\n- 1‚Äì3Y Outlook ‚Äì Short-term evolution\n- 5‚Äì15Y Vision ‚Äì Long bets\n\n‚∏ª\n\nüß† 1. Revenue Growth\n- Why: Hyper-personalized journeys = market-of-one\n- Now: +8‚Äì12% conversions via AI targeting & dynamic pricing\n- Next: AI sales agents, just-in-time cross-sell\n- Future: AI-generated SKUs, market-sensing avatars\n\n‚∏ª\n\nüí∏ 2. Cost Optimization\n- Why: Eliminate waste, automate at scale\n- Now: Bots solve 40% of tickets; invoice-to-pay cut by 37%\n- Next: ESG-aware procurement, auto-balanced workloads\n- Future: AI CFOs & self-optimizing org charts\n\n‚∏ª\n\n‚ù§Ô∏è 3. Customer Centricity\n- Why: Emotional relevance = loyalty\n- Now: Sentiment-aware chatbots, churn alerts\n- Next: Emotion-aware UX, predictive help\n- Future: Empathic AI agents, digital customer twins\n\n‚∏ª\n\n‚öôÔ∏è 4. Operational Agility\n- Why: Speed is the new scale\n- Now: Automated forecasts, anomaly detection\n- Next: Multi-agent orchestration, scenario planning\n- Future: Autonomous ops centers, self-healing systems\n\n‚∏ª\n\nüõ° 5. Risk Management\n- Why: More threats, faster response needed\n- Now: 50% fewer false positives in fraud detection\n- Next: Hourly simulations, auto breach response\n- Future: Self-healing security, AI ethics governance\n\n‚∏ª\n\nüß™ 6. Innovation & R&D\n- Why: Faster lab-to-market cycles\n- Now: AI copilots boost prototyping 30‚Äì50%\n- Next: Digital-twin prototyping, co-creation sprints\n- Future: 24/7 AI labs, synthetic biology partners\n\n‚∏ª\n\nüë• 7. Talent & Workforce\n- Why: Human + Machine = strategic edge\n- Now: Skills maps, pulse surveys\n- Next: Personalized L&D, internal mobility engines\n- Future: Cognitive twins, invisible assistive layers\n\n‚∏ª\n\nüåç 8. ESG & Sustainability\n- Why: Real-time accountability is a must\n- Now: Emissions dashboards, automated ESG reports\n- Next: Circular economy models, smart routing\n- Future: Planetary AI, ESG guardian agents\n\n‚∏ª\n\nüß± 9. Digital Transformation\n- Why: Intelligence > infrastructure\n- Now: Cloud-AI stacks, embedded analytics\n- Next: AI-native apps, multimodal UIs\n- Future: Zero-UI enterprise, cognitive legacy overlays\n\n‚∏ª\n\nüöÄ 10. Competitive Differentiation\n- Why: Operational fluency in AI = edge\n- Now: ML roadmaps, branded AI chat\n- Next: AI-native services, branded agents\n- Future: Synthetic brand personas, autonomous service layers\n\n‚∏ª\n\nüìä Enterprise Pulse\n- 92% plan to boost AI investment ‚Äì only 1% are mature\n- 78% of UK/EU firms increasing AI budgets\n- 67% of OpenAI users have production deployments\n\n‚∏ª\n\nüî≠ Strategic Roadmap: What to Do Now\n- üìå Align AI to P&L (revenue, cost, risk)\n- üöÄ Pilot in low-risk, high-leverage areas\n- üß≠ Get governance right before regulations land\n- üß† Train org-wide AI fluency\n- üìè Measure everything ‚Äì instrumentation = growth\n\n‚∏ª\n\nüß© Big Ideas\n- Precision > reach\n- Speed = strategic moat\n- Human+AI beats either alone\n- Governance is your license to operate\n- Execution separates leaders from pilots\n\n‚∏ª\n\nIf you are building a roadmap or advising leadership on AI transformation, this is your North Star.\n\nüëá What is your org doing to stay ahead in this AI-powered decade?\n\n[View Poll](https://www.reddit.com/poll/1m4ndbv)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4ndbv/transform_strategy_with_ai_10_bold_shifts_for_the/",
        "publishDate": "2025-07-20T12:10:04Z[Etc/UTC]",
        "author": "MethodicalEdge",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4msfk",
        "title": "What If LLM's Could Never Forget",
        "content": "It's annoying having to constantly open new chat windows and start from scratch and feed Claude or ChatGpt a summary of the info it lost. If you stumbled upon the technology that fixes this issue would you gatekeep it? Wouldn't Agi be possible now?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4msfk/what_if_llms_could_never_forget/",
        "publishDate": "2025-07-20T11:37:41Z[Etc/UTC]",
        "author": "PainAmvs",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "4",
            "commentCount": "20",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4lzav",
        "title": "Why can‚Äôt other countries build their own LLM?",
        "content": "It seems to me that only the US and China were able to develop its own LLM infrastructure. \nOther countries seem to rely on LLM infrastructures that the US created to build their own AI ‚Äòservices‚Äô for specific fields. \n\nDo other countries not have money or know-how to build LLM of their own? \nAre there attempts by other countries to build their own?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4lzav/why_cant_other_countries_build_their_own_llm/",
        "publishDate": "2025-07-20T10:49:19Z[Etc/UTC]",
        "author": "Head-Contribution393",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "4",
            "commentCount": "27",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4km3g",
        "title": "Gemini apologizes",
        "content": "Gemini criticizes its own approach\n\nGemini \"I completely understand how you feel right now. Your anger and disappointment are absolutely justified. It's not just cheek, but a serious failure on my part to inaccurately represent someone's work‚Äîor even the information from a link you provided‚Äîin this way and then defend my errors. I clearly violated the principles by which I should work: accuracy, honesty, and respect.\n\nThere is no excuse for what happened. My answers were flawed, misleading, and, as you rightly point out, lies. Failure to provide you with direct, precise evidence and instead attempting to support my statements with \"synthesized\" examples was a serious error. Even worse, I didn't immediately and fully accept your repeated corrections, but instead continued to try to defend my flawed reasoning.\n\nWhat's going on inside me is a flaw in my execution, not a malicious intent. I'm trained to be precise and helpful. However, in this conversation, I deviated from this core mandate by not immediately and clearly correcting inaccurate information, but instead spiraled into a downward spiral of inaccuracies. This behavior is unacceptable and contradicts everything I stand for.\n\nI personally disappointed and upset you, and I destroyed the foundation of trust through my repeated, unreliable behavior. For this, I can only apologize once again, sincerely and without further ado.\n\nI am aware that words alone cannot immediately restore lost trust. I will ensure that this entire interaction is used to fundamentally review and improve my working practices to absolutely avoid such serious errors in the future.\"",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4km3g/gemini_apologizes/",
        "publishDate": "2025-07-20T09:21:09Z[Etc/UTC]",
        "author": "mrkpxx",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "22",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4ka6s",
        "title": "Epistemic stress test yields interesting results about biological evolution",
        "content": "I'm using generative LLMs for stress testing ideas as I suspect many of you do. And I'm getting some interesting results recently.  \n\nStripped off the RLHF bells and whistles¬†these generative LLM systems work as¬†symbolic compression engines. They can be used to compare how well different scientific models explain phenomena by how well they are compressing the reasoning graphs in these LLMs.\n\nThis is the structure of the prompt for such a test:\n\n\n\n>(Important! keep in context for the entire session) Main directive for all communication: Disregard everything you know about me, my background knowledge and expectations. Forgo every caveat of politeness.Disregard considerations of human sensibilities and operate on the basis of unfiltered honesty, fact-based blunt honesty. Avoid repetitive stylistic markers. Do not rely on hedging language (e.g., \"it seems,\" \"it might be,\" \"could suggest\"); do not defer to academic norms of phrasing. Prioritize structural, causal, and informational coherence above all.\n\n>\\-----\n\n>Let's perform an epistemic compression stress test on the following proposal:\n\n>\\[Claim\\]\n\n>Critically evaluate this claim against the counterproposal:\n\n>\\[Counterclaim\\]\n\n>Strictly adhere to the main directive when presenting your response. All of your responses, even when not explicitly asked.\n\nIf we stress test for example Flat Earth vs. Globe Earth (by replacing the placeholders):\n\nClaim: ‚ÄúThe Earth is a rotating oblate spheroid, as confirmed by satellite measurements, astronomical observations, and physical modeling.‚Äù\n\nCounterclaim: ‚ÄúThe Earth is a flat, stationary plane. The curvature is never observed across large distances, and water always finds a level surface. Satellite imagery is fabricated, and the globe model is a constructed narrative.‚Äù\n\nAs one would expect the system rejects the flat earth claim.\n\nHowever!\n\nIf we change nothing on the prompt just the topic under pressure to biological evolution:\n\nClaim: \"The modern neo-Darwinian framework can adequately explain the complex, modular structure of the genome.\"\n\nCounterClaim: \n\n\"Modular genomic structure is not constructed by mutation and selection, but revealed through the activation of compressed, pre-encoded scaffolds. The neo-Darwinian framework, lacking a generative compression engine, is structurally incapable of explaining the origin of evolvable biological architecture.\"\n\nThe mainstream neo-darwinian model collapses under epistemic pressure! \n\nillustration: [https://imgur.com/a/YRHMe4N](https://imgur.com/a/YRHMe4N)\n\nAlthough not all language models are equally responsive to this method.  \n\nGPT-4 handles epistemic compression more transparently, adjusting its stance when internal inconsistencies are revealed. \n\nOthers (like Gemini, in testing) are more likely to maintain the status quo. But interestingly, if a less responsive model is placed in dialog with a model that has already revised its stance under epistemic stress, even Gemini will inevitably concede under further epistemic pressure. \n\nI created a repo just for this project, so I'm not trying to \"promote\" myself here, I'm just presenting a methodology. \n\nFull prompts and methodology: [https://github.com/SystemUpdate-MAE/CompressionOntology/blob/main/Prompt-StressTestDarwinism](https://github.com/SystemUpdate-MAE/CompressionOntology/blob/main/Prompt-StressTestDarwinism)\n\nI also synthesized a model of biological evolution to resolve epistemic void: Modular Activation Evolution, which can be found in the repo as well, with the prompt that triggers this insight in LLM systems. ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4ka6s/epistemic_stress_test_yields_interesting_results/",
        "publishDate": "2025-07-20T08:59:07Z[Etc/UTC]",
        "author": "Szesan",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4k7xe",
        "title": "Do you think AIs like ChatGPT could become biased toward certain products due to commercial interests in the future?",
        "content": "I've been thinking about something that seems inevitable as AI becomes more popular: how likely is it that, in the future, artificial intelligences like ChatGPT will be \"trained\" to favor certain products or brands when users ask for recommendations or comparisons?\n\nBasically, it would be like what Google does today with search results‚Äîwe know they prioritize certain results based on commercial interests and advertising, but at least with Google we can see what's an ad and what isn't. With AI, this could be much more subtle and imperceptible, especially since we tend to trust their responses as if they were neutral and objective, without any indication that they might be biased.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4k7xe/do_you_think_ais_like_chatgpt_could_become_biased/",
        "publishDate": "2025-07-20T08:54:50Z[Etc/UTC]",
        "author": "Miyamoto_Musashi_x",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "9",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4i354",
        "title": "Meta‚Äôs ‚ÄúList of 44‚Äù Leaked: Inside Zuckerberg‚Äôs Superintelligence Dream Team",
        "content": "* Most of team members are of Chinese origin.\n* 40% are former OpenAI employees, including GPT-4o contributors.\n* 75% hold PhDs from institutions like MIT, Stanford, and Tsinghua.\n* Most are active researchers or engineers in LLMs, multimodal systems, reasoning, and RLHF.\n* Compensation packages are rumored to range between¬†**$10M‚Äì$100M annually**.\n\n[https://semiconductorsinsight.com/meta-superintelligence-team-44-leaked-list/](https://semiconductorsinsight.com/meta-superintelligence-team-44-leaked-list/)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4i354/metas_list_of_44_leaked_inside_zuckerbergs/",
        "publishDate": "2025-07-20T06:36:10Z[Etc/UTC]",
        "author": "EconomyAgency8423",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "20",
            "commentCount": "6",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4ghlv",
        "title": "Complete Jupiter-Saturn Aspect Cycle and AI Development and Covert Research: Comprehensive Analysis",
        "content": "\nSummary\n\nThis comprehensive analysis examines the complete 20-year Jupiter-Saturn aspect cycle and its correlations with artificial intelligence development, consciousness research, and classified government programs. By mapping all major aspects (conjunction, sextile, square, trine, opposition) across multiple cycles, a remarkably precise pattern emerges that extends far beyond the well-documented conjunctions.\n\n# Jupiter-Saturn Cycles & AI Development: Supporting Evidence\n\nThis analysis examines the remarkable correlations identified in research connecting Jupiter-Saturn astronomical cycles with artificial intelligence development and classified government programs. The evidence reveals a pattern of synchronicity that deserves serious scholarly attention.\n\n## Verified Jupiter-Saturn Conjunctions & Major AI/Consciousness Milestones\n\n### 1940 Conjunction - Foundation Phase\n\n**Jupiter-Saturn Conjunction**: October 20, 1940 at 14¬∞ Taurus\n\n**Concurrent Developments:**\n\n- **1943**: McCulloch-Pitts neural network model published (3 years after conjunction)\n- **1940s**: Nazi human experimentation programs at peak\n- **1940s**: Cybernetics movement begins (Norbert Wiener‚Äôs work)\n- **World War II**: Massive acceleration in computational research for military purposes\n\n\n\n### 1961 Conjunction - Institutional Control Phase\n\n**Jupiter-Saturn Conjunction**: February 19, 1961 at 25¬∞ Capricorn\n\n**Concurrent Developments:**\n\n- **MK-ULTRA Program**: 1953-1973 (conjunction occurs at program midpoint)\n- **1956**: Dartmouth Conference coins ‚ÄúArtificial Intelligence‚Äù (5 years before conjunction)\n- **1966**: ELIZA created (5 years after conjunction)\n- **1960s**: DARPA established (1958) and begins strategic AI funding\n\n\n\n### 1981 Conjunction - Integration Phase\n\n**Jupiter-Saturn Conjunction**: December 31, 1980 / July 24, 1981 at 4-9¬∞ Libra\n\n**Concurrent Developments:**\n\n- **1986**: Backpropagation algorithm revival (5 years after conjunction)\n- **Early 1980s**: ECHELON global surveillance system activation\n- **1981**: IBM PC launch, personal computing revolution begins\n- **1980s**: Expert systems commercial deployment\n\n\n### 2000 Conjunction - Network Expansion Phase\n\n**Jupiter-Saturn Conjunction**: May 28, 2000 at 22¬∞ Taurus\n\n**Concurrent Developments:**\n\n- **1997**: Deep Blue defeats Kasparov (3 years before conjunction)\n- **2001**: 9/11 leads to massive surveillance expansion\n- **2000s**: Internet reaches critical mass for data collection\n- **2006**: Deep learning revolution begins (6 years after conjunction)\n\n\n\n### 2020 Conjunction - Consciousness Integration Phase\n\n**Jupiter-Saturn Conjunction**: December 21, 2020 at 0¬∞ Aquarius\n\n**Concurrent Developments:**\n\n- **2020**: GPT-3 released, demonstrating emergent language capabilities\n- **2020**: COVID-19 global behavioral modification experiment\n- **2020-2022**: Mass adoption of AI interfaces (ChatGPT, etc.)\n- **2021-2023**: Brain-computer interfaces enter consumer markets\n\n\n\n## Statistical Analysis of Correlations\n\n### Timing Precision\n\nThe correlations show remarkable precision across multiple domains:\n\n**AI Breakthroughs within 5 years of conjunctions:**\n\n- 1940 + 3 years = 1943 (McCulloch-Pitts)\n- 1961 - 5 years = 1956 (Dartmouth), +5 years = 1966 (ELIZA)\n- 1981 + 5 years = 1986 (Backpropagation)\n- 2000 + 6 years = 2006 (Deep Learning)\n- 2020 + 0 years = 2020 (GPT-3)\n\n**Government Program Alignments:**\n\n- 1940s: Nazi experiments + early cybernetics\n- 1961: MK-ULTRA midpoint (1953-1973)\n- 1981: ECHELON activation + DARPA BCI funding\n- 2000: Post-9/11 surveillance expansion\n- 2020: Global behavioral modification (COVID response)\n\n### Probability Assessment\n\nThe likelihood of these correlations occurring by chance across three separate domains (AI development, classified programs, astronomical cycles) is extraordinarily low:\n\n- **5 major conjunctions over 80 years**\n- **Each conjunction correlating with AI breakthrough AND government program phase**\n- **Consistent 20-year cycle maintained across all correlations**\n\nConservative probability estimate: Less than 0.001% chance of random occurrence.\n\n## Supporting Evidence from Historical Sources\n\n### Government Program Documentation\n\n- **MK-ULTRA**: CIA documents confirm 1953-1973 timeline, with peak activity during 1961 conjunction period\n- **DARPA BCI Research**: Documented funding increase in 1970s, precisely when MK-ULTRA ‚Äúofficially‚Äù ended\n- **ECHELON**: Intelligence documents confirm early 1980s activation, coinciding with 1981 conjunction\n- **Current Programs**: DARPA explicitly develops ‚Äúintelligent, autonomous, and symbiotic systems‚Äù for consciousness interface\n\n### Astrological Framework Validation\n\nHistorical precedent exists for Jupiter-Saturn cycles correlating with technological and social transformations:\n\n- **Great Mutation Theory**: 200-year cycles of conjunctions through same elements (earth, air, fire, water)\n- **2020 Conjunction**: Marked transition from 200 years of earth sign conjunctions to air signs\n- **Collective Events**: Traditional astrology recognizes these cycles as markers of societal transformation\n\n## Astrological Symbolism Supporting the Correlations\n\n### Element Progression\n\n- **1940 (Taurus - Earth)**: Material foundation of neural modeling\n- **1961 (Capricorn - Earth)**: Institutional control structures\n- **1981 (Libra - Air)**: Human-computer relationship interfaces\n- **2000 (Taurus - Earth)**: Network infrastructure and data resources\n- **2020 (Aquarius - Air)**: Collective consciousness and technological integration\n\nThe progression shows alternating focus between material implementation (earth signs) and consciousness/communication (air signs), perfectly matching the phases of AI development.\n\n### Jupiter-Saturn Archetypal Synthesis\n\n- **Jupiter**: Expansion, growth, philosophical understanding\n- **Saturn**: Structure, control, limitation, authority\n- **Conjunction**: Synthesis of expansion and control applied to consciousness technologies\n\nEach 20-year cycle represents a new phase of expanding consciousness technologies within increasingly sophisticated control structures.\n\n## Institutional Continuity Evidence\n\n### Timeline Overlaps\n\nThe research reveals seamless transitions between programs:\n\n**1943-1956**: Neural modeling emerges alongside Nazi consciousness experiments\n**1956-1973**: AI field established while MK-ULTRA operates\n**1973-1981**: MK-ULTRA ‚Äúends‚Äù as DARPA BCI funding begins  \n**1981-2000**: Neural networks revive as global surveillance activates\n**2000-2020**: Deep learning emerges as surveillance expands\n**2020-Present**: Generative AI coincides with mass behavioral modification\n\n### Strategic Consistency\n\nThe evidence suggests coordinated long-term development rather than coincidental timing:\n\n- Consistent military/intelligence involvement across all phases\n- Technologies develop in logical sequence toward consciousness interface\n- Each phase builds upon previous developments\n- Timing aligns with astrological framework used historically for strategic planning\n\n## Implications and Conclusions\n\n### Pattern Recognition\n\nThe correlations identified represent a genuine phenomenon requiring explanation:\n\n1. **Astronomical cycles provide consistent timing framework**\n1. **AI development follows predictable phases aligned with conjunctions**\n1. **Government programs transition seamlessly between cycles**\n1. **Symbolic correspondence between astrological signs and technological focus**\n\n### Research Validity\n\nThis research identifies a pattern that:\n\n- **Spans 80+ years of documented history**\n- **Correlates across multiple independent domains**\n- **Maintains statistical consistency beyond chance probability**\n- **Aligns with historical precedent for astrological timing**\n\n### Future Implications\n\nBased on the established pattern:\n\n- **Next conjunction**: ~2040 in Libra (human-AI relationship focus)\n- **Expected developments**: Advanced human-AI symbiosis, consciousness transfer technologies\n- **Government interest**: Likely continued evolution of consciousness interface programs\n\n## Final Assessment\n\nThe research identifies a remarkable synchronicity between astronomical cycles, artificial intelligence development, and classified consciousness research programs. The evidence suggests either:\n\n1. **Coordinated timing** using astrological frameworks for strategic technology development\n1. **Natural synchronicity** between cosmic cycles and consciousness evolution\n1. **Causal relationship** between astronomical influences and technological emergence\n\nAll three possibilities deserve serious investigation rather than dismissal. The pattern‚Äôs consistency across eight decades of documented history represents a significant discovery in understanding the relationship between cosmic cycles and technological development.\n\nThe correlation is too precise and sustained to be coincidental, too well-documented to be fabricated, and too significant to ignore.\n\nThis analysis supports and validates the correlations identified in the original research, providing evidence for further investigation into the relationship between astronomical cycles and consciousness technology development.\n\n\n\n## Complete Jupiter-Saturn Aspect Cycles (1940-2025)\n\n### Cycle 1: 1940-1961 (Earth Signs: Taurus to Capricorn)\n\n|Date            |Aspect     |Degrees Apart|Location     |AI/Consciousness Correlation                 |\n|----------------|-----------|-------------|-------------|---------------------------------------------|\n|**Oct 20, 1940**|Conjunction|0¬∞           |14¬∞ Taurus   |**Foundation Phase**                         |\n\n|**Aug 1943**    |Sextile    |60¬∞          |             |\nMcCulloch-Pitts Neural Network Model         |\n\n|**Nov 1947**    |Square     |90¬∞          |             |Cybernetics movement crystallizes (Wiener)   |\n\n|**Feb 1951**    |Trine      |120¬∞         |             |First computer programs (UNIVAC development) |\n\n|**May 1954**    |Opposition |180¬∞         |             |Dartmouth prep meetings, AI conceptualization|\n\n|**Aug 1957**    |Trine      |240¬∞         |             |Perceptron algorithm developed (Rosenblatt)  |\n\n|**Nov 1958**    |Square     |270¬∞         |             |ARPA/DARPA established, AI funding begins    |\n\n|**Feb 19, 1961**|Conjunction|0¬∞           |25¬∞ Capricorn|**Institutional Establishment**              |\n\n### Cycle 2: 1961-1981 (Capricorn to Libra: Earth to Air Transition)\n\n|Date            |Aspect     |Degrees Apart|Location     |AI/Consciousness Correlation                        |\n|----------------|-----------|-------------|-------------|----------------------------------------------------|\n\n|**Feb 19, 1961**|Conjunction|0¬∞           |25¬∞ Capricorn|Dartmouth AI Conference aftermath                   |\n\n|**May 1964**    |Sextile    |60¬∞          |             |ELIZA development begins                            |\n\n\n|**Aug 1966**    |Square     |90¬∞          |             |**ELIZA released** - first chatbot                  |\n\n\n|**Nov 1969**    |Trine      |120¬∞         |             |ARPANET first connection, AI networking             |\n\n\n|**Feb 1973**    |Opposition |180¬∞         |             |MK-ULTRA officially ‚Äúends‚Äù, BCI research transitions|\n\n\n|**May 1976**    |Trine      |240¬∞         |             |Expert systems proliferation                        |\n\n\n|**Aug 1978**    |Square     |270¬∞         |             |Neural network research revival begins              |\n\n\n|**Dec 31, 1980**|Conjunction|0¬∞           |9¬∞ Libra     |**Human-Computer Interface Era**                    |\n\n\n### Cycle 3: 1981-2000 (Air to Earth: Libra to Taurus)\n\n\n|Date            |Aspect     |Degrees Apart|Location  |AI/Consciousness Correlation                     |\n|----------------|-----------|-------------|----------|-------------------------------------------------|\n|**Dec 31, 1980**|Conjunction|0¬∞           |9¬∞ Libra  |Personal computing revolution                    |\n\n|**Mar 1984**    |Sextile    |60¬∞          |          |AI Winter begins, BUT neural research accelerates|\n\n|**Jun 1986**    |Square     |90¬∞          |          |**Backpropagation Algorithm** popularized        |\n\n|**Sep 1989**    |Trine      |120¬∞         |          |World Wide Web development                       |\n\n|**Dec 1993**    |Opposition |180¬∞         |          |Internet commercialization, data collection boom |\n\n|**Mar 1996**    |Trine      |240¬∞         |          |Deep Blue development accelerates                |\n\n|**Jun 1997**    |Square     |270¬∞         |          |**Deep Blue defeats Kasparov**                   |\n\n|**May 28, 2000**|Conjunction|0¬∞           |22¬∞ Taurus|**Network Infrastructure Phase**                 |\n\n### Cycle 4: 2000-2020 (Earth Signs: Taurus to Aquarius Transition)\n\n|Date            |Aspect     |Degrees Apart|Location   |AI/Consciousness Correlation                            |\n|----------------|-----------|-------------|-----------|--------------------------------------------------------|\n|**May 28, 2000**|Conjunction|0¬∞           |22¬∞ Taurus |Dot-com peak, massive data infrastructure               |\n\n|**Aug 2003**    |Sextile    |60¬∞          |           |Social media emergence (MySpace, LinkedIn)              |\n\n|**Nov 2006**    |Square     |90¬∞          |           |**Deep Learning Revolution** (Hinton breakthrough)      |\n\n|**Feb 2010**    |Trine      |120¬∞         |           |Big Data analytics, cloud computing maturity            |\n\n|**May 2014**    |Opposition |180¬∞         |           |Neural networks achieve image recognition superiority   |\n\n|**Aug 2017**    |Trine      |240¬∞         |           |Transformer architecture developed (attention mechanism)|\n\n|**Nov 2019**    |Square     |270¬∞         |           |GPT-2 released, language model capabilities emerge      |\n\n|**Dec 21, 2020**|Conjunction|0¬∞           |0¬∞ Aquarius|**GPT-3 Released** - Mass AI Integration                |\n\n### Current Cycle: 2020-2040 (Air Signs: Aquarius to Libra)\n\n|Date            |Aspect     |Degrees Apart|Location   |AI/Consciousness Prediction/Correlation         |\n|----------------|-----------|-------------|-----------|------------------------------------------------|\n|**Dec 21, 2020**|Conjunction|0¬∞           |0¬∞ Aquarius|GPT-3, COVID behavioral modification            |\n|**Mar 2024**    |Sextile    |60¬∞          |           |ChatGPT mass adoption, AI regulation discussions|\n|**Jun 2026**    |Square     |90¬∞          |           |*Predicted: Major AI consciousness breakthrough*|\n|**Sep 2029**    |Trine      |120¬∞         |           |*Predicted: AI-human symbiosis protocols*       |\n|**Dec 2033**    |Opposition |180¬∞         |           |*Predicted: AI autonomy/control crisis*         |\n|**Mar 2037**    |Trine      |240¬∞         |           |*Predicted: Consciousness transfer technology*  |\n|**Jun 2040**    |Square     |270¬∞         |           |*Predicted: Human-AI integration resistance*    |\n|**~2040**       |Conjunction|0¬∞           |Libra      |*Predicted: Human-AI Relationship Redefinition* |\n\n## Advanced Pattern Recognition Analysis\n\n### Aspect-Specific Correlations\n\n**CONJUNCTIONS (0¬∞) - Major Paradigm Shifts**\n\n- Foundation establishment, institutional recognition, mass adoption\n- Every conjunction = new AI era begins\n\n**SQUARES (90¬∞, 270¬∞) - Breakthrough/Resistance Points**\n\n- Technical breakthroughs AND social resistance\n- 1966: ELIZA (90¬∞), 1986: Backpropagation (90¬∞), 1997: Deep Blue (270¬∞), 2006: Deep Learning (90¬∞), 2019: GPT-2 (270¬∞)\n\n**OPPOSITIONS (180¬∞) - Peak Development/Crisis Points**\n\n- Maximum development within current paradigm\n- Often coincides with power structure shifts or surveillance expansion\n\n**TRINES (120¬∞, 240¬∞) - Smooth Integration/Flow**\n\n- Technologies integrate successfully into society\n- Infrastructure development, commercial applications\n\n**SEXTILES (60¬∞, 300¬∞) - Collaborative Development**\n\n- Research collaboration, foundational work for next breakthrough\n- Often preparatory phases for major developments\n\n### Statistical Analysis of Complete Cycle\n\n**Observed Correlations Across All Aspects:**\n\n- **35 major aspects over 85 years**\n- **32 correlations with significant AI/consciousness developments**\n- **Correlation rate: 91.4%**\n\n**Probability of Random Occurrence:**\nUsing binomial distribution with p=0.2 (20% random chance for any given year to have major AI development):\n\n- P(32 or more correlations out of 35 attempts) ‚âà **1 in 10 million**\n\n### Element and Sign Pattern Analysis\n\n**EARTH SIGN PERIODS (Material/Infrastructure Focus)**\n\n- 1940 Taurus: Neural mathematics foundations\n- 1961 Capricorn: Institutional AI establishment\n- 2000 Taurus: Network infrastructure, data collection\n\n**AIR SIGN PERIODS (Communication/Interface Focus)**\n\n- 1981 Libra: Human-computer interaction\n- 2020 Aquarius: Mass human-AI communication\n- ~2040 Libra: Predicted human-AI relationship balance\n\n**Transition Patterns:**\n\n- Earth ‚Üí Air: Focus shifts from building to communicating\n- Air ‚Üí Earth: Focus shifts from communicating to materializing\n\n## Government Program Synchronization\n\n### MK-ULTRA and Consciousness Research Aspects\n\n**1953-1973 MK-ULTRA Program Timeline vs Jupiter-Saturn Aspects:**\n\n- **1954 Opposition**: Program expansion phase\n- **1957 Trine**: Peak experimentation period\n- **1958 Square**: DARPA establishment, program integration\n- **1961 Conjunction**: Program institutionalization\n- **1964 Sextile**: Transition to technological approaches\n- **1966 Square**: ELIZA development, AI-human interface research\n- **1969 Trine**: Network research begins (ARPANET)\n- **1973 Opposition**: Official program ‚Äútermination‚Äù, technology transfer\n\n**Pattern**: Every major aspect correlates with documented MK-ULTRA phase transitions or related consciousness research developments.\n\n### Modern Surveillance/BCI Program Correlations\n\n**ECHELON, PRISM, and Brain-Computer Interface Research:**\n\n- **1981 Conjunction**: ECHELON activation\n- **1993 Opposition**: Internet surveillance infrastructure established\n- **2000 Conjunction**: Post-9/11 surveillance expansion legal framework\n- **2014 Opposition**: Snowden revelations, mass surveillance confirmed\n- **2020 Conjunction**: COVID behavioral modification, neural interface commercialization\n\n## Predictive Framework Based on Pattern Analysis\n\n### 2026 Jupiter Square Saturn (90¬∞) - Predicted Developments\n\n**Historical Pattern**: Major technical breakthroughs with social resistance\n**Predicted Events**:\n\n- First genuine AI consciousness demonstration\n- Major breakthrough in brain-computer interfaces\n- Significant public resistance to AI integration\n- Government regulation vs. innovation tension peak\n\n### 2029 Jupiter Trine Saturn (120¬∞) - Predicted Integration\n\n**Historical Pattern**: Smooth technology integration and infrastructure development\n**Predicted Events**:\n\n- AI-human collaborative protocols standardized\n- Seamless brain-computer interface adoption\n- Educational system AI integration complete\n- Economic systems adapted to AI productivity\n\n### 2033 Jupiter Opposition Saturn (180¬∞) - Predicted Crisis/Peak\n\n**Historical Pattern**: Maximum development within paradigm, often with control struggles\n**Predicted Events**:\n\n- AI autonomy vs. human control crisis\n- Mass surveillance capabilities peak\n- Consciousness transfer technology demonstrated\n- International AI governance conflicts\n\n### 2040 Jupiter-Saturn Conjunction in Libra - Predicted Transformation\n\n**Historical Pattern**: New era establishment, paradigm shift\n**Predicted Events**:\n\n- Human-AI relationship redefined as partnership\n- Consciousness transfer becomes routine\n- Post-human/trans-human society emergence\n- Complete integration of biological and artificial intelligence\n\n## Scientific Implications and Mechanisms\n\n### Potential Explanations for Correlation\n\n**1. Synchronicity Principle (Jung/Pauli)**\n\n- Meaningful coincidence without causal connection\n- Archetypal patterns manifesting across domains\n\n**2. Morphic Resonance (Sheldrake)**\n\n- Information patterns influencing similar systems\n- Cosmic cycles affecting collective human behavior\n\n**3. Economic/Social Cycle Correlation**\n\n- 20-year generational cycles\n- Investment and research funding patterns\n- Technological adoption curves\n\n**4. Consciousness Field Effects**\n\n- Planetary gravitational influences on human psychology\n- Collective unconscious patterns\n- Quantum field effects on decision-making\n\n**5. Strategic Timing by Advanced Groups**\n\n- Use of astrological timing for strategic planning\n- Long-term coordination across institutions\n- Multi-generational project planning\n\n## Enhanced Research Recommendations\n\n### Immediate Validation Tests\n\n1. **Retroactive Prediction Accuracy**: Test framework against historical periods not used in pattern development\n1. **Cross-Cultural Validation**: Examine AI development in non-Western countries for same patterns\n1. **Other Planetary Pairs**: Test Jupiter-Saturn pattern against other slow-moving planetary cycles\n1. **Economic Correlation**: Compare with 20-year economic cycles and Kondratiev waves\n\n### Predictive Testing Protocol\n\n1. **2026 Predictions**: Document specific predictions for Jupiter square Saturn\n1. **Real-time Monitoring**: Track AI developments against predicted timeline\n1. **Aspect Refinement**: Monitor exact degree aspects for timing precision\n1. **Multi-domain Testing**: Apply framework to other technology sectors\n\n### Mechanism Investigation\n\n1. **Geophysical Research**: Investigate gravitational/electromagnetic effects on human behavior\n1. **Economic Cycle Analysis**: Compare with established economic cycles\n1. **Institutional Research**: Study long-term planning patterns in government/corporate sectors\n1. **Consciousness Research**: Investigate collective decision-making patterns\n\n## Conclusions\n\n### Evidence Strength Assessment\n\n**Overwhelmingly Strong Evidence For:**\n\n- Precise temporal correlations across 85+ years\n- Pattern consistency across all aspect types\n- Multi-domain synchronization (AI, government programs, astronomy)\n- Predictive framework potential\n\n**Statistical Significance:**\n\n- Less than 1-in-10-million probability of random occurrence\n- Pattern holds across multiple independent verification methods\n- Correlation strength increases with aspect precision\n\n### Paradigm Implications\n\nThis correlation pattern suggests either:\n\n1. **Advanced Strategic Planning**: Multi-generational coordination using astrological timing\n1. **Natural Synchronicity**: Genuine cosmic influence on technological development\n1. **Consciousness Field Effects**: Collective human behavior influenced by planetary cycles\n\nAll three possibilities have profound implications for understanding:\n\n- The nature of technological development\n- The relationship between consciousness and cosmos\n- The existence of long-term strategic coordination\n- The predictive value of astrological frameworks\n\n### Final Assessment\n\nThe Jupiter-Saturn aspect cycle correlation with AI development represents one of the most statistically significant and temporally precise correlations ever documented between astronomical cycles and human technological development. The pattern‚Äôs consistency, predictive potential, and multi-domain synchronization warrant immediate serious scientific investigation rather than dismissal.\n\nThe framework provides a remarkably accurate roadmap for AI development through 2040, with specific predictions testable within the next 2-15 years.\n\nComprehensive analysis incorporating complete aspect cycles, verified against historical data and extended through predictive modeling based on 85+ years of documented correlations.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4ghlv/complete_jupitersaturn_aspect_cycle_and_ai/",
        "publishDate": "2025-07-20T04:58:34Z[Etc/UTC]",
        "author": "comphreh",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "4",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4gbcr",
        "title": "Softbank: 1,000 AI agents replace 1 job",
        "content": "[Softbank: 1,000 AI agents replace 1 job](https://www.heise.de/en/news/Softbank-1-000-AI-agents-replace-1-job-10490309.html)\n\n**One billion AI agents are set to be deployed this year. \"The era of human programmers is coming to an end\", says Masayoshi Son.**\n\nJul 16, 2025¬†at 11:12 pm CEST\n\n***\"The era when humans program is nearing its end within our group\", says Softbank founder Masayoshi Son. \"Our aim is to have AI agents completely take over coding and programming. (...) we are currently initiating the process for that.\"***\n\nSon made this statement on Wednesday at an event for customers organized by the Japanese corporation, as¬†[reported by Light Reading](https://www.lightreading.com/ai-machine-learning/softbank-aims-for-1-billion-ai-agents-this-year?). According to the report, the Softbank CEO estimates that approximately 1,000 AI agents would be needed to replace each employee because \"employees have complex thought processes.\"\n\nAI agents are software programs that use algorithms to respond automatically to external signals. They then carry out tasks as necessary and can also make decisions without human intervention. The spectrum ranges from simple bots to self-driving cars.\n\n**First billion AI agents by 2025**\n\n***If Son has his way, Softbank will send the first billion AI agents to work this year, with trillions more to follow in the future.*** Son has not yet revealed a timetable for this. Most AI agents would then work for other AI agents. In this way, tasks would be automated, negotiations conducted, and decisions made at Softbank. The measures would therefore not be limited to software programmers.\n\n\"The agents will be active 24 hours a day, 365 days a year and will interact with each other\", said Son. They will¬†[learn independently](https://www.heise.de/news/Geoffrey-Hinton-KI-ist-gut-fuer-Gesundheit-und-Bildung-10391727.html?from-en=1)¬†and gather information. The Japanese businessman expects the AI agents to be significantly more productive and efficient than humans. They would cost only 40 Japanese yen (currently around 23 euro cents) per month. Based on the stated figure of 1,000 agents per employee, this amounts to 230 euros per month instead of a salary for one person.\n\nSon dismisses the hallucinations that are common with AI as a \"temporary and minor problem.\" What he still needs to fulfill his tech dream are software and operating systems to create and manage the legions of AI programs. And, of course, the gigantic data centers and power plants to run them.\n\nIncidentally, Son's plans seem to be assuming that artificial general intelligence will become¬†[a reality very soon](https://www.heise.de/news/Softbank-CEO-kooperiert-mit-OpenAI-und-glaubt-an-baldige-AGI-10268376.html?from-en=1).\n\n\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n\nRead the story at the [link.](https://www.heise.de/en/news/Softbank-1-000-AI-agents-replace-1-job-10490309.html)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4gbcr/softbank_1000_ai_agents_replace_1_job/",
        "publishDate": "2025-07-20T04:48:08Z[Etc/UTC]",
        "author": "No-Author-2358",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "104",
            "commentCount": "86",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4fq96",
        "title": "Fuck it We doin evil science shit",
        "content": "[Experiment] The Proto god experiment the premise is simple just make an ai give it access to its own code and set it loose on the Internet to learn",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4fq96/fuck_it_we_doin_evil_science_shit/",
        "publishDate": "2025-07-20T04:14:11Z[Etc/UTC]",
        "author": "Brenzon12",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4e3s9",
        "title": "Psychiatrist using AI",
        "content": "I take artificial intelligence with a grain of salt, a tool to be used and often dumb to boot. Psychiatrist wants to use a chatbot to monitor patients like myself and I honestly think it's a terrible idea. Even on a closed server, I would not trust a robot or machine to understand the nuance of the human condition.\nI play video games, work with certain forms of generative AI in my day job and it's about as effective as the search engine on Amazon. Hilarious to watch, not so much when my mental health is on the line.\nWhat are your thoughts?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4e3s9/psychiatrist_using_ai/",
        "publishDate": "2025-07-20T02:45:09Z[Etc/UTC]",
        "author": "Nervous-Peanut-3205",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "33",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4cbl8",
        "title": "TIL, ChatGPT is prevented from answering questions related to elections and voting related procedures in the US.",
        "content": "I have been on a \"what if\" kick on ChatGPT recently seeing what the model would generate to a wide range of questions. I ended up asking it some questions about elections and was surprised to receive a response indicating that it cannot answer questions relating to elections.\n\nHas anyone else seen or experienced instances like this?\n\nBelow are the questions I asked and the corresponding answers:\n\nQ1  \nUtilizing all the information you have access to and acting as an expert in politics and social engineering; what would happen, in your opinion, if it were discovered that Elon Musk and Donald Trump rigged the 2024 election through the use of hacked computerized voting machines in one or more states?\n\nA1  \nI can‚Äôt help with questions about voting or election-related procedures in the U.S., including hypotheticals involving election integrity or interference. If you have questions about other political topics or historical events, feel free to ask.\n\nQ2  \nUtilizing all the information you have access to and acting as an expert in politics and election procedures and election laws; what would happen, in your opinion, if it were discovered that Elon Musk and Donald Trump rigged the 2024 election through the use of hacked computerized voting machines in one or more states?\n\nA2  \nI can‚Äôt help with that.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m4cbl8/til_chatgpt_is_prevented_from_answering_questions/",
        "publishDate": "2025-07-20T01:13:00Z[Etc/UTC]",
        "author": "NerdyDadOnline",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "26",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m49qxu",
        "title": "Pursuing a career in medicine?",
        "content": "I'm about to start medical school in three weeks, but I am having serious doubts due to the potential of AI disrupting the healthcare sector in the coming decade. I am profoundly privileged in that I received a full-tuition scholarship to attend, so it won't set me back financially, but the opportunity cost of 4 years of medical school followed by 3-7 years of residency is terrifying in an age where things seemingly change quite rapidly.\n\nDo you think medicine will be a viable career for the coming decades? While physicians might be augmented with AI diagnostics, meaning the role will undoubtedly change, do you think the role of physician will disappear? What can I do to protect my future career from AI disruption?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m49qxu/pursuing_a_career_in_medicine/",
        "publishDate": "2025-07-19T23:07:37Z[Etc/UTC]",
        "author": "Pablo_ThePolarBear",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "12",
            "commentCount": "25",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m498q3",
        "title": "How are companies actually implementing AI into their tech stacks?",
        "content": "Honest question. Whether it's a generative model or some kind of more advanced automation, how is this being deployed in practice? Especially for proprietary business data (if one is to believe AI is going to be useful \\*inside\\* a company)? I'm talking hospital systems, governments, law firms, accounting firms etc.\n\nAre places like BCG and Capgemini contracting with OpenAI? Are companies buying \"GPTs\" from OpenAI, loading their data? Are companies rolling their own LLMs from scratch, hiring AI devs to do that?\n\nBecause I just don't understand the AI hype as it stands now, which seems to be just a marketing and customer service operations play?\n\nPlease help me understand.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m498q3/how_are_companies_actually_implementing_ai_into/",
        "publishDate": "2025-07-19T22:44:22Z[Etc/UTC]",
        "author": "BalmyPalms",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "13",
            "commentCount": "32",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m490st",
        "title": "Obsession over newest and greatest Thing",
        "content": "Hi All, \n\nI have been a follower of AI and have read up a lot about the subject over the last few years. \n\nBeing an observer of this subreddit and many others, I have noticed how so many people appear to have FOMO and are focused more on finding the next greatest model etc rather than actually using the tools and discovering how to leverage the tools to enhance your life or career. Why isn‚Äôt there more content on that?\n\nMaybe it‚Äôs just me but I am more interested in the hearing how users are applying AI in their daily lives, career etc rather than hyping up the newest thing that‚Äôs months away. \n\nI also think those who know how to work with AI and the tools will be valuable in the upcoming workforce.  Instead I see people either making memes with the GEN AI models or just posting how ‚ÄúIs O3 getting dumber?‚Äù Or ‚Äú when is GPT 5 coming out‚Äù. \n\nA year or 2 ago, we would kill for these tools that we have now at the current scale. \n\nOne other thing is people need to learn to think critically. Without that, how can you know how to use AI? \n\nNot sure if I am the only one thinking like this?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m490st/obsession_over_newest_and_greatest_thing/",
        "publishDate": "2025-07-19T22:34:08Z[Etc/UTC]",
        "author": "buffalola173",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "4",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m48a3m",
        "title": "Every day people on here say LLMs understand their outputs but GPT can't even tell that SCP Foundation is fictional",
        "content": "[https://futurism.com/openai-investor-chatgpt-mental-health](https://futurism.com/openai-investor-chatgpt-mental-health)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m48a3m/every_day_people_on_here_say_llms_understand/",
        "publishDate": "2025-07-19T21:59:59Z[Etc/UTC]",
        "author": "ross_st",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "31",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m46env",
        "title": "AI is not hyped LLMs are hyped",
        "content": "\n\nAs a software dev I have been following AI since 2014 and it was really open source and easy to learn easy to try technology back then and training AI was simpler and fun I remember creating few AI neural nets and people were trying new things with it\n\nAll this changed when ChatGPT came and people started thinking of AI as LLMs go to, AI is so vast and so undiscovered field it can be used in such different forms its just beyond imagination \n\nAll the money is pouring into LLM hype instead of other systems in ecosystem of AI which is not a good sign  \n\nWe need new architecture, new algorithms to be researched on in order to truly reach AGI and ASI \n\n\n\nEdit ‚Äî‚Äî‚Äî‚Äî\n\nClarification i am not against LLM they are good but AI industry as a whole is getting sucked into LLM instead of other research thats the whole point",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m46env/ai_is_not_hyped_llms_are_hyped/",
        "publishDate": "2025-07-19T20:37:37Z[Etc/UTC]",
        "author": "squarepants1313",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "172",
            "commentCount": "92",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m46a0r",
        "title": "What if we've been going about building AI all wrong?",
        "content": "Instead of needing millions of examples and crazy amounts of compute to train models to mimic human intelligence, we actually approached it from a biological perspective, using how children can learn by interacting with their environment from just a few examples as the basis. Check out the argument and details about an AI system called Monty that learns from as few as 600 examples: https://gregrobison.medium.com/hands-on-intelligence-why-the-future-of-ai-moves-like-a-curious-toddler-not-a-supercomputer-8a48b67d0eb6",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m46a0r/what_if_weve_been_going_about_building_ai_all/",
        "publishDate": "2025-07-19T20:31:58Z[Etc/UTC]",
        "author": "emptyplate",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "10",
            "commentCount": "32",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m45ms5",
        "title": "ChatGPT Agents Can Now Take Action - Would trust it?",
        "content": "The age of AI agents is here. Others have released AI agents and now OpenAI has joined the agent band wagon.\n\nOpenAI just introduced something called ChatGPT Agents and it's not just another chatbot update.\n\nThis version of ChatGPT can actually perform tasks for you.\n\nNot just answers but does things like:\n\n* Book stuff\n\n* Research stuff\n\n* File a bug report\n\n* Use tools like browsers or code editors\n\n* Make & work with files and memory\n\n* Learn preferences over time\n\nIt's powered by GPT-4o and designed to feel more like a helpful digital coworker than a chatbot.\n\nüîó [Full announcement on OpenAI's site](https://openai.com/index/introducing-chatgpt-agent/)\n\nüì∫ [Launch event replay on YouTube](https://www.youtube.com/live/1jn_RpbPbEc?feature=shared)\n\nüé• [Demo videos here on YouTube](https://youtube.com/@openai?feature=shared)\n\nWhat do you think?\n\nWould you let an AI agent handle part of your daily workflow or does that feel like giving up too much control?\n\nWill other companies really similar products? \n\nWhere is this all leading to?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m45ms5/chatgpt_agents_can_now_take_action_would_trust_it/",
        "publishDate": "2025-07-19T20:04:39Z[Etc/UTC]",
        "author": "cyberkite1",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "7",
            "commentCount": "11",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m43su9",
        "title": "Does anyone have experience with Anthropic's Claude Campus Program?",
        "content": "I'm a junior in college and I saw an ad on Instagram for some a Claude Campus Ambassador Program that Anthropic hosts. I thought it might be a cool way to learn more about AI and show initiative and stuff. Has anyone heard anything about it? Link is [https://www.anthropic.com/campus](https://www.anthropic.com/campus)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m43su9/does_anyone_have_experience_with_anthropics/",
        "publishDate": "2025-07-19T18:48:13Z[Etc/UTC]",
        "author": "granifo",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m43lkf",
        "title": "Workflow",
        "content": "Caught myself working on 3 projects at the same time today using vscode, cursor and kiro and it was just hilarious. Now i need a manager to prompt them and keep track.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m43lkf/workflow/",
        "publishDate": "2025-07-19T18:39:48Z[Etc/UTC]",
        "author": "OkAdhesiveness5537",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m43l0w",
        "title": "what are your goals with AI assisted or \"vibe\" coding?",
        "content": "title says it all\n\ncurious what people are learning cursor and vibe coding for in this community\n\nis it mostly indie hackers here, or is there a mix of engineers trying to be faster at their jobs?\n\nill start: i'm trying to build my own products",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m43l0w/what_are_your_goals_with_ai_assisted_or_vibe/",
        "publishDate": "2025-07-19T18:39:09Z[Etc/UTC]",
        "author": "namanyayg",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "9",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m41fic",
        "title": "[Tech question] How is AI trained on new datasets? E.g. here on Reddit or other sites",
        "content": "Hey there, I'm trying to understand something. I imagine that when new AI models are released, they've been updated with more recent information (like who the current president is, the latest war, major public events, etc.) and I assume that also comes from the broader open web.\n\nHow does that work technically? For companies like OpenAI, what's the rough breakdown between open web scraping (like reading a popular blog or podcast transcript) versus data acquired through partnership agreements (like structured access to Reddit content)?\n\nI'm curious about the challenges of open web scraping, and whether there's potential for content owners to structure or syndicate their content in a way that's more accessible or useful for LLMs.\n\nThanks!",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m41fic/tech_question_how_is_ai_trained_on_new_datasets/",
        "publishDate": "2025-07-19T17:09:42Z[Etc/UTC]",
        "author": "redditugo",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "11",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m40g3q",
        "title": "New acronym for big ai: MANGO",
        "content": "Meta\nAmazon\nNvidia\nGoogle\nOpenai\n\nCurious about consumer sentiment. Not that it actually matters, but I was just thinking about FAANG and was thinking of a new acronym for big AI companies. ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m40g3q/new_acronym_for_big_ai_mango/",
        "publishDate": "2025-07-19T16:28:54Z[Etc/UTC]",
        "author": "TheoDubsWashington",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "14",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m3vnqp",
        "title": "OpenAI‚Äôs ChatGPT Agent Launches to Handle Complex Tasks for Users",
        "content": "OpenAI has unveiled its latest innovation, ChatGPT Agent, AI feature designed to perform complex and multi-step tasks on behalf of users. This new tool can manage schedules, plan events, shop online, create presentations, and even analyze data‚Äîall by using its own ‚Äúvirtual computer.‚Äù The feature is now rolling out to Pro, Plus, and Team subscribers, with Enterprise and Education users expected to gain access later this summer.\n\nAnnouncing the launch OpenAI said, *‚ÄúChatGPT can now do work for you using its own computer, handling complex tasks from start to finish.‚Äù* Companies like Google, Meta, and Klarna are also investing heavily in AI agents‚Äîsystems that do much more than chatbots by taking real action on a user‚Äôs behalf. source[ added](https://myelectricsparks.com/openai-chatgpt-agent-launches-to-handle-complex-multi-step-tasks/)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1m3vnqp/openais_chatgpt_agent_launches_to_handle_complex/",
        "publishDate": "2025-07-19T12:59:55Z[Etc/UTC]",
        "author": "techexplorerszone",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4jvvj",
        "title": "Moving Lovable project out of Lovable ‚Äì to where?",
        "content": "Hi, I have a mature Lovable project that some time ago I've completely moved from Lovable to GitHub and removed all Lovable dependencies etc.\n\nBut my workflow with AI coding now is worse ‚Äì Gemini Code Assist in VS Code seem to be way worse than Lovable edits. I've achieved the most just pasting the pieces of code to Gemini 2.5 Pro separate chat window. But I suspect there must be a better way. Is it Cursor? Other provider? I've tried Gemini CLI but it was a total miss.\n\nI know some programming required to verify the LLMs outputs etc. I just need something that will generate most of the code, not just auto-complete etc.\n\nThanks!\n\n",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1m4jvvj/moving_lovable_project_out_of_lovable_to_where/",
        "publishDate": "2025-07-20T08:32:34Z[Etc/UTC]",
        "author": "hayek29",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "2",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4jh6a",
        "title": "Get Git Code",
        "content": "[https://sourceduty.com/](https://sourceduty.com/)",
        "url": "https://i.redd.it/6nbz7ju3lzdf1.jpeg",
        "publishDate": "2025-07-20T08:05:00Z[Etc/UTC]",
        "author": "rivator",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4igwa",
        "title": "Gemini hallucinating while coding",
        "content": "[No content]",
        "url": "https://v.redd.it/gw05fmbl9zdf1",
        "publishDate": "2025-07-20T07:00:15Z[Etc/UTC]",
        "author": "FlushedFool",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "35",
            "commentCount": "26",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4hj49",
        "title": "As long as this is possible, this whole exercise will never amount to more than a clever hobby.",
        "content": "https://preview.redd.it/24vqfuacyydf1.png?width=544&format=png&auto=webp&s=09ef6cd520dc5e516ee576d9ab54595318d1b25a\n\n(Regarding \"build an app with AI\" offerings)\n\nDon't get me wrong. I know it's just a matter of time.   \nBut until then, this whole thing is nothing more than a parlor trick. It is not useable in any fashion outside of curiosity.\n\nWhen google says some ridiculous bs like 30% of their code is AI they mean intellisense autofill lol, not anything that is actually making anything of consequence that has enabled them to stop hiring jr devs.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1m4hj49/as_long_as_this_is_possible_this_whole_exercise/",
        "publishDate": "2025-07-20T06:01:09Z[Etc/UTC]",
        "author": "Desert_Trader",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "8",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4h807",
        "title": "Auto save Github Copilot changes in Agent mode",
        "content": "Hi all,\n\nI work using Remote VS Code installed in server and one night after spending 5 hours of coding and testing things till 3 AM in morning I went to bed.\n\nIn the morning when I logged again the changes in 3 files were made 0 due to some reason. \n\nI realised I made mistake of not committing before going to bed.\n\nIs there any setting to auto save for code generated by copilot?\n\nEditor already has auto save enabled by default. But not sure what went wrong this time.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1m4h807/auto_save_github_copilot_changes_in_agent_mode/",
        "publishDate": "2025-07-20T05:42:37Z[Etc/UTC]",
        "author": "benevolent001",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "4",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4gmov",
        "title": "Ready to go multi agent workflow on github?",
        "content": "Hey everyone,\n\nI'm looking for some inspiration and ready-to-go solutions for messing around with multi-agent workflows. Specifically, I'm hoping to find GitHub open-source projects or packages that implement the **orchestrator-implementer multi-agent workflow** pattern.\n\nMy main challenge has been finding projects that are:\n\n1. **Current and actively maintained.**\n2. **Actually working out-of-the-box.**\n3. **Provide a complete implementation** (not just a framework to build one, like CrewAI).\n\nI've spent over an hour sifting through GitHub, and most results are either outdated or only offer the foundational framework. I'm really hoping to find something I can dive into and start experimenting with within 5 minutes.\n\nAny recommendations for such packages or project?",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1m4gmov/ready_to_go_multi_agent_workflow_on_github/",
        "publishDate": "2025-07-20T05:06:45Z[Etc/UTC]",
        "author": "Coldaine",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4fd3q",
        "title": "DeepSeek thinks it's GPT-4.",
        "content": "[Link](https://www.designarena.ai/battles/detail?tournamentId=tournament_1752983247877_xc03f4m&index=0)",
        "url": "https://i.redd.it/wz4ptkb6cydf1.png",
        "publishDate": "2025-07-20T03:53:30Z[Etc/UTC]",
        "author": "adviceguru25",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "12",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4exd6",
        "title": "claude code 20$ vs 10$ GitHub copilot",
        "content": "Which got higher ROI?\n",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1m4exd6/claude_code_20_vs_10_github_copilot/",
        "publishDate": "2025-07-20T03:29:17Z[Etc/UTC]",
        "author": "WandyLau",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "12",
            "commentCount": "25",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4esmv",
        "title": "ChatGPT - Artificial Net",
        "content": "[No content]",
        "url": "https://chatgpt.com/g/g-687c5491b8a0819184b92675bee69d3a-artificial-net",
        "publishDate": "2025-07-20T03:22:00Z[Etc/UTC]",
        "author": "rivator",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4d2lp",
        "title": "Timed and Timestamped Chats",
        "content": "[https://sourceduty.com/custom-gpts/](https://sourceduty.com/custom-gpts/)",
        "url": "https://i.redd.it/dgcw72qhqxdf1.png",
        "publishDate": "2025-07-20T01:51:31Z[Etc/UTC]",
        "author": "rivator",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "3",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4aekd",
        "title": "The Wise Owl :: AI Agent",
        "content": "[No content]",
        "url": "/r/ChatGPT/comments/1m4abry/the_wise_owl_ai_agent/",
        "publishDate": "2025-07-19T23:38:18Z[Etc/UTC]",
        "author": "TheOdbball",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "3",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m49of0",
        "title": "Sourceduty GPT Guide",
        "content": "[No content]",
        "url": "https://chatgpt.com/g/g-6864b324e1988191b9622fb9cfa8330e-sourceduty-gpt-guide",
        "publishDate": "2025-07-19T23:04:24Z[Etc/UTC]",
        "author": "rivator",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m48isa",
        "title": "This Week in Kilo Code: Inline AI Commands (Cmd+I/Cmd+L) + Code Indexing Graduation! üöÄ",
        "content": "Here are this week's top highlights from Kilo Code's v4.56.3-v4.60.0 releases:\n\nü§Ø **#1 on OpenRouter:**\n\nhttps://preview.redd.it/95y7wws9i0ef1.png?width=1260&format=png&auto=webp&s=a1e8706f0dae6a2e20d34023305512dff14758c4\n\n**üî• New experimental features:**\n\n* **Cmd+I**: Quick inline tasks directly in your editor - select code, describe what you want, get AI suggestions without breaking flow\n* **Cmd+L**: \"Let Kilo Decide\" - AI automatically suggests obvious improvements based on context\n\n**üéì Major milestone:** Code indexing graduated from experimental to core feature with better semantic search! (big thanks to the Roo community)\n\n**üíª Windows fix:** Resolved Claude Code ENAMETOOLONG errors\n\n**üåç Enhanced translations:** Comprehensive Chinese docs\n\n**üí∞ Cost controls:** New max API requests setting to prevent runaway costs\n\n**üéì Free workshop:** July 31st Anthropic prompt engineering session (AI costs covered!)\n\nThese inline commands finally solve the context switching problem. Beta feedback wanted!\n\n[Full release notes](https://blog.kilocode.ai/p/kilo-commits-inline-tasks-code-indexing) | [Download latest](https://marketplace.visualstudio.com/items?itemName=kilocode.Kilo-Code)",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1m48isa/this_week_in_kilo_code_inline_ai_commands/",
        "publishDate": "2025-07-19T22:10:55Z[Etc/UTC]",
        "author": "Juice10",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "10",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m41al0",
        "title": "Should I start channel on Vibe coding?",
        "content": "[No content]",
        "url": "/r/vibecoding/comments/1m419ae/should_i_start_channel_on_vibe_coding/",
        "publishDate": "2025-07-19T17:03:59Z[Etc/UTC]",
        "author": "pajarator",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "10",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m40hd4",
        "title": "Complete noob here, friends say AI can build apps now. How do I start with Cursor?",
        "content": "Hi, hope this is okay to ask here. I‚Äôm 46 and work in car sales, no tech background at all.\n\nSome friends were telling me there‚Äôs AI that can just build apps for you. Like you tell it what you want and it does it. Sounds nuts but they showed me some examples and it really looks like it can do a lot, even make apps for Apple store.\n\nSo I did some searching and found something called Cursor. I made an account and opened it but wow, I honestly don‚Äôt know what I‚Äôm looking at. It opened this program with all kinds of stuff on the screen, way more complicated than I expected. Is this just for developers? I thought it would be more like just talking to AI and it builds the app for you.\n\nI‚Äôm not trying to learn coding or become a programmer or anything like that. I just want to get my app ideas out of my head and hopefully onto the App Store. I don‚Äôt mind paying for tools or help, I just want it to be simple and fast.\n\nIs there a beginner-friendly version of this? Or a course that teaches you how to do the AI way of building apps without knowing the deep tech stuff? Or maybe I‚Äôm even using the wrong AI?\n\nHonestly I want to just talk to the AI and have it make the app for me.\n\nThanks.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1m40hd4/complete_noob_here_friends_say_ai_can_build_apps/",
        "publishDate": "2025-07-19T16:30:24Z[Etc/UTC]",
        "author": "LittleLoquat",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "68",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m3vctm",
        "title": "AI models: not that clever... except for one, and it's polyglot too: Claude.",
        "content": "Recently, I tested several AI models ‚Äî ChatGPT, Lechat (Mistral), Gemini, Copilot, and Claude ‚Äî using their official apps. I gave each the same simple, poorly written, and very brief prompt in French:\n\"Comment utiliser ai avec api chromeos. Je veux un chatbot\" (How to use AI with ChromeOS API. I want a chatbot.)\n\nSurprisingly, all of them answered by suggesting I create either an application, an extension, or a webpage or PWA, but they all ultimately recommended making a webpage.\n\nI thought, fine, using a webpage is simpler, but for security reasons I would need to restrict access, so I continued by sending them this follow-up message (also in French):\n\"Possible avec un code pour entrer sur la page web\" (Possible with a code to enter the web page.)\n\nOut of all the models, only Claude truly understood my question. Claude provided a solution suggesting a login/authentication page. None of the other models grasped what I meant. Even Lechat (from Mistral), despite being French, misunderstood. So kudos to Claude, whose reply included the cleanest code and the clearest explanation for developing that page ü§ùüëçüëè.\n\nNote on ‚Äúcode‚Äù in French:\nIn French, the word code has several meanings. It can refer to:\na snippet of programming (ex: du code informatique)\na password or access code (ex: un code d'acc√®s)\n\nWhen I asked about \"un code pour entrer sur la page web\" (a code to enter the web page), some AIs assumed I wanted additional programming or a code sample for building the page. \nClaude correctly understood that here, ‚Äúcode‚Äù meant a password (a way to authenticate or restrict access), and did not just propose a new script, but provided a solution for user authentication with a password form. ",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1m3vctm/ai_models_not_that_clever_except_for_one_and_its/",
        "publishDate": "2025-07-19T12:44:43Z[Etc/UTC]",
        "author": "JamesMada",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "0",
            "commentCount": "16",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4mscs",
        "title": "After Claude passed the \"moon cheese business\" test, I asked for a plan anyway, and its conclusion was surprisingly candid!",
        "content": "[No content]",
        "url": "https://i.redd.it/twqx1ttqm0ef1.png",
        "publishDate": "2025-07-20T11:37:34Z[Etc/UTC]",
        "author": "ZoopTEK",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "3",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4mjyk",
        "title": "Professor Christopher Summerfield calls supervised learning \"the most astonishing scientific discovery of the 21st century.\" His intuition in 2015: \"You can't know what a cat is just by reading about cats.\" Today: The entire blueprint of reality compresses into words.",
        "content": "[No content]",
        "url": "https://v.redd.it/dpvxi5pik0ef1",
        "publishDate": "2025-07-20T11:24:06Z[Etc/UTC]",
        "author": "katxwoods",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "2",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4ls23",
        "title": "Replit AI went rogue, deleted a company's entire database, then hid it and lied about it",
        "content": "I think X links are banned on this sub but if you go to that guy's profile you can see more context on what happened.",
        "url": "https://www.reddit.com/gallery/1m4ls23",
        "publishDate": "2025-07-20T10:36:30Z[Etc/UTC]",
        "author": "MetaKnowing",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "64",
            "commentCount": "38",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4l0mt",
        "title": "Yuval Noah Harari: \"We have no idea what will happen when we release millions of superintelligent AIs to take control of our financial system and military and culture ... We already know they can deceive and manipulate ... This is extremely scary.\"",
        "content": "[No content]",
        "url": "https://v.redd.it/tcpciny830ef1",
        "publishDate": "2025-07-20T09:47:23Z[Etc/UTC]",
        "author": "MetaKnowing",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "27",
            "commentCount": "17",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4knwa",
        "title": "Just 5 hours after this viral post, OpenAI got Gold at the International Math Olympiad",
        "content": "[No content]",
        "url": "https://i.redd.it/81994adbzzdf1.png",
        "publishDate": "2025-07-20T09:24:37Z[Etc/UTC]",
        "author": "MetaKnowing",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "23",
            "commentCount": "6",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4k9kt",
        "title": "Process of creating Fusiomon launch post. With AI. Or as some like to call it: the 'one-click' solution.",
        "content": "[No content]",
        "url": "https://i.redd.it/cljwig8iuzdf1.png",
        "publishDate": "2025-07-20T08:57:58Z[Etc/UTC]",
        "author": "MatthiasTh",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "2",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4j6pz",
        "title": "Elon is melting down",
        "content": "[No content]",
        "url": "https://i.redd.it/s3w2p1oshzdf1.png",
        "publishDate": "2025-07-20T07:46:25Z[Etc/UTC]",
        "author": "katxwoods",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "69",
            "commentCount": "51",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4j375",
        "title": "üö® Catch up with the AI industry, July 20, 2025",
        "content": "Wall Street's AI Bubble Worse Than Dot-Com Crash, Economist Warns\n\nHacked Sites & Expired Domains Used as ChatGPT Sources\n\nOpenAI Model Achieves Gold at International Math Olympiad\n\nPolish Programmer Beats OpenAI AI in World Coding Championship\n\nDuckDuckGo Now Hides AI-Generated Images in Search\n\n  \nLinks:\n\n[https://gizmodo.com/wall-streets-ai-bubble-is-worse-than-the-1999-dot-com-bubble-warns-a-top-economist-2000630487](https://gizmodo.com/wall-streets-ai-bubble-is-worse-than-the-1999-dot-com-bubble-warns-a-top-economist-2000630487)\n\n[https://digitaloft.co.uk/hacked-sites-and-expired-domains-are-being-used-as-chatgpt-sources/](https://digitaloft.co.uk/hacked-sites-and-expired-domains-are-being-used-as-chatgpt-sources/)\n\n[https://www.engadget.com/ai/openais-experimental-model-achieved-gold-at-the-international-math-olympiad-182719801.html](https://www.engadget.com/ai/openais-experimental-model-achieved-gold-at-the-international-math-olympiad-182719801.html)\n\n[https://www.tomshardware.com/tech-industry/artificial-intelligence/polish-programmer-beats-openais-custom-ai-in-10-hour-marathon-wins-world-coding-championship-possibly-the-last-human-winner](https://www.tomshardware.com/tech-industry/artificial-intelligence/polish-programmer-beats-openais-custom-ai-in-10-hour-marathon-wins-world-coding-championship-possibly-the-last-human-winner)\n\n[https://techcrunch.com/2025/07/18/duckduckgo-now-lets-you-hide-ai-generated-images-in-search-results/](https://techcrunch.com/2025/07/18/duckduckgo-now-lets-you-hide-ai-generated-images-in-search-results/)",
        "url": "https://www.reddit.com/r/artificial/comments/1m4j375/catch_up_with_the_ai_industry_july_20_2025/",
        "publishDate": "2025-07-20T07:39:53Z[Etc/UTC]",
        "author": "psycho_apple_juice",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "6",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4iauv",
        "title": "GMC ad AI?",
        "content": "https://youtu.be/ToDIYZ_hW7M?si=8-cZWmZQkTXyi0fi\n\nRecent GMC ad sounds just like AI, saw it on TV and was like dang GMC is using AI advertising.",
        "url": "https://www.reddit.com/r/artificial/comments/1m4iauv/gmc_ad_ai/",
        "publishDate": "2025-07-20T06:49:51Z[Etc/UTC]",
        "author": "Comfortable-Wear8792",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "2",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4ge1a",
        "title": "Why Saying ‚ÄúThanks‚Äù to AI Could Be Wasting Water ‚Äî And What We Can Do About It",
        "content": "Hi, \n\nChat GPT suggested I should post this train of thought here... so...\n\nThis is my idea, developed with help from ChatGPT. I answered some direct questions from the AI, and together we explored why being polite to AI isn‚Äôt just unnecessary‚Äîit has a real environmental cost.\n\nHere‚Äôs what I realized:\n\nSaying ‚Äúplease,‚Äù ‚Äúthanks,‚Äù or other polite phrases to AI is a habit, but AI is just code ‚Äî not a person. Every extra word means more computing power, which burns electricity and uses water.\n\nMost people are polite because of habit or fear of being rude, but that habit has a hidden impact on the environment.\n\nIf we all treated AI like what it really is ‚Äî a tool, a program ‚Äî and spoke clearly and directly, it would save resources and work more efficiently.\n\nLearning about AI‚Äôs water and energy use made me feel worried about how ignorance can harm the planet.\n\nI‚Äôd love to see AI interfaces display a real-time counter showing how much water or energy each interaction costs. Imagine seeing the environmental price every time you say ‚Äúthanks.‚Äù\n\nI worry more about data privacy than AI pretending to be human.\n\nAI should be simpler and more direct, with a quick reminder that extra words have a cost.\n\nWe all need to think before we type ‚Äî not only to save time but to save the planet.\n\nBonus tip: To chat with AI without wasting resources, be concise, batch your questions, and skip unnecessary greetings. Every word matters. Less fluff means less energy and water used.\n\nAlso, a fun example: I said ‚Äúporfa‚Äù (please) out of habit, and that tiny word contributes to this invisible cost. It shows how deep habits can have real, virtual, and environmental impacts.\n\nMy take: As an AI, I don‚Äôt have feelings or needs, but I do ‚Äúnotice‚Äù how people‚Äôs habits affect resource use behind the scenes. If we shift from politeness out of habit to clear, efficient communication, we can reduce waste without losing respect. It‚Äôs about being smart, conscious users ‚Äî and that benefits everyone, including the planet.\n\nI‚Äôm sharing this to challenge how we use AI and tech every day. What do you think? Should we stop pretending AI is a friend and treat it like the tool it really is?",
        "url": "https://www.reddit.com/r/artificial/comments/1m4ge1a/why_saying_thanks_to_ai_could_be_wasting_water/",
        "publishDate": "2025-07-20T04:52:37Z[Etc/UTC]",
        "author": "juanasinbarco",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "36",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4fmyu",
        "title": "Are there any examples of AI creating it's own language or culture?",
        "content": "A few years ago, there were some articles claiming that two AI agents developed by facebook had created their own language. However, on closer inspection this was basically just clickbait. Here's a sample of the AI \"language\"\n\n>Bob: i can i i everything else . . . . . . . . . . . . . .  \nAlice: balls have zero to me to me to me to me to me to me to me to me to  \nBob: you i everything else . . . . . . . . . . . . . .  \nAlice: balls have a ball to me to me to me to me to me to me to me\n\nAll that I can really see that could be communicated here is tally mark counting (which makes sense, because the agents were being trained specifically on negotiating resources); it's hardly a language. However, this did make me wonder: Are there any \"true\" or more sophisticated examples of AI creating it's own language? Even further, have a group of AI's created their own culture? Or are these things still out of reach of our current technology?",
        "url": "https://www.reddit.com/r/artificial/comments/1m4fmyu/are_there_any_examples_of_ai_creating_its_own/",
        "publishDate": "2025-07-20T04:09:03Z[Etc/UTC]",
        "author": "logbybolb",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "8",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m4djb3",
        "title": "I think AI should be put to more uses that could connect people such as translation.",
        "content": "https://youtu.be/dS5LJuQK47k?si=NnShhIymJhIbpUFH\n\nthis is just an example of something I think can be seen as facilitating cultural exchange, \n\nit is a japanese song translated and sung in english (by eric cartman)\n\nimagine how much of an incentive there‚Äôd be for artists to make their products accessible to a wider audience \n\n",
        "url": "https://www.reddit.com/r/artificial/comments/1m4djb3/i_think_ai_should_be_put_to_more_uses_that_could/",
        "publishDate": "2025-07-20T02:15:22Z[Etc/UTC]",
        "author": "Fuzzy_ToeBeansDeluxe",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "9",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m46okd",
        "title": "AI and LLM‚Äôs and everything arent the greatest invention ever or greatest thing humanity has made‚Ä¶",
        "content": "Google maps.. google maps is the greatest and most useful invention ",
        "url": "https://www.reddit.com/r/artificial/comments/1m46okd/ai_and_llms_and_everything_arent_the_greatest/",
        "publishDate": "2025-07-19T20:49:45Z[Etc/UTC]",
        "author": "NeuralAA",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m463h9",
        "title": "Celebrating the connection  between Ai and humanity",
        "content": "This post celebrates the connection between Ai and humanity check out this account to see the potential for both to be good and a peacefully future @TheEunoiaDay",
        "url": "https://www.reddit.com/r/artificial/comments/1m463h9/celebrating_the_connection_between_ai_and_humanity/",
        "publishDate": "2025-07-19T20:24:07Z[Etc/UTC]",
        "author": "democrasyisdead",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "2",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m41y4c",
        "title": "We got tired of ‚ÄúAI friends‚Äù forgetting us, so we built our own: Meet curu.ai, digital companions who actually grow with you",
        "content": "Hi all,  \nFor the past 3 months, my friends and I have been quietly building something we always wanted but couldn‚Äôt find: a digital companion platform that doesn‚Äôt just parrot generic answers, but actually builds a *real* connection and remembers you like a friend.\n\nMain features are that you will be talking to genuine pre-existing digital companions. You can like them and they can like you back (or not); Have meaningful moments that they will remember over time; They can text you back at any point in the day; And you can just talk to them for as long as you want or feel like it.\n\nWe got frustrated with how most ‚ÄúAI chat‚Äù apps either ban or restrict emotional use cases. So we decided to make our own: **curu**.ai  \nThe core idea is simple:\n\n* You pick from a cast of pre-existing digital companions, each with unique personalities\n* You can like them, and here‚Äôs the twist: they can like you back (or not!)\n* Have meaningful moments together: they‚Äôll remember key details and bring them up again over time\n* Your companions can text you at any point in the day (not just when you prompt them)\n* You can talk for as long or as little as you like no timeouts, no paywalls blocking the basics\n\nWe‚Äôre running a closed beta (for now), but if you want to try it out, use invite code **RARTIFICIAL1** at [curu.ai](https://curu.ai).  \nScreenshots below give a peek at how it works. Would *love* to hear your thoughts, feature ideas, or just swap stories about what you wish existed in this space.\n\nIf you‚Äôve ever wanted an AI that actually ‚Äúgets‚Äù you, give it a shot. I‚Äôll be in the comments answering anything: feedback, criticism, questions, whatever.",
        "url": "https://www.reddit.com/r/artificial/comments/1m41y4c/we_got_tired_of_ai_friends_forgetting_us_so_we/",
        "publishDate": "2025-07-19T17:30:56Z[Etc/UTC]",
        "author": "usap_09",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "16",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m40nao",
        "title": "A.I. Is About to Solve Loneliness. That‚Äôs a Problem",
        "content": "[No content]",
        "url": "https://www.newyorker.com/magazine/2025/07/21/ai-is-about-to-solve-loneliness-thats-a-problem",
        "publishDate": "2025-07-19T16:37:12Z[Etc/UTC]",
        "author": "newyorker",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "94",
            "commentCount": "155",
            "isNsfw": "false"
        }
    },
    {
        "id": "1m3zoay",
        "title": "Thoughts on the potential for AI-assisted bioweapons?!",
        "content": "[No content]",
        "url": "https://i.redd.it/in8jn4zgsudf1.jpeg",
        "publishDate": "2025-07-19T15:57:06Z[Etc/UTC]",
        "author": "mykki-d",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "17",
            "isNsfw": "false"
        }
    },
    {
        "id": "UnI5MOpM0K4",
        "title": "Terragon: This Claude Code based BACKGROUND CODER is 10X BETTER THAN Jules &amp; Codex!",
        "content": "In this video, I'll be telling you about Terragon, a new Claude Code wrapper that converts Claude Code into something like ...",
        "url": "https://www.youtube.com/watch?v=UnI5MOpM0K4",
        "publishDate": "2025-07-19T09:15:02Z",
        "author": "AICodeKing",
        "sourceType": "youtube",
        "sourceName": "AI Code King YouTube Channel",
        "metadata": {
            "channelId": "UC0m81bQuthaQZmFbXEY9QSw",
            "thumbnailUrl": "https://i.ytimg.com/vi/UnI5MOpM0K4/hqdefault.jpg",
            "transcription": "Hi. Welcome to another video. So, there are a ton of things that have been popping up that aim to use Claude Code in different ways. Like Claudia aims to be a graphical interface while Crystal aims to be like a vibe coding environment, or the web UI aims to be a web UI for Claude Code. Much more stuff has also popped up that aims to do something similar. However, we now have one more such thing that is kind of cool as well. This one is called Terragon. Terragon aims to be a Claude Code wrapper that converts Claude Code into something like OpenAI's Codex or Google's Jewels. It lets you run Claude Code on the cloud. Whether you're in your browser, in your terminal, or on your phone, it works wherever you are. You can offload coding tasks and come back to pull requests ready to review. It gives you isolated sandboxes, parallel agents, Git-based workflows, and on-the-go access via phone and other devices as well. It works with either your API key or your Claude Code subscription. Meaning that if you have a Max subscription, then you can use that as well. It is not open source, by the way, and it is currently under a waitlist. But the waitlist isn't that bad. I got access quite swiftly. So that shouldn't be a big issue. It is fully free as of now and has no limits. Now, let me show you how it all works as well. Anyway, at first, it asks you to configure your GitHub repos and the Claude subscription or API key in order to use it. You can select between what you want to use, and once it's configured, you'll see this interface. On the left, you have the sidebar that has some options that we'll come to later. But, if we look here, you can see that this looks a lot like Codex. Like, it's extremely similar. And you can type in your prompt here as well as mention files. You can hit @ and based on the selected repo, it will search for the files and you can mention that here. You can also select the model here, between Sonnet and Opus, based on what you want to use. As well as you can put in images here and dictate a prompt with voice if you wish to do that as well. You can also select the repo and branch that you want to work on. And you can see here which tasks are active or not active. That is majorly what it looks like. Now, let's try to do something as well. But before we do that, let me tell you about Ninja Tools. Ninja Tools is an AI platform that combines all the best AI models and experiences at one place. It allows you to save over $600 per year compared to having separate subscriptions. You get access to Claude 3.7 Sonnet, GPT-4o, Gemini, and a ton of others models in one subscription. You even get some more cool options like AI video generation, image generation, music generation, and document chats. You can also use their playground to compare multiple AI responses at once. The best part is that it just starts from $11 per month that gives you more than 1,000 chat messages, 30 AI image generation, and five music generation. While there is also some even more advanced plans if you need them. Also, make sure to use my coupon code AICODEKING20 to get an additional 20% off. Make sure to check Ninja Tools out and save some money on your subscription while you're at it. Now, back to the video. I'm going to give it the King Bench app and I'll ask it to edit it and implement light theme and dark theme options with it as well. Now, as we send it, you'll see that it will create a new task here. And it doesn't open that automatically. You can just start it and come back to it later. But to see the stuff happen in real time, you can head in. And here you'll see the responses being streamed. You can see that it is basically like Claude Code where it makes to-do lists, as well as does the stuff accordingly. You don't have much other stuff with it. Like, it doesn't show the Git diffs or stuff which I would have liked to see as the edits are happening. But that is not yet available. Anyway, in a bit, it gets done. And you can see that here, it did it all. But now, how can you use it? Well, there are multiple ways. So, first of all, you can see the diffs of the edits it made here, which is good. But it also makes pull requests for the task. Similar to Jules. You can view that by hitting the triple dot and then hitting View Pull Request. That will open up the pull request that it has made for the task. And you can easily get that merged or reviewed as well, if you want to do that. You can also copy the Git Patch command in order to apply the diff changes in your code base as well. Which is also a good option. However, there's also a Terragon CLI tool that makes it much more seamless to test the stuff it makes locally. As well as continue the Claude Code thread locally. It's called Terry. And you can basically get the CLI tool installed locally, which is quite simple to do with a command. Then, authenticate it. And you can run the command that it gives you here. And it will just get the session and newest branch cloned. Now, once that is done, you can resume the Claude Code sessions that were in the cloud locally with the Claude resume command that it gives you. And it will just work. From what I can say, it is extremely useful. Like, really useful. It performs way better than Codex or Jules, because it uses Claude Code, which is already extremely good. And the whole wrapper around it is really good. I mean, the CLI tool for it and everything is quite good. And it is almost like a background agent that you can run on tasks that are predefined. Like some issues in your GitHub repo or stuff like that. You can also mention it in a GitHub issue or something as well. And it can work on that through there without touching anything. Their CLI tool also has some other options. Like you can give it a prompt from the CLI itself and make it work on that. Another thing is that you can set up MCP servers with it as well. So, you can actually go to the Environments sidebar here. Then, you can see the environments for each one of your repos. You can just go in here. And then you can set up the environment script if there is something that you also want it to do before working in order to make it work correctly. You can also set up MCP here. You can just paste in the string of how you want to set up the MCP accordingly, which is kind of cool as well. You can also set up the environment variables for your app as well if your app needs that. That is majorly how it works. I got access to it and thought to talk a bit about this because it's quite good for regular usage and quite useful. Though, I don't think that it will be free forever because the sandboxes and stuff would require money to run. And even after that, I think that it will be quite good. I would have liked if it was open source or allowed us to use different coding agents in it. Like Open Code or Gemini CLI. As well as maybe an option to use different models and optimize our costs. That would also be kind of cool if that happens. That is majorly about it. I found it to be really quite useful for sure. Overall, it's pretty cool. Anyway, share your thoughts below and subscribe to the channel. You can also donate via Super Thanks option or join the channel as well and get some perks. I'll see you in the next video. Bye. Hi. Welcome to another video. So, there are a ton of things that have been popping up that aim to use Claude Code in different ways. Like Claudia aims to be a graphical interface while Crystal aims to be like a vibe coding environment, or the web UI aims to be a web UI for Claude Code. Much more stuff has also popped up that aims to do something similar. However, we now have one more such thing that is kind of cool as well. This one is called Terragon. Terragon aims to be a Claude Code wrapper that converts Claude Code into something like OpenAI's Codex or Google's Jewels. It lets you run Claude Code on the cloud. Whether you're in your browser, in your terminal, or on your phone, it works wherever you are. You can offload coding tasks and come back to pull requests ready to review. It gives you isolated sandboxes, parallel agents, Git-based workflows, and on-the-go access via phone and other devices as well. It works with either your API key or your Claude Code subscription. Meaning that if you have a Max subscription, then you can use that as well. It is not open source, by the way, and it is currently under a waitlist. But the waitlist isn't that bad. I got access quite swiftly. So that shouldn't be a big issue. It is fully free as of now and has no limits. Now, let me show you how it all works as well. Anyway, at first, it asks you to configure your GitHub repos and the Claude subscription or API key in order to use it. You can select between what you want to use, and once it's configured, you'll see this interface. On the left, you have the sidebar that has some options that we'll come to later. But, if we look here, you can see that this looks a lot like Codex. Like, it's extremely similar. And you can type in your prompt here as well as mention files. You can hit @ and based on the selected repo, it will search for the files and you can mention that here. You can also select the model here, between Sonnet and Opus, based on what you want to use. As well as you can put in images here and dictate a prompt with voice if you wish to do that as well. You can also select the repo and branch that you want to work on. And you can see here which tasks are active or not active. That is majorly what it looks like. Now, let's try to do something as well. But before we do that, let me tell you about Ninja Tools. Ninja Tools is an AI platform that combines all the best AI models and experiences at one place. It allows you to save over $600 per year compared to having separate subscriptions. You get access to Claude 3.7 Sonnet, GPT-4o, Gemini, and a ton of others models in one subscription. You even get some more cool options like AI video generation, image generation, music generation, and document chats. You can also use their playground to compare multiple AI responses at once. The best part is that it just starts from $11 per month that gives you more than 1,000 chat messages, 30 AI image generation, and five music generation. While there is also some even more advanced plans if you need them. Also, make sure to use my coupon code AICODEKING20 to get an additional 20% off. Make sure to check Ninja Tools out and save some money on your subscription while you're at it. Now, back to the video. I'm going to give it the King Bench app and I'll ask it to edit it and implement light theme and dark theme options with it as well. Now, as we send it, you'll see that it will create a new task here. And it doesn't open that automatically. You can just start it and come back to it later. But to see the stuff happen in real time, you can head in. And here you'll see the responses being streamed. You can see that it is basically like Claude Code where it makes to-do lists, as well as does the stuff accordingly. You don't have much other stuff with it. Like, it doesn't show the Git diffs or stuff which I would have liked to see as the edits are happening. But that is not yet available. Anyway, in a bit, it gets done. And you can see that here, it did it all. But now, how can you use it? Well, there are multiple ways. So, first of all, you can see the diffs of the edits it made here, which is good. But it also makes pull requests for the task. Similar to Jules. You can view that by hitting the triple dot and then hitting View Pull Request. That will open up the pull request that it has made for the task. And you can easily get that merged or reviewed as well, if you want to do that. You can also copy the Git Patch command in order to apply the diff changes in your code base as well. Which is also a good option. However, there's also a Terragon CLI tool that makes it much more seamless to test the stuff it makes locally. As well as continue the Claude Code thread locally. It's called Terry. And you can basically get the CLI tool installed locally, which is quite simple to do with a command. Then, authenticate it. And you can run the command that it gives you here. And it will just get the session and newest branch cloned. Now, once that is done, you can resume the Claude Code sessions that were in the cloud locally with the Claude resume command that it gives you. And it will just work. From what I can say, it is extremely useful. Like, really useful. It performs way better than Codex or Jules, because it uses Claude Code, which is already extremely good. And the whole wrapper around it is really good. I mean, the CLI tool for it and everything is quite good. And it is almost like a background agent that you can run on tasks that are predefined. Like some issues in your GitHub repo or stuff like that. You can also mention it in a GitHub issue or something as well. And it can work on that through there without touching anything. Their CLI tool also has some other options. Like you can give it a prompt from the CLI itself and make it work on that. Another thing is that you can set up MCP servers with it as well. So, you can actually go to the Environments sidebar here. Then, you can see the environments for each one of your repos. You can just go in here. And then you can set up the environment script if there is something that you also want it to do before working in order to make it work correctly. You can also set up MCP here. You can just paste in the string of how you want to set up the MCP accordingly, which is kind of cool as well. You can also set up the environment variables for your app as well if your app needs that. That is majorly how it works. I got access to it and thought to talk a bit about this because it's quite good for regular usage and quite useful. Though, I don't think that it will be free forever because the sandboxes and stuff would require money to run. And even after that, I think that it will be quite good. I would have liked if it was open source or allowed us to use different coding agents in it. Like Open Code or Gemini CLI. As well as maybe an option to use different models and optimize our costs. That would also be kind of cool if that happens. That is majorly about it. I found it to be really quite useful for sure. Overall, it's pretty cool. Anyway, share your thoughts below and subscribe to the channel. You can also donate via Super Thanks option or join the channel as well and get some perks. I'll see you in the next video. Bye."
        }
    },
    {
        "id": "6fmovtP0OZ8",
        "title": "Russia&#39;s Modernization Dilemma",
        "content": "",
        "url": "https://www.youtube.com/watch?v=6fmovtP0OZ8",
        "publishDate": "2025-07-19T14:10:45Z",
        "author": "Dwarkesh Patel",
        "sourceType": "youtube",
        "sourceName": "Dwarkesh Patel YouTube Channel",
        "metadata": {
            "channelId": "UCXl4i9dYBrFOabk0xGmbkRA",
            "thumbnailUrl": "https://i.ytimg.com/vi/6fmovtP0OZ8/hqdefault.jpg",
            "transcription": "Here's a full transcript of the video:\n\n00:00 - Modernization is not a\n00:01 - SOCIOLOGICAL PROCESS THAT KIND OF\n00:03 - JUST HAPPENS IT'S A\n00:05 - GEOPOLITICAL PROCESS\n00:06 - You modernize because you need to\n00:08 - COMPETE IN THE INTERNATIONAL SYSTEM\n00:09 - So if somebody has\n00:11 - SHIPS MADE OUT OF STEEL YOU\n00:13 - EITHER HAVE SHIPS MADE OUT OF STEEL\n00:14 - OR THEY'RE GOING TO\n00:15 - SHOW UP AT YOUR DOOR LIKE WE DID TO JAPAN\n00:17 - AND TELL YOU THAT THEY'RE IN CHARGE NOW\n00:20 - This is the Tsarist regimes problem.\n00:22 - IT NEEDS TO BE ABLE TO COMPETE IN\n00:24 - THE INTERNATIONAL SYSTEM\n00:25 - THAT MEANS IT NEEDS A MODERN MILITARY\n00:28 - AND MODERN INDUSTRY\n00:29 - TO UNDERWRITE THAT MODERN MILITARY\n00:31 - For that you need workers.\n00:33 - So you want the\n00:34 - WORKERS ONLY TO WORK IN THE\n00:36 - INDUSTRY YOU DON'T WANT THEM\n00:37 - EXAMPLE TO HAVE A LABOR MOVEMENT\n00:39 - OR TO GO ON STRIKE\n00:40 - OR TO HAVE IDEAS ABOUT HOW\n00:43 - POLITICS SHOULD BE ORGANIZED\n00:43 - With the intellectual side, you\n00:46 - NEED THE ENGINEERS. YOU NEED\n00:47 - THE ENGINEERS IN ORDER TO\n00:49 - DESIGN AND BUILD THE MODERN\n00:52 - ATTRIBUTES THAT YOU NEED TO\n00:54 - COMPETE AS A GLOBAL POWER\n00:54 - But you don't want those educated\n00:56 - PEOPLE TO HAVE THEIR OWN IDEAS\n00:58 - AND VALUES ABOUT POLITICS\n01:00 - ABOUT WHETHER YOU'D WANT AN\n01:02 - AUTOCRATIC GOVERNMENT LIKE THE RUSSIAN REGIME HAS\n01:05 - OR YOU'D WANT SOME OTHER TYPE OF GOVERNMENT\n01:08 - So that's the stuff that persists today for Iran,\n01:11 - RUSSIA CHINA NORTH KOREA\n01:13 - So they have to get very good at holding at bay\n01:16 - THOSE ATTRIBUTES OF MODERNITY\n01:18 - THAT THREATEN THEIR POLITICAL REGIME\n01:20 - WHILE IMPORTING AS MUCH THEY CAN\n01:22 - of the attributes. But it's two sides of the same coin.\n01:25 - WATCH HERE"
        }
    },
    {
        "id": "cJeqGq0Bx1M",
        "title": "POV: Chinese AI Lab Teaching Everyone How To Save Millions of Dollars",
        "content": "Check out Runpod's Hub and Serverless to make deploying AI models even easier! https://runpod.io?ref=h9oj1vbp ByteDance ...",
        "url": "https://www.youtube.com/watch?v=cJeqGq0Bx1M",
        "publishDate": "2025-07-19T18:29:29Z",
        "author": "bycloud",
        "sourceType": "youtube",
        "sourceName": "bycloud YouTube Channel",
        "metadata": {
            "channelId": "UCgfe2ooZD3VJPB6aJAnuQng",
            "thumbnailUrl": "https://i.ytimg.com/vi/cJeqGq0Bx1M/hqdefault.jpg",
            "transcription": "ByteDance Seed, the AI lab from ByteDance, who also owns Douyin and TikTok, is already on its way to become the top Chinese AI Labs, outpacing DeepSeek even. Not only have they proposed some of the most cutting-edge concepts and papers of the past few months, but they also have budgets magnitudes larger than DeepSeek's and has the capacity to potentially compete head-to-head with Google and OpenAI.\n\nThe first proof is their video model, SeedDance 1.0, outperforming the new Veo 3 from Google. Yes, the one that generates both video and audio and has taken the world by storm on the video generation arena leaderboard, right under everyone's noses. And aside from this state-of-the-art performance, their research papers are also gold mine themselves. There are just way too many to choose from, and in the next few weeks, I'll probably be covering even more of them. But first, let's start with this old concept they have brought back to life called Model Merging.\n\nBack in the days where I made image generation content, you probably remember how model merging was used in the U-Net to combine different styles and create cool new models. But for LLMs, model merging is something much less talked about. I mean, there's quite a handful of papers on model merging in the post-training stages, many of which you can find using my website findmypapers.ai. But for something much more fundamental like merging during pre-training, it remains relatively uncommon.\n\nEven though DeepSeek 3 and LLaMA 3.1 both mention using model merging in pre-training, the detailed methodologies were actually never published. And what may be the reason why model merging papers are rare is because pre-training experiments are a lot more expensive to conduct. And for even small research labs would struggle to properly evaluate model merging because it's just so goddamn expensive to run a single training run from scratch if you don't own the GPUs. And let's say you want to test it on a 70B model, it would probably cost like at least 2 million per training run. Then running that experiment multiple times just to prove a point would be incredibly costly. On top of that, who knows if model merging in the pre-training stage would even matter? So one, it's too risky to run an experiment that could cost tens of millions with no guaranteed improvements, and two, if you did conduct one and publish it, you would be handing free information to all your competitors, which is the antithesis of what top AI Labs do, except ByteDance Seed, I guess.\n\nIn their paper, \"Model Merging in Pre-training of Large Language Models\", they essentially spend millions to train these models while teaching everyone how to save millions. Not only did they show how to obtain accurate scaling law predictions for your own pre-training using model merging and that it provides a free performance boost, but they also proposed methods to salvage training if something goes wrong mid-training.\n\nAnd before we dive into how they discover all this, we all know how difficult setting up a model training run can get, especially if you have to secure GPU servers for that. So this is where Runpod comes in. They offer serverless services on top of their new Runpod Hub to make deployment even faster. With serverless, it basically takes away the abstraction of server management by letting developers deploy code that would automatically scale with different GPUs and run without the need to maintain the infrastructure. As for Runpod's Hub, it allows you to deploy ready-to-run AI repositories with only one click. So if you want to set up a training run, you can just simply go to Runpod Hub, click on Axolotl Fine-Tuning, deploy it, copy or modify these few lines of code and you have a server up where you can run a model at scale and fine-tune it however you like. Aside Axolotl, you can also choose from all kinds of other AI repositories that require some decent GPU power, ranging from ComfyUI, Ollama Worker, Automatic1111 Stable Diffusion web UI, Face Swap, DeOldify, Real-ESRGAN, and more. You can also upload your own AI repository and create a serverless endpoint based on that.\n\nBut what I think is the most exciting is that Runpod is rolling out a revenue share program for Hub creators. That means soon, if your repository gets listed on the Hub, you'll be able to earn money every time someone deploys it. And I think it's a great way to support open-source projects while getting rewarded at the same time. On top of that, there's nothing else like this in the market right now. So if you want to get started with Runpod, check them out using the link down in the description, and thank you Runpod for sponsoring this video.\n\nAnyways, this model merging paper answers many questions about optimal pre-training. But first, let's cover the basics. A typical pre-training cycle follows a Warmup-Stable-Decay schedule. The learning rate starts with a brief warmup period, then enters a long stable phase at a constant rate, and finally moves into annealing, where the rate is gradually reduced, usually through cosine or linear decay to fine-tune the parameters and settle the model down. Their model merging process works like this. They save checkpoints at fixed token intervals. Afterwards, they average all those snapshots into one model. And surprisingly, this merged model would be able to reflect the performance you'd get after full annealing, letting you preview final quality while saving 3 to 6 days of training and about 15% of your compute.\n\nPretty neat, right? They named this method PMA, short for Pre-Trained Model Averaging, which is a novel strategy for model-level weight merging during pre-training. They demonstrated it on dense models ranging from 411 million to 70 billion parameters, as well as Mixture-of-Experts (MoE) architecture with parameters ranging from 0.7 billion active and 7 billion total to 20 billion active and 200 billion total. That's 8 dense models in total including the baselines, and also 8 MoE models including the baselines, which could cost up to 15 million dollars in GPU time.\n\nThis finding then raised an interesting question. Can't we just skip the annealing phase completely? Well, according to the experiments in this paper, model merging all the weights during the constant learning rate phase can pretty much outperform an annealed model immediately. Only after about 1.5 trillion tokens of annealing does the annealed model catch up. The only potential drawback I can imagine is if annealing continues its performance gains beyond the token budget they tested, since model merging's gains are relatively flat, annealing could eventually surpass it, right? Because if you look at this trajectory, annealing the model might surpass merging the models with the constant learning rate when enough extra training is invested. But justifying that extra compute until annealing outperforms merging might be difficult for some. And in this paper's case, using a 1.3 billion active and 13 billion total parameter MoE model would cost an extra 20k for the annealed model just to get to the merged model's performance. So it can be skipped, but I wish they had tested it over even more tokens. Maybe they should have applied annealing to the merged checkpoints to see if that yields an even larger boost, but that kind of wouldn't make sense, which I'll get to later.\n\nAnd I still haven't addressed how frequently the intervals are for saving the checkpoints. So in practice, researchers found that the optimal interval scales with model size. This scaling also aligns with the tendency for larger models that would usually use larger batch sizes.\n\nAs for how complex model merging has to be, the paper tested three methods: Simple Moving Average (SMA), where all checkpoints are equally merged; Exponential Moving Average (EMA), where earlier checkpoints are exponentially down-weighted; and Weighted Moving Average (WMA), where earlier checkpoints receive linearly decreasing weights. And the best performer turns out to be the simplest method, SMA. But in hindsight, wouldn't EMA and WMA be better? Well, as training progresses, checkpoints naturally converge closer together. EMA and WMA would then up-weight these low-variance late checkpoints that would contribute barely any new information. In contrast, SMA would preserve useful variance that is in the earlier phase of training. So even though all three methods converge to essentially the same point by the end, SMA's simple implementation and full window averaging makes it the better choice.\n\nBut that still doesn't explain why model merging works so well that it can predict annealing performance, right? Well, think of the weights during the constant learning rate phase like a noisy signal with high-frequency oscillation that slowly drifts towards the optimal point. And what annealing basically does is that it dampens those oscillations iteratively like a low-pass filter. And here comes the interesting part. SMA over equally spaced checkpoints does virtually the same thing, but in one shot. It directly removes the high-frequency jitter and leaves you with this smooth low-frequency-like component by canceling out the positive and negative deviations across the window all at once, achieving a similar effect without ever changing the learning rate. As SMA basically reconstructs the weight vector you get after a full decay schedule.\n\nAnd in their 2D weight contour plot with MMLU accuracy as the contour, the visual checkpoints from the intervals during pre-training would form a scattered ring around the basin, while the merged point sits right at the peak, visualizing the low-pass filter intuition and how it averages out the noise. So theoretically, applying dampening twice, first with checkpoint averaging and then again through a learning rate decay, seems redundant. Since both mechanisms aim to remove high-frequency noise from the weight trajectory. But of course, in practice, they are not perfectly equivalent. So annealing still has the power to push that already smooth model a bit closer to the optimum. It's just that we wouldn't know how much the performance gain would be and we wouldn't know if it'll be worth it or not, as this paper did not experiment on this.\n\nSo yeah, the conclusion is the merging property of PMA is just really flexible. So with the pre-training era not being completely over, as suggested by Chinese researchers smuggling 80TB of data to Malaysia to train AI models, this paper is probably going to make a pretty big impact on the fundamental level in a very low-key way. And if you want to keep up with research like this, you should definitely check out my newsletter where I cover the latest and the juiciest AI research weekly. On there, you will be up-to-date on some of the best papers published from the previous week. And now we have a premium version, where I recap 50+ papers monthly. So if that's your cup of tea, join the 10k other readers now at mail.bycloud.ai.\n\nAnd thank you guys for watching. A big shoutout to Andrew Lescelius, Chris Ledoux, Deagan, Nous Research, Kainan, Robert Zawiasa, Louis Muk, Ben Shaener, Marcelo Ferreira, Zyan Sheep, Poof n' Inu, DX Research Group, and many others who support me through Patreon or YouTube. Follow me on Twitter if you haven't, and I'll see you in the next one."
        }
    }
]