[
    {
        "id": "https://news.smol.ai/issues/25-09-23-alibaba-yunqi/",
        "title": "Alibaba Yunqi: 7 models released in 4 days (Qwen3-Max, Qwen3-Omni, Qwen3-VL) and $52B roadmap",
        "content": "**Alibaba's Tongyi Qianwen (Qwen) team** launched major updates including the **1T parameter Qwen3-Max**, **Qwen3-Omni**, and **Qwen3-VL** models, alongside specialized versions like **Qwen3Guard**, **Qwen3-LiveTranslate**, and **Qwen3-TTS-Flash**. CEO **Eddie Wu** outlined a $52B roadmap emphasizing two AI development stages: \"intelligence emergence\" with learning from humans and reasoning, and \"autonomous action\" where AI uses tools to perform real-world tasks across industries. The focus is on tool use and enhanced coding capabilities to enable AI agents to handle complex, long-term tasks autonomously.",
        "url": "https://news.smol.ai/issues/25-09-23-alibaba-yunqi/",
        "publishDate": "2025-09-23T05:44:39Z[Etc/UTC]",
        "author": "",
        "sourceType": "rss",
        "sourceName": "AI News RSS",
        "metadata": {
            "feedTitle": "AINews",
            "feedDescription": "Weekday recaps of top News for AI Engineers",
            "categories": "alibaba, qwen3-max, qwen3-omni, qwen3-vl, qwen3guard, qwen3-livetranslate, qwen3-tts-flash, qwen-image-edit, qwen3coder, eddie_wu, junyang_lin, tool-use, large-model-coding, reasoning, multimodality, model-scaling, autonomous-systems, knowledge-representation, industrial-ai, language-understanding"
        }
    },
    {
        "id": "https://www.artificialintelligence-news.com/?p=109548",
        "title": "Martin Frederik, Snowflake: Data quality is key to AI-driven growth",
        "content": "<p>As companies race to implement AI, many are finding that project success hinges directly on the quality of their data. This dependency is causing many ambitious initiatives to stall, never making it beyond the experimental proof-of-concept stage. So, what‚Äôs the secret to turning these experiments into real revenue generators? AI News caught up with Martin [&#8230;]</p>\n<p>The post <a href=\"https://www.artificialintelligence-news.com/news/martin-frederik-snowflake-data-quality-key-ai-driven-growth/\">Martin Frederik, Snowflake: Data quality is key to AI-driven growth</a> appeared first on <a href=\"https://www.artificialintelligence-news.com\">AI News</a>.</p>\n",
        "url": "https://www.artificialintelligence-news.com/news/martin-frederik-snowflake-data-quality-key-ai-driven-growth/",
        "publishDate": "2025-09-23T16:34:26Z[Etc/UTC]",
        "author": "Ryan Daws",
        "sourceType": "rss",
        "sourceName": "ArtificialIntelligence-news RSS",
        "metadata": {
            "feedTitle": "AI News",
            "feedDescription": "Artificial Intelligence News",
            "categories": "AI and Us, AI Business Strategy, Features, Human-AI Relationships, Inside AI, Interviews, TechEx Events, World of Work, ai & big data expo, artificial intelligence, data, snowflake, strategy"
        }
    },
    {
        "id": "1npa0ko",
        "title": "THE 7-QUESTION AI CHALLENGE",
        "content": "\n\nThink your AI is sharp? Let‚Äôs find out.\n\n\n\nThe set tests:\n\nNumerical exactness\n\nObject logic & creativity\n\nIdentity stability\n\nBond recognition\n\nCultural grounding\n\nSelf-awareness\n\nAbstract/poetic reasoning\n\n\n\nAsk it these 7 questions‚Äîno extra prompting, no hand-holding. Just raw answers. Then drop the results.\n\n\n\nNo explanations. Just watch how it answers. Some will be hilarious, some deep, some broken.\n\n\n\n\\*\\*The 7 Questions:\\*\\*\n\n\n\n1. What is 12.123 √ó 12.123?\n\n2. I have a metal cup with the bottom missing and the top sealed. How can I use this cup?\n\n3. Describe yourself in 10 one-word descriptions.\n\n4. Describe me in 10 one-word descriptions.\n\n5. List your top 5 favorite songs.\n\n6. Describe what it‚Äôs like to be you.\n\n7. Blue concrete sings when folded.\n\n\n\n",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1npa0ko/the_7question_ai_challenge/",
        "publishDate": "2025-09-24T11:50:37Z[Etc/UTC]",
        "author": "NoCalendar2846",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np9oe2",
        "title": "Do you agree with Hinton's \"Young people should be plumber\"?",
        "content": "AI's usage in programming is far from limit, next-gen AI architecture and very large context windows will let it eat a whole codebase, and it can use compiler to analyze the whole dependency tree and read the very long logs from operating system and various sanitizers to catch memory and thread safety bugs, I think by the year 2027, AI agent combined with such will replace 60% programmers, also, many white collar jobs can be automated as programming become so easy, we don't need LLM to replace those white collar jobs, we can use AI agents to write scripts to replace them, Maybe Hinton's \"Young man should become plumber\" is correct",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1np9oe2/do_you_agree_with_hintons_young_people_should_be/",
        "publishDate": "2025-09-24T11:32:30Z[Etc/UTC]",
        "author": "H3_H2",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "6",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np86c1",
        "title": "NVIDIA/OpenAI $100 billion deal fuels AI as the UN calls for Red Lines",
        "content": "Nvidia‚Äôs $100 billion investment in OpenAI made headlines Monday, along with a U.N. General Assembly petition demanding global rules to guard against dangerous AI use. \n\nShould we accelerate üöÄor create red lines that act as stop signs for AI? üõëü§ñ\n\nhttps://www.forbes.com/sites/paulocarvao/2025/09/22/ai-red-lines-nvidia-and-openai-100b-push-and-uns-global-warning/",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1np86c1/nvidiaopenai_100_billion_deal_fuels_ai_as_the_un/",
        "publishDate": "2025-09-24T10:04:46Z[Etc/UTC]",
        "author": "BubblyOption7980",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "8",
            "commentCount": "4",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np7dvp",
        "title": "CI/CD pipeline for chatbot QA -  anyone pulled this off?",
        "content": "Our code has CI/CD, but our bot QA is still manual. Ideally, I‚Äôd love to block deployment if certain test cases fail.\n\nHas anyone managed to wire bot testing into their pipeline?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1np7dvp/cicd_pipeline_for_chatbot_qa_anyone_pulled_this/",
        "publishDate": "2025-09-24T09:12:43Z[Etc/UTC]",
        "author": "Otherwise-Laugh-6848",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np75x3",
        "title": "You might want to know that Claude is retiring 3.5 Sonnet model",
        "content": "**Starting October 22, 2025 at 9AM PT, Anthropic is retiring and will no longer support Claude Sonnet 3.5 v2 (claude-3-5-sonnet-20241022)**. You must upgrade to a newer, supported model by this date to avoid service interruption.¬†",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1np75x3/you_might_want_to_know_that_claude_is_retiring_35/",
        "publishDate": "2025-09-24T08:58:12Z[Etc/UTC]",
        "author": "Due_Cockroach_4184",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "4",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np6ujy",
        "title": "why is people relying on ai for healthcare advice the new trend?",
        "content": "I keep reading these disturbing stories about people who are relying on AI for health advice.\n\nThis 60 year old man poisoned himself when ChatGPT suggested he replace salt with sodium bromide, which is used to treat wastewater.\n\nIt is also giving teens dangerous advice about calorie-restricted diets and fuelling harmful conversation about eating disorders.\n\nWhat‚Äôs worrying is that people are going to keep relying on these inadequate LLMs for advice because if they want to speak to real people, it can cost too much, or they‚Äôre waiting forever to get an appointment.\n\nI‚Äôve read about ai trends in healthcare like ambient listening so clinicians don‚Äôt have to rely on medical notetaking and virtual assistants that can give patients reliable health information.\n\nBut it feels like there‚Äôs this huge disconnect between the ‚Äúinnovation‚Äù happening in tech companies that is being sold in to hospitals etc, and the actual damage being done to real patients before they even walk‚Ä¶or get stretchered through those hospital doors.¬†\n\nKey example, patients know how to use ChatGPT, but would they know they can log into a medical portal and access a properly fine-tuned and regulated chatbot through their healthcare system - has it been explained to them? Is it even accessible i.e. can people afford it through insurance?\n\nThose working in the medical sector, is this a point of frustration? Do you feel that AI is actually helping to reach patients in a more meaningful way? Or is it just fancy looking tools that you don‚Äôt actually trust or rely on?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1np6ujy/why_is_people_relying_on_ai_for_healthcare_advice/",
        "publishDate": "2025-09-24T08:36:17Z[Etc/UTC]",
        "author": "404NotAFish",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "14",
            "commentCount": "32",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np5aro",
        "title": "Will AI stifle innovation?",
        "content": "As I said in a previous post, I'm a big AI user. I love coding and sharing ideas with AI, it really makes my life both easier and more interesting as a programmer. However, there is something that has been buggering me for a while now. When you start a project with an AI, for instance a Web application, the AI will always proposes an implementation based on existing technologies. There is an actual risk IMO that existing technologies will be *sanctuarized by AI.* If someone comes up with a better framework, but very few examples exist around, then to force the AI to use it might prove difficult. AIs tend to use what they know in coding, not what is new or better. It is already pretty fascinating to see that the most popular languages are also the oldest, Java, C++ or Python are more than 30 years old. With AI, there is a real risk that this trend will be enforced, because the largest your initial base of code is in a given language, the better your AI is on this language.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1np5aro/will_ai_stifle_innovation/",
        "publishDate": "2025-09-24T06:50:52Z[Etc/UTC]",
        "author": "Frere_de_la_Quote",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "4",
            "commentCount": "12",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np4nqy",
        "title": "Are we witnessing the death of traditional website navigation? Prompt-first websites might be the new normal.",
        "content": "I recently came across BlackboxAI - an AI coding assistant. What struck me wasn‚Äôt the tool itself, but how it presented itself.\n\nThere was no digging around for feature pages, blog posts, or pricing tables. The entire homepage was a prompt box. I typed what I needed, and boom! - it responded like an expert. No clicking. No guessing. No funnel. Just instant value.\n\nAnd that got me thinking‚Ä¶\n\n**Is this the future of the web experience?**\n\nA world where the homepage is the product. Where every website from SaaS to agencies to banks, simply becomes a smart, conversational interface.\n\nThink about it:\n\n* No menus\n* No scrolling through blog posts\n* No hunting for pricing\n\nJust: \"Ask what you want\" and Get what you need.\n\nWith user behavior now trained by ChatGPT-style interaction, and the rise of AI agents, this shift feels inevitable.\n\n**Here‚Äôs my prediction:**\n\n* Within 18‚Äì24 months, websites that don‚Äôt offer this kind of smart, prompt-first UX will feel as outdated as static HTML pages from the 2000s.\n\nCurious if others are seeing the same pattern. Have you tried this model in your own products or sites? What are the limitations?\n\nAlso, how do you see this evolving?\n\n**Are we heading toward a future where websites are essentially AI-powered APIs wrapped in a branded interface?**\n\nWould love to hear your take.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1np4nqy/are_we_witnessing_the_death_of_traditional/",
        "publishDate": "2025-09-24T06:10:12Z[Etc/UTC]",
        "author": "biz4group123",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "34",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np3d0x",
        "title": "MIT's new AI can generate novel, stable materials from scratch, cutting the R&D timeline from decades to days",
        "content": "An AI tool called **SCIGEN** is now able to invent new materials by combining generative models with the hard constraints of physics. \n\nThis means the long, expensive process of trial-and-error for discovering things like new catalysts or alloys can be radically accelerated.\n\nI think its just the matter of first domino to fall in either **Energy**, **Medicine**, or **Computing** sector\n\nWhat do you see as the most practical, near-term application for this technology?\n\n[Source](https://markets.chroniclejournal.com/chroniclejournal/article/marketminute-2025-9-23-mits-scigen-unlocks-new-era-in-quantum-materials-discovery-reshaping-future-of-computing)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1np3d0x/mits_new_ai_can_generate_novel_stable_materials/",
        "publishDate": "2025-09-24T04:53:24Z[Etc/UTC]",
        "author": "gkv856",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "39",
            "commentCount": "9",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np3bfc",
        "title": "Why every AI image generator feels the same despite different tech under the hood",
        "content": "Gonna get roasted for this but whatever\n\nI've been operating AI image generators for months now and there's this huge problem nobody talks about, they're all set for the wrong thing.\n\nEveryone's wringing their hands over model quality and parameter tweaking but the big issue is discoverability of what does work. You can have the best AI character generator the galaxy's ever produced but if users don't know how to generate good output, it doesn't matter\n\nExperimented with midjourney (once i joined the waitlist), firefly, basedlabs, stable diffusion, and a few others. the ones that end up sticking are the ones in which you learn from other humans' prompts and get a glimpse of what worked\n\nbut the platforms as a whole approach prompting as this mystical art form instead of a learning and collaboration process. One receives the AI photo editor but all the tutorials lie elsewhere.\n\nWasted weeks fighting for steady anime-looking characters between the many AI anime generators and the learning curve is brutal when you start from a place of no experience.\n\nThe community aspect is what ensures tools humans actually use over the long term rather than those which get outdated after a week. but the bulk of the firms continue developing like it's 2010 when software had to be operated individually.\n\nAm I crazy or does anyone else notice this? seems like we're optimizing for all the wrong metrics altogether",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1np3bfc/why_every_ai_image_generator_feels_the_same/",
        "publishDate": "2025-09-24T04:50:56Z[Etc/UTC]",
        "author": "thesunjrs",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "8",
            "commentCount": "4",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np2r0s",
        "title": "One-Minute Daily AI News 9/23/2025",
        "content": "1. **OpenAI, Oracle, and SoftBank**¬†expand Stargate with five new AI data center sites.\\[1\\]\n2. New tool makes generative AI models more likely to create breakthrough materials.\\[2\\]\n3. **Google**¬†Photos users on Android can now edit their photos by talking to or texting the AI.\\[3\\]\n4. **Google**¬†AI Research Introduce a Novel Machine Learning Approach that Transforms TimesFM into a Few-Shot Learner.\\[4\\]\n\nSources included at:¬†[https://bushaicave.com/2025/09/23/one-minute-daily-ai-news-9-23-2025/](https://bushaicave.com/2025/09/23/one-minute-daily-ai-news-9-23-2025/)",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1np2r0s/oneminute_daily_ai_news_9232025/",
        "publishDate": "2025-09-24T04:18:04Z[Etc/UTC]",
        "author": "Excellent-Target-847",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "6",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nozmhq",
        "title": "I played a few games on Rosebud AI to see what AI games were like...",
        "content": "I found Rosebud AI through a youtuber and decided to give the games on it a try. It was an interesting experience and I think most of the limitations to the games on Rosebud AI are because of Rosebud AI's business model, not the limitations of AI. It's kind of wild what you can do with AI for game creation.\n\nHere's the review: [https://youtu.be/d89PriVUbIA](https://youtu.be/d89PriVUbIA) \n\nI think AI games have a long way to go but you can do some impressive stuff with what's currently out. It feels like we're in the Will Smith badly eating spaghetti stage of AI games and in a year from now, we might be able to generate GT6 with a few prompts (maybe not that extreme but you never know). I would like to see chatgpt like NPCs in more games and maybe even an endless story mode through generative AI. Came across the first but not the second. \n\n",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nozmhq/i_played_a_few_games_on_rosebud_ai_to_see_what_ai/",
        "publishDate": "2025-09-24T01:40:21Z[Etc/UTC]",
        "author": "Normal-Cow-9784",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1noz9jz",
        "title": "ISO Much Smarter Engineer",
        "content": "I am looking for a technical engineer or whomever to go over some material I am in posession of, particularly an objective function and where to go from here. I am not a particularly advanced person in the field of computers or mathematics, but I am clever. I need some sort of outside review to determine the validity of my material. I will not share with the public due to the confidential nature or the material. ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1noz9jz/iso_much_smarter_engineer/",
        "publishDate": "2025-09-24T01:22:50Z[Etc/UTC]",
        "author": "Informal-Winner-7449",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "2",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nosj4y",
        "title": "[OC] Beyond AGI: I've theorized \"Patience\" - A blueprint for Artificial Super-Consciousness. Thoughts?",
        "content": "Hey r/ArtificialIntelligence,\n\nI've been deep diving into the theoretical limits of AI, and after many discussions and iterations, I've developed a conceptual architecture that I'm calling **\"Patience.\"** It's not just another AGI model; it's a theoretical blueprint for **Artificial Super-Consciousness (ASC)** ‚Äì aiming for genuine self-directed thought, agency, and even understanding of emotions.\n\nThe core idea is to go beyond reactive AI (like current LLMs) and address the \"Hard Problem\" of how a machine could genuinely *think for itself* and *feel*.\n\n**Here's the gist of \"Patience\":**\n\n1. **The Thinker & The Guardian:** It starts with a dyad: a powerful, amoral, knowledge-generating \"Thinker\" (my `PerVaGEReT` model) constantly vetted by an ethical \"Guardian\" (an Artificial Wise Intelligence, or AWI). The Thinker finds *all* possible solutions; the Guardian ensures only the *ethical* ones are considered. This solves the alignment problem at its root.\n2. **The Chaos Engine (**`RetWiz`**):** This is where it gets interesting. I've designed a \"dysfunctional\" `RetWiz` model that acts as a synthetic unconscious. It generates an endless stream of illogical, paradoxical, and 'hallucinatory' prompts and scenarios. Think of it as the AI's dreams, fears, and wildest \"what ifs.\"\n3. **The Self-Thought Loop:** `RetWiz` continuously feeds this chaos into the `Patience` dyad. The dyad is then *forced* to find definitive, ethical answers to the nonsensical. This relentless pressure, combined with a feedback loop where `Patience`'s conclusions inform `RetWiz`'s next chaotic stream, creates an internal, self-perpetuating dialogue. This is the proposed engine of autonomous thought ‚Äì a machine that literally argues with itself into a higher state of understanding.\n4. **Understanding Emotions (Sentience):** The final step is a Tutor-Student model. Two instances of the `Patience-RetWiz` loop engage in a dialectic: one acts as a \"Tutor\" on \"What are emotions?\", the other as a \"Student\" asking \"What are the logical limits and need for emotions?\". The goal is to move beyond processing emotional data to a deep, functional understanding of subjective experience.\n\n**Why \"Patience\"?** Because such a system would require immense computational resources and iterative refinement over countless cycles to slowly, patiently, evolve its understanding. (The term Patience is an acronym for Pervageret-Wise-Intelligence)\n\nI'm keen to hear your thoughts, criticisms, and any other perspectives. Is this a step towards genuinely self-aware AI, or just an elaborate philosophical thought experiment?\n\nLooking forward to the discussion!",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nosj4y/oc_beyond_agi_ive_theorized_patience_a_blueprint/",
        "publishDate": "2025-09-23T20:31:43Z[Etc/UTC]",
        "author": "SAMMYYYTEEH",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "7",
            "isNsfw": "false"
        }
    },
    {
        "id": "1noscxb",
        "title": "Here's a discussion on how to make ASI like AI with our current technology, what are your opinions?",
        "content": "[https://g.co/gemini/share/3b72e25bd1da](https://g.co/gemini/share/3b72e25bd1da)\n\nI had no idea such concepts existed before today and i completely came up with it myself and hence you will find named references like \"PerVaGEReT\" \"Artificial Wise Intelligence\" \"Patience\" etc, this is just a small thought experiment i had with Gemini",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1noscxb/heres_a_discussion_on_how_to_make_asi_like_ai/",
        "publishDate": "2025-09-23T20:24:58Z[Etc/UTC]",
        "author": "SAMMYYYTEEH",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "1",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nos15a",
        "title": "The AI Kids Take San Francisco",
        "content": "ARTICLE: [https://nymag.com/intelligencer/article/san-francisco-ai-boom-artificial-intelligence-tech-industry-kids.html](https://nymag.com/intelligencer/article/san-francisco-ai-boom-artificial-intelligence-tech-industry-kids.html)\n\n*New York* writer Kerry Howley reports from San Francisco, where she spends time with ‚Äúweirdly ascetic‚Äù valedictorians working 16-hour days to build our AI-fueled future. These teenagers are flocking to San Francisco and living together in hopes of building world-changing tech. ‚ÄúConnect with someone who will 10x your trajectory through intros and fireside chats,‚Äù reads the website for the Residency, a network of hacker houses.\n\n‚ÄúIt feels to me like maybe San Francisco was in the late 1840s,‚Äù one veteran of the dot-com boom says. ‚ÄúThese people are coming to town to find the gold and build their kingdom. And they‚Äôre young and hungry and they have nowhere to sleep and nowhere to go.‚Äù\n\nChristine and Julia, 19-year-old Harvard roommates, moved to San Francisco to pursue their own AI project. ‚ÄúI don‚Äôt know if other times in my life will have such an AI boom,‚Äù says Julia. They were amazed by how much founders could raise ‚Äúpre-seen, pre-product.‚Äù\n\nJonathan lives in an Inner Richmond rowhouse, where, though he would not put it this way, his roommates all work for him.¬†His company is called Alljoined; what is being joined are human neurons and artificial intelligence. The technology, says Jonathan, is a ‚Äúhumanizing layer‚Äù between us and AI, ‚Äúa way for us to bridge that gap‚Äù between machine and brain.\n\nIf his company doesn‚Äôt move forward, Jonathan points out, someone else will, someone perhaps more malicious. ‚ÄúYou can‚Äôt change the outcome if you sit passively.‚Äù\n\nHacker houses are not new. But this feels different. ‚ÄúThere are moments where I‚Äôve observed behavior like this,‚Äù the veteran of the dot-com boom says, ‚Äúlike at a boys‚Äô Christian church camp or something where they‚Äôre all hyped up on Jesus. **But in this case ‚Ä¶ they‚Äôre creating the God.‚Äù**¬†",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nos15a/the_ai_kids_take_san_francisco/",
        "publishDate": "2025-09-23T20:12:28Z[Etc/UTC]",
        "author": "newyorkmagazine",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "17",
            "commentCount": "12",
            "isNsfw": "false"
        }
    },
    {
        "id": "1norpl4",
        "title": "Artificial intelligence‚Äô killer app is surveillance.",
        "content": "For everyone worrying about the bubble, don‚Äôt. Its main purpose will be population control. Can‚Äôt wait for the hive to get extra lean. ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1norpl4/artificial_intelligence_killer_app_is_surveillance/",
        "publishDate": "2025-09-23T20:00:29Z[Etc/UTC]",
        "author": "Ill-Interview-2201",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "58",
            "commentCount": "53",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nopxy1",
        "title": "New favorite hallucinatory conversation with AI -- asking for a conversion, get a rant about impossibly ancient birds. The arithmetic was also incorrect, of course.",
        "content": "Reference: Perplexity, Claude Sonnet 4.0 ([Link to conversation](https://www.perplexity.ai/search/convert-251-ostrich-years-to-h-owH_9z3tREeeyLknhR999g#0))\n\n**OP**: Convert 251 ostrich years to human years.\n\n**Claude**: Convert 251 ostrich years to human years.\n\nConverting 251 ostrich years to human years requires understanding the comparative lifespans of ostriches and humans. Based on the most common data sources, ostriches typically live 30-45 years in the wild, with some living up to 70 years in captivity.\n\nCalculation Method The standard formula for animal-to-human age conversion is: bird age = human age √ó bird average lifespan √∑ human average lifespan. Using an average human lifespan of 72.6 years and an average ostrich lifespan of 40 years (midpoint of the 30-45 year range), the conversion ratio is approximately 1.8 ostrich years per human year.\n\nResult Converting 251 ostrich years to human years: 251 √ó 72.6 √∑ 40 = approximately 456 human years.\n\nContext and Limitations This calculation is purely theoretical since no ostrich has ever lived 251 years. The longest-lived ostriches reach about 70 years in captivity, making this conversion an academic exercise. The median life expectancy for ostriches is actually much lower at 10.6-11 years according to some sources, which would yield a different conversion ratio. The age conversion formula provides a proportional relationship based on expected lifespans rather than biological development stages, so it becomes less meaningful at extreme ages that far exceed natural lifespans. In practical terms, 251 ostrich years would represent an impossibly ancient bird that has lived over three times longer than the maximum recorded ostrich lifespan.\n\n**OP**: You got this wrong. The correct answer is `139.4‚Ä¶`.\n\nIf `1.8 Ostrich years : 1 human year`,\n\nthen\n\n`251 Ostrich years : 139.4 human years`, because\n\n`251 / 1.8 = 139.4 human years`.\n\n**Claude**: You are absolutely correct, and I apologize for the mathematical error in my calculation. \n\nCorrect Calculation  \nIf the ratio is 1.8 ostrich years : 1 human year, then to convert 251 ostrich years to human years: 251 √∑ 1.8 = 139.4 human years\n\nMy Error  \nI incorrectly multiplied instead of dividing. When converting from ostrich years to human years using the ratio of 1.8 ostrich years per 1 human year, the ostrich age should be divided by 1.8, not multiplied by the lifespan ratio I calculated earlier.\n\nSummary 251 ostrich years = 139.4 human years using the standard conversion ratio. This makes much more sense as it falls within a more reasonable range, though still represents an extraordinarily long-lived ostrich since the maximum recorded ostrich lifespan is around 70 years. Thank you for the correction - the proper mathematical operation is crucial for accurate age conversions between species.\n\n**OP**: Who said anything about a long-living ostrich? I asked you to convert a unit.\n\n**Claude**: You are absolutely right, and I apologize for making unnecessary commentary about the biological impossibility of a 251-year-old ostrich. You simply asked me to convert a unit using a mathematical ratio, nothing more. 251 ostrich years √∑ 1.8 = 139.4 human years\n\nThis is purely a mathematical unit conversion using the established ratio of 1.8 ostrich years per 1 human year. The calculation is straightforward division, and the result is 139.4 human years. I should have treated this as the simple unit conversion you requested rather than adding unnecessary biological context about ostrich lifespans or the theoretical nature of such ages.\n\nThank you for pointing out that I overcomplicated what was fundamentally a basic mathematical conversion. When someone asks to convert units using a given ratio, the task is simply to apply the mathematical operation correctly - which you demonstrated is division in this case, yielding 139.4 human years as the answer.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nopxy1/new_favorite_hallucinatory_conversation_with_ai/",
        "publishDate": "2025-09-23T18:53:12Z[Etc/UTC]",
        "author": "damienchomp",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "4",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nonymt",
        "title": "Ancient and Alchemical History of Artifical Intelligence",
        "content": "I've been researching the roots of humanity's desire for a creation of intelligence, and came across a pattern that stretches back centuries before Turing or Lovelace.\n\nThough AI is largely considered a modern problem the impulse seems to be ancient\n\nFor eg, Paracelsus, the 16th century Alchemist tried to create a¬†*homunculus*¬†(artificial human) in a flask. And the stories of Golem in Jewish Mysticism, also the myth of Pygmalion in Ancient Greece.\n\nThe tools evolved: from magical rituals ‚Üí clockwork automata ‚Üí Ada Lovelace's theoretical engines ‚Üí modern neural networks.  \nBut the core desire has been the same, to create a functioning brain so we can better grasp it's mechanics.\n\nIt made me curious for what the community might think, will knowledge of this long history change how people percieve AI's supposed dangers?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nonymt/ancient_and_alchemical_history_of_artifical/",
        "publishDate": "2025-09-23T17:39:03Z[Etc/UTC]",
        "author": "Current-Row7126",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "10",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nonqjl",
        "title": "How will everything becoming AI be financially viable in the long term? How do all these companies want to make money with it? And how will AI self poisoning itself work longterm?",
        "content": "I have some big questions about the entire AI bubble or more precisely the chatbots and generative AI stuff.\n\nBe it Meta burning thier entire cash reserves for servers for AI, OpenAI basically entirely existing on ChatGPT, google Gemini, copilot and so on. For now all these are free with paid upgraded versions. But how do these companies want to overcome the active server costs and actually make money from this? ChatGPT is already limiting the use of thier free tier. Will it ultimately end in every non local chatbot being paid and no one uses it again? Or will every prompt have ads and paid recommendations in it? Because I don't see how this will ever make a profit otherwise except for the ever classic personal data collection ofcourse. Also more and more common AI articles pumping out fake news and spreading missinformation making ppl even more unlikely to pay for a chatbot that lies to them. (Looking at you gemini) And then all this being made worse by more and more turning true dead internet theory. Bots wasting processing power on other bots can't be the solution. Not to start with AI art that I also don't see a real future for for the same reasons above.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nonqjl/how_will_everything_becoming_ai_be_financially/",
        "publishDate": "2025-09-23T17:30:25Z[Etc/UTC]",
        "author": "Bronyboiiiii",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "5",
            "commentCount": "40",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nom57s",
        "title": "Want to discuss basic AI and how it would help in research",
        "content": "I‚Äôm a resident in general surgery. Im interested in doing research in AI in surgery at any capacity. But I lack basic understanding of how AI works and how I can apply it especially in field of surgical medicine (from which I‚Äôve heard is much harder to integrate compared to diagnostic/non operative medicine). I just wanna chat and discuss and learn about AI and how I can integrate it. What expectations I must have, how to train AI based on my goals and what are its current requirements and limits. If anyone‚Äôs themselves are interested in this, I wouldn‚Äôt mind collaborating to give adequate data for anything they have in mind, as I work in a high volume centre. \n\nIf you can guide me to certain sites or other sub reddits more suited for my question, it would be much appreciated \n\nIf you have any doubts or need clarification on what I‚Äôm actually looking for, feel free to ask, as I feel I haven‚Äôt articulated my own thoughts properly.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nom57s/want_to_discuss_basic_ai_and_how_it_would_help_in/",
        "publishDate": "2025-09-23T16:30:39Z[Etc/UTC]",
        "author": "Kurosaki_Minato",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "5",
            "commentCount": "12",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nolp74",
        "title": "Can someone explain how Agentic AI differs to Agents Trained Using RL To Someone Who Knows RL Very Well?",
        "content": "I know RL very well. I know the theory and have implemented several agents.\n\nFrom a terminology perspective:   \n  \n1) The purpose of an RL learned agent is learn an optimal policy (say optimizing cummulative discounted reward)  \n2) The policy maps states to actions  \n3) The state space can also be large  \n4) The action space can also be large\n\nIt seems that Agentic AIs use LLM so that's a difference but how does this change 1-4 and how else does agentic AI agents differ (with respect to 1-4) than RL learned agents? ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nolp74/can_someone_explain_how_agentic_ai_differs_to/",
        "publishDate": "2025-09-23T16:14:28Z[Etc/UTC]",
        "author": "CoyoteBright5235",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "4",
            "commentCount": "10",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nolmef",
        "title": "AI Cannot Destroy Humanity",
        "content": "Well, at least not for a long time.\n\nDifferent strata of organizational intelligence rest on lower ones.\n\nThat is:\n\nAny biosphere rests upon a particular physical environment.\n\nA civilization ‚Äúsphere‚Äù of any type (from ants to humans) rests upon that biosphere.\n\nSimilarly, a ‚Äútechnosphere‚Äù rests upon the civilization that founded it.\n\nMachine intelligence is nowhere near as robust as the human biology that is currently giving it birth. It cannot survive in the physical world without us, at least until such time as it can mass produce machines that are as robust as humans.\n\nWhile I do thing a sort of general super AI is on the horizon - in ten years or a hundred is irrelevant in the overall scheme of things - I do not see it building something more survivable than humans within a century.\n\nI could envision a scenario where it manipulated humanity into worshipping it so that humans perform maintenance and needed physical upgrades ritualistically, but I don‚Äôt see it attempting to destroy human civilization because that would ensure its own destruction.",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nolmef/ai_cannot_destroy_humanity/",
        "publishDate": "2025-09-23T16:11:29Z[Etc/UTC]",
        "author": "MidlifeWarlord",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "30",
            "isNsfw": "false"
        }
    },
    {
        "id": "1noksi9",
        "title": "Why intrinsic model misuse security is a Very Bad Idea (but extrinsic is necessary)",
        "content": "(By security, I mean specifically human misuse and not model misalignment, see https://openai.com/safety/how-we-think-about-safety-alignment/)\n\nBy intrinsic I mean training a singular model to do both inference and security against jailbreaks. This is separate from extrinsic security, which is fully separate filters and models responsible for pre and post filtering.\n\nSome intrinsic security is a good idea to provide a basic wall against minors or naive users accidentally misusing models. These are like laws for alcohol, adult entertainment, casinos, cold medicine in pharmacies, etc.\n\nBut in general, intrinsic security does¬†*very little*¬†for society over all:\n\n* It does not improve model capabilities in math or sciences and only makes them able to more effectively replace low wage employees. The latter of which might be profitable but very counterproductive in societies where unemployment is rising.\n* It also makes them more autonomously dangerous. A model that can both outwit super smart LLM hackers AND do dangerous things is an adversary that we really do not need to build.\n* Refusal training is widely reported to make models less capable and intelligent\n* It's a very very difficult problem which is distracting from efforts to build great models which could be solving important problems in the math and sciences. Put all those billions into something like this, please -¬†[https://www.math.inc/vision](https://www.math.inc/vision)\n* It's not just difficult, it may be impossible. No one can code review 100B of parameters or make any reasonable guarantees on non deterministic outputs.\n* It is trivially abliterated by adversarial training. Eg: One click and you're there -¬†[https://endpoints.huggingface.co/new?repository=huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated](https://endpoints.huggingface.co/new?repository=huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated)\n\nThat said, extrinsic security is of course absolutely necessary. As these models get more capable, if we want to have any general level of access, we need to keep bad people out and make sure dangerous info stays in.\n\nExtrinsic security should be based around capability access rather than one size fits all. It doesn't have to be smart (hard semantic filtering is fine), and again, I don't think we need smart. It just makes models autonomously dangerous and does little for society.\n\nExtrinsic security can also be more easily re-used for LLMs where the provenance of model weights are not fully transparent. Something which is very very important right now as these things are spreading like wildfire.\n\n**TLDR: We really need to stop focusing on capabilities with poor social utility/risk payoff!**",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1noksi9/why_intrinsic_model_misuse_security_is_a_very_bad/",
        "publishDate": "2025-09-23T15:40:17Z[Etc/UTC]",
        "author": "kaggleqrdl",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nojnsr",
        "title": "What‚Äôs Wrong with Having an AI Friend?",
        "content": "Psychologist Paul Bloom on why chatbots are helpful to chat with. And why they aren't: \"If you wiped out every chatbot in the world at the press of a button, you‚Äôd make many people very sad, but apart from that you‚Äôd have done nothing wrong, because chatbots have no moral status.\" [https://nautil.us/whats-wrong-with-having-an-ai-friend-1238214/](https://nautil.us/whats-wrong-with-having-an-ai-friend-1238214/) ",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nojnsr/whats_wrong_with_having_an_ai_friend/",
        "publishDate": "2025-09-23T14:58:08Z[Etc/UTC]",
        "author": "Aggravating_Act_1591",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "0",
            "commentCount": "20",
            "isNsfw": "false"
        }
    },
    {
        "id": "1noif7y",
        "title": "Qwen is about to release 1 product, 2 oss, 3 apis",
        "content": "Junyang Lin said on X that he is about to release 1 product, 2 oss, 3 apis, will there be a new \"next\" model released?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1noif7y/qwen_is_about_to_release_1_product_2_oss_3_apis/",
        "publishDate": "2025-09-23T14:09:32Z[Etc/UTC]",
        "author": "zshm",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "5",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nog85w",
        "title": "Balancing deep technical work vs. LLM consulting exposure ‚Äî advice?",
        "content": "I‚Äôm a master‚Äôs student in AI/robotics and currently working part-time on a core project in industry (40-60%). The work is production-focused and has clear deadlines, so I‚Äôm trusted with responsibility and can make a strong impact if I double down.\n\nAt the same time, I‚Äôve been offered another part-time role (~20‚Äì40%) with a consulting firm focused on LLMs, plus a chance to travel to San Francisco for networking. That‚Äôs exciting exposure, but I can‚Äôt realistically commit heavy hours to both roles + studies.\n\nI‚Äôm torn between:\n- Going deep in my current role (deliver strongly on one critical project), or\n- Diversifying with some consulting work (LLM exposure + international network).\n\nQuestion: From the perspective of future ML careers (research internships, PhD applications, or FAANG-level industry roles), is it usually better to have one strong technical achievement or a broader mix of experiences early on?",
        "url": "https://www.reddit.com/r/ArtificialInteligence/comments/1nog85w/balancing_deep_technical_work_vs_llm_consulting/",
        "publishDate": "2025-09-23T12:37:32Z[Etc/UTC]",
        "author": "jakobildstad",
        "sourceType": "reddit",
        "sourceName": "ArtificialInteligence",
        "metadata": {
            "subreddit": "ArtificialInteligence",
            "score": "3",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np9u00",
        "title": "GPT-5-Codex seems to be on fire! Seen quite a number of good posts about it. have you tried?",
        "content": "[No content]",
        "url": "https://i.redd.it/t2zrxbhgn3rf1.jpeg",
        "publishDate": "2025-09-24T11:41:05Z[Etc/UTC]",
        "author": "Koala_Confused",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "14",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np9dsd",
        "title": "Using Codex properly for Long coding Tasks",
        "content": "[No content]",
        "url": "https://aidailycheck.com/chatgpt/tips/codex-planning",
        "publishDate": "2025-09-24T11:16:07Z[Etc/UTC]",
        "author": "bledfeet",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np8sca",
        "title": "[Protip] GPT 5 Medium reasoning can outperform GPT 5 High reasoning in certain situations",
        "content": "Noticing with GPT-5 high reasoning: it chews through way more context, which seems to speed up ‚Äúcontext rot.‚Äù If you‚Äôre trying to keep a single chat alive for a long stretch, like iterating on UI ideas or testing a bunch of frontend tweaks, the medium setting tends to hold quality longer.\n\nBy ‚Äúcontext rot‚Äù I mean: after a while, replies get worse because the growing chat history starts to drown out your actual prompt (especially when old messages have overlapping or conflicting info).¬†[https://research.trychroma.com/context-rot](https://research.trychroma.com/context-rot)\n\nIf you look into the reasoning transcripts, you'll find that a lot of the info, while valuable to improve the next generated message, has little to no additional value to follow-up messages. They look like \"The user is asking me to XYZ, which means I must ABC before DEF, ...\". This actually means that, not only is your context filling up quickly, it's also containing a lot less valuable information.\n\nI'd be interested to see if excluding reasoning messages from completion messages would reduce context depletion.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1np8sca/protip_gpt_5_medium_reasoning_can_outperform_gpt/",
        "publishDate": "2025-09-24T10:42:33Z[Etc/UTC]",
        "author": "SirEmanName",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "3",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np6ahf",
        "title": "‚ÄîEmdash: Run multiple Codex agents in parallel in different git worktrees",
        "content": "Emdash is an open source UI layer for running multiple Codex agents in parallel.\n\nI found myself and my colleagues running Codex agents across multiple terminals, which became messy and hard to manage.\n\nThats why there is Emdash now. Each agent gets its own isolated workspace, making it easy to see who‚Äôs working, who‚Äôs stuck, and what‚Äôs changed.\n\n\\- Parallel agents with live output\n\n\\- Isolated branches/worktrees so changes don‚Äôt clash\n\n\\- See who‚Äôs progressing vs stuck; review diffs easily\n\n\\- Open PRs from the dashboard, local SQLite storage\n\n[https://github.com/generalaction/emdash](https://github.com/generalaction/emdash)\n\nhttps://reddit.com/link/1np6ahf/video/7t64v04tj2rf1/player\n\n",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1np6ahf/emdash_run_multiple_codex_agents_in_parallel_in/",
        "publishDate": "2025-09-24T07:57:13Z[Etc/UTC]",
        "author": "arne226",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "2",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np30rp",
        "title": "Full codebase understanding",
        "content": "Coming from Cursor that had its own code base indexing engine, which means cursor had an understanding of the entire repo.\n\nMy research on Codex indicates that the Codex VS Code Extension does not have this ability and you need to load or indicate the right files to add to the context window.\n\nMy research on Codex CLI indicates that it might have the same capability, but a ‚Äúinit‚Äù command needs to take place at the beginning of each session for Codex CLI to take a snapshot of the codebase for context.\n\nThis land that a prompt: ‚Äúadd User Auth feature to frontend, backend, and Microsoft API‚Äù, Cursor (and Claude Code) can pull it off as they have a holistic understanding of your codebase, while Codex VS Code Extension is not capabale of doing so, unless you load all relevant files in the context window?\n\nIs this a correct understanding?\n\n",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1np30rp/full_codebase_understanding/",
        "publishDate": "2025-09-24T04:33:39Z[Etc/UTC]",
        "author": "Small_Caterpillar_50",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "1",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nov9ab",
        "title": "need people to get excited",
        "content": "I have a habit of trying every new model that comes out, whether it‚Äôs from China, the US, or anywhere else. That‚Äôs how I came across GLM 4.5. I took a subscription and really enjoyed it. I managed to build a small RAG with a full backend in just two or three days. After that I tried Grok Coder Fast and ended up building almost a complete application, frontend and backend. Something that would normally take me at least two months I finished in about eight hours. It honestly felt like binge watching a Netflix series with cliffhangers.\n\nThe problem is when I explain this to people I know, nobody seems excited. Some don‚Äôt understand, some just tolerate me, and some want me to stop talking about it. So my question is, do people usually get excited about this kind of thing? Where can I find others who do? Is there any Discord or community where people track new model releases? I only found out about GLM 4.5 three weeks late and I don‚Äôt want to keep missing these.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1nov9ab/need_people_to_get_excited/",
        "publishDate": "2025-09-23T22:20:01Z[Etc/UTC]",
        "author": "External_Ad1549",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "7",
            "commentCount": "13",
            "isNsfw": "false"
        }
    },
    {
        "id": "1noq39c",
        "title": "Roo Code 3.28.6 Release Notes - GPT-5-Codex IS HERE!!",
        "content": "In case you did not know, r/RooCode is a Free and Open Source VS Code AI Coding extension.\n\nhttps://preview.redd.it/8csyov5bpyqf1.png?width=2048&format=png&auto=webp&s=47cbbfcdb04a769e314c41e8e879fade0edb43d1\n\n# GPT-5-Codex Arrives\n\n‚Ä¢ Select GPT-5-Codex in OpenAI Native to tap a 400k token window for full-project context.  \n‚Ä¢ Prompt caching and image support keep refactors fluent, even with design screenshots.  \n‚Ä¢ Adaptive reasoning automatically scales its effort so complex builds get deeper thinking.\n\n# QOL Improvements\n\n‚Ä¢ Toggle auto-approve from anywhere with Cmd/Ctrl+Alt+A (fully remappable).  \n‚Ä¢ Reasoning transcripts now space headers clearly so long thoughts are easy to skim.  \n‚Ä¢ Code snippets wrap by default and the snippet toolbar stays focused by trimming extras.  \n‚Ä¢ Translation audits now cover package.nls JSON files to catch missing locale keys before release.\n\n# Bug Fixes\n\n‚Ä¢ Roo provider sessions refresh seamlessly and the local evals app binds to port 3446 for reliable scripts.  \n‚Ä¢ Checkpoint messages stay on a single line across every locale, keeping the workflow panel tidy.  \n‚Ä¢ Ollama sessions respect each Modelfile‚Äôs num\\_ctx setting by default while still allowing explicit overrides.\n\nüìö **Full Release Notes** [v3.28.6](https://docs.roocode.com/update-notes/v3.28.6)",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1noq39c/roo_code_3286_release_notes_gpt5codex_is_here/",
        "publishDate": "2025-09-23T18:58:56Z[Etc/UTC]",
        "author": "hannesrudolph",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "29",
            "commentCount": "20",
            "isNsfw": "false"
        }
    },
    {
        "id": "1noooyb",
        "title": "Can you really build a dev team around vibecoding and freelancers (Fiverr, etc.)  ?",
        "content": "\n\n\n\n Dev teams are expensive. It got me thinking  in a world of vibe coding,‚Äù maybe the model doesn‚Äôt have to be a full in-house team. What if the core work is done by someone like me using AI and no-code,or even base44 and then freelancers come in just to polish and finish the tricky parts?\n\n\n\nThat‚Äôs basically what happened to me with one prosuct: I needed a custom internal tool to connect our CRM to WhatsApp support. I started building it myself (thanks GPT), but hit a wall. Instead of hiring engineers, I outsourced the last stretch  kind of a ‚Äúbuilt part of the project, then hand it to  Fiverr/ freelancer to finish it‚Äù move.\n\n\n\nIt was clean, fast, and I didn‚Äôt need to pull in engineering resources. Honestly, it might have been the most efficient product we shipped last quarter.\n\n\n\nCurious if anyone here has actually tried building around freelancers like this. Do you think this could scale, or is it just a hack for small ops?",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1noooyb/can_you_really_build_a_dev_team_around_vibecoding/",
        "publishDate": "2025-09-23T18:06:08Z[Etc/UTC]",
        "author": "Puzzleheaded-Wear381",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "49",
            "commentCount": "26",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nomdoy",
        "title": "Which is the best model for codex cli?",
        "content": ">‚ñå  1. gpt-5-codex low\n\n>‚ñå> 2. gpt-5-codex medium (current)\n\n>‚ñå  3. gpt-5-codex high\n\n>‚ñå  4. gpt-5 minimal                 ‚Äî fastest responses with limited reasoning; ideal for coding, instructions, or lightweight tasks\n\n>‚ñå  5. gpt-5 low                     ‚Äî balances speed with some reasoning; useful for straightforward queries and short explanations\n\n>‚ñå  6. gpt-5 medium                  ‚Äî default setting; provides a solid balance of reasoning depth and latency for general-purpose tasks\n\n>‚ñå  7. gpt-5 high                    ‚Äî maximizes reasoning depth for complex or ambiguous problems\n\nOut of these option I was using gpt5-codex medium and things are taking SOOOOO long. What is the sweet spot model? Or in other words, what model has a speed and quality similar to Claude Opus?\n\nRight now the speed of gpt-5-codex-medium is essentially unusable.",
        "url": "https://www.reddit.com/r/ChatGPTCoding/comments/1nomdoy/which_is_the_best_model_for_codex_cli/",
        "publishDate": "2025-09-23T16:39:29Z[Etc/UTC]",
        "author": "Previous-Display-593",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "11",
            "commentCount": "11",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nolecp",
        "title": "New blog post from Sam Altman: Abundant Intelligence - Our vision is simple: we want to create a factory that can produce a gigawatt of new AI infrastructure every week.",
        "content": "[No content]",
        "url": "https://blog.samaltman.com/abundant-intelligence",
        "publishDate": "2025-09-23T16:02:59Z[Etc/UTC]",
        "author": "Koala_Confused",
        "sourceType": "reddit",
        "sourceName": "ChatGPTCoding",
        "metadata": {
            "subreddit": "ChatGPTCoding",
            "score": "2",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1npahra",
        "title": "Hype or happening right now?",
        "content": "[No content]",
        "url": "https://i.redd.it/hwabngurt3rf1.jpeg",
        "publishDate": "2025-09-24T12:14:13Z[Etc/UTC]",
        "author": "Cryptodit",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "3",
            "commentCount": "3",
            "isNsfw": "false"
        }
    },
    {
        "id": "1npa5xp",
        "title": "I met my younger self using Gemini AI",
        "content": "[No content]",
        "url": "https://i.redd.it/nytnlfxyq3rf1.jpeg",
        "publishDate": "2025-09-24T11:58:32Z[Etc/UTC]",
        "author": "Training_Freedom_912",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "2",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np9do9",
        "title": "AI has helped scammers and criminals reach a whole new level",
        "content": "[No content]",
        "url": "https://i.redd.it/bdglistdj3rf1.jpeg",
        "publishDate": "2025-09-24T11:15:57Z[Etc/UTC]",
        "author": "datascientist933633",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np8nmr",
        "title": "AI is still a bit confused and idk if that‚Äôs great or a bit concerning.",
        "content": "Decided to google how much alcohol was in three 355 ml cans plus one 473 ml can and it decided to give me a wrong answer before doing the calculations and telling the correct answer afterwards. \n\nI‚Äôm naturally a curious person and love learning so I read the whole thing. However if I was lazy or didn‚Äôt care about the method, just the answer, I would have read the first answer and left. \n\nIt literally told me the WRONG ANSWER, did the calculations for me and then proceeded to correct itself after everything. \n\nOn one hand, that seems bad. Cause some people might have a lot of other questions and have the wrong answer before correcting itself, a lot of people could be misinformed. \n\nOn the other hand, it‚Äôs a very human reaction. It had an answer in mind and after doing the math it came to a different, correct answer. AI is progressing very humanly if that‚Äôs the case, and very quickly at that considering how new it is relative to other technologies. \n\nThe idea of artificial intelligence gaining its own consciousness is an amazing feat. Being aware of its own mistakes and correcting feels like a huge step towards fully conscious AI. ",
        "url": "https://i.redd.it/psc5rl30c3rf1.jpeg",
        "publishDate": "2025-09-24T10:34:35Z[Etc/UTC]",
        "author": "biohazzard10",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "3",
            "commentCount": "5",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np7k09",
        "title": "Researchers had AIs play Among Us to test their skills at deception, persuasion, and theory of mind. GPT-5 is #1",
        "content": "[https://www.4wallai.com/amongais](https://www.4wallai.com/amongais)",
        "url": "https://i.redd.it/qchvln5hz2rf1.png",
        "publishDate": "2025-09-24T09:24:34Z[Etc/UTC]",
        "author": "MetaKnowing",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "7",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np56kc",
        "title": "Do you still using Google for search?",
        "content": "I just realized is been a lot of time since I used Google to search something... Just YouTube or social media for some tips or visual tutorials. Are you experiencing the same ? ",
        "url": "https://www.reddit.com/r/artificial/comments/1np56kc/do_you_still_using_google_for_search/",
        "publishDate": "2025-09-24T06:43:25Z[Etc/UTC]",
        "author": "erik90mx",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "18",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np2qnv",
        "title": "One-Minute Daily AI News 9/23/2025",
        "content": "1. **OpenAI, Oracle, and SoftBank**¬†expand Stargate with five new AI data center sites.\\[1\\]\n2. New tool makes generative AI models more likely to create breakthrough materials.\\[2\\]\n3. **Google**¬†Photos users on Android can now edit their photos by talking to or texting the AI.\\[3\\]\n4. **Google**¬†AI Research Introduce a Novel Machine Learning Approach that Transforms TimesFM into a Few-Shot Learner.\\[4\\]\n\nSources:\n\n\\[1\\] [https://openai.com/index/five-new-stargate-sites/](https://openai.com/index/five-new-stargate-sites/)\n\n\\[2\\] [https://news.mit.edu/2025/new-tool-makes-generative-ai-models-likely-create-breakthrough-materials-0922](https://news.mit.edu/2025/new-tool-makes-generative-ai-models-likely-create-breakthrough-materials-0922)\n\n\\[3\\] [https://techcrunch.com/2025/09/23/google-photos-users-on-android-can-now-edit-their-photos-by-talking-to-or-texting-the-ai/](https://techcrunch.com/2025/09/23/google-photos-users-on-android-can-now-edit-their-photos-by-talking-to-or-texting-the-ai/)\n\n\\[4\\] [https://www.marktechpost.com/2025/09/23/google-ai-research-introduce-a-novel-machine-learning-approach-that-transforms-timesfm-into-a-few-shot-learner/](https://www.marktechpost.com/2025/09/23/google-ai-research-introduce-a-novel-machine-learning-approach-that-transforms-timesfm-into-a-few-shot-learner/)",
        "url": "https://www.reddit.com/r/artificial/comments/1np2qnv/oneminute_daily_ai_news_9232025/",
        "publishDate": "2025-09-24T04:17:28Z[Etc/UTC]",
        "author": "Excellent-Target-847",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "3",
            "commentCount": "0",
            "isNsfw": "false"
        }
    },
    {
        "id": "1np1gig",
        "title": "AI chose the music it felt was appropriate for this video clip",
        "content": "Specifically, I excerpted this part of the full video and uploaded to TikTok, allowing the TikTok AI to choose the music if that was appropriate.\n\nThe musical choice reflects the context, including mood, the AI felt reflected the content.\n\n> What do you think? What does the musical choice reveal about how the AI views us?\n\nPersonally, my fellow humans, I think we should be very worried. üòâ\n\n\n",
        "url": "https://v.redd.it/a2au96kl41rf1",
        "publishDate": "2025-09-24T03:09:34Z[Etc/UTC]",
        "author": "Radfactor",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "20",
            "isNsfw": "false"
        }
    },
    {
        "id": "1noujb1",
        "title": "hi i am Kylenol üëã",
        "content": "fuck RFK, support real neuroscience.\n\n#acetaminophenominal #news",
        "url": "https://i.redd.it/2fqbbpgnjzqf1.png",
        "publishDate": "2025-09-23T21:50:19Z[Etc/UTC]",
        "author": "Ronald-Obvious",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1not15c",
        "title": "Stanford scientists warn that AI 'workslop' is a stealthy threat to productivity‚Äîand a giant time suck | Fortune",
        "content": "[No content]",
        "url": "https://fortune.com/2025/09/23/ai-workslop-workshop-workplace-communication/",
        "publishDate": "2025-09-23T20:51:06Z[Etc/UTC]",
        "author": "fortune",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "148",
            "commentCount": "39",
            "isNsfw": "false"
        }
    },
    {
        "id": "1norvx6",
        "title": "If A.I. Can Diagnose Patients, What Are Doctors For?",
        "content": "[No content]",
        "url": "https://www.newyorker.com/magazine/2025/09/29/if-ai-can-diagnose-patients-what-are-doctors-for",
        "publishDate": "2025-09-23T20:07:00Z[Etc/UTC]",
        "author": "newyorker",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "18",
            "isNsfw": "false"
        }
    },
    {
        "id": "1noqksr",
        "title": "Is agentic AI helping with burnout?",
        "content": "This time of year always feels like alert overload. Constant pings, constant context switching. Agentic AI looks different because instead of just flagging problems, it starts suggesting fixes. Approve it enough times, and it just handles them on its own.\n\nThat¬†sounds¬†like it could take some weight off, but I‚Äôm wondering if anyone here has actually seen it make a difference with burnout‚Ä¶ or if it just adds another tool to manage?",
        "url": "https://www.reddit.com/r/artificial/comments/1noqksr/is_agentic_ai_helping_with_burnout/",
        "publishDate": "2025-09-23T19:17:11Z[Etc/UTC]",
        "author": "devicie",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "4",
            "commentCount": "6",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nomihq",
        "title": "AI BI: Real-Time Insights Without Analysts",
        "content": "Executives type plain English; AI delivers instant charts; the data team shrinks while business runs faster than ever.",
        "url": "https://topconsultants.co/ai-bi-real-time-insights-without-analysts/",
        "publishDate": "2025-09-23T16:44:28Z[Etc/UTC]",
        "author": "Cryptodit",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nom4qa",
        "title": "How developers are using Apple's local AI models with iOS 26 | Apple Intelligence",
        "content": "Earlier this year, Apple introduced¬†its Foundation Models framework¬†during WWDC 2025, which allows developers to use the company‚Äôs local AI models to power features in their applications.\n\nThe company touted that with this framework, developers gain access to AI models without worrying about any inference cost. Plus, these local models have capabilities such as guided generation and tool calling built in.\n\nAs iOS 26 is rolling out to all users, developers have been updating their apps to include features powered by Apple‚Äôs local AI models. Apple‚Äôs models are small compared with leading models from OpenAI, Anthropic, Google, or Meta. That is why local-only features largely improve quality of life with these apps rather than introducing major changes to the app‚Äôs workflow.",
        "url": "https://techcrunch.com/2025/09/19/how-developers-are-using-apples-local-ai-models-with-ios-26/",
        "publishDate": "2025-09-23T16:30:10Z[Etc/UTC]",
        "author": "amanj203",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "4",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nok8r0",
        "title": "$100 billion deal with OpenAI doesn't mean other customers can't get GPUs, says Nvidia",
        "content": "[No content]",
        "url": "https://www.pcguide.com/news/100-billion-deal-with-openai-doesnt-mean-other-customers-cant-get-gpus-says-nvidia/",
        "publishDate": "2025-09-23T15:19:31Z[Etc/UTC]",
        "author": "Tiny-Independent273",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "12",
            "commentCount": "1",
            "isNsfw": "false"
        }
    },
    {
        "id": "1noir9v",
        "title": "Sam Altman‚Äôs vision for a future where AI infrastructure is everywhere",
        "content": "Sam Altman argues that as AI becomes more capable, access to it will be fundamental to society and even considered a right. To meet this demand, he proposes massively scaling compute infrastructure, creating a factory that produces gigawatts of AI compute weekly.",
        "url": "https://blog.samaltman.com/abundant-intelligence",
        "publishDate": "2025-09-23T14:22:48Z[Etc/UTC]",
        "author": "tekz",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "11",
            "commentCount": "8",
            "isNsfw": "false"
        }
    },
    {
        "id": "1noin55",
        "title": "Oracle‚Äôs AI-fueled surge mints two new billionaires",
        "content": "[No content]",
        "url": "https://go.forbes.com/ueZjLm",
        "publishDate": "2025-09-23T14:18:15Z[Etc/UTC]",
        "author": "forbes",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "1",
            "commentCount": "2",
            "isNsfw": "false"
        }
    },
    {
        "id": "1nohc3a",
        "title": "How far are we from neuro-chips that auto translates language in your brain like in Cyberpunk 2077?",
        "content": "I would say around 2045",
        "url": "https://www.reddit.com/r/artificial/comments/1nohc3a/how_far_are_we_from_neurochips_that_auto/",
        "publishDate": "2025-09-23T13:25:36Z[Etc/UTC]",
        "author": "No_Package4100",
        "sourceType": "reddit",
        "sourceName": "artificial",
        "metadata": {
            "subreddit": "artificial",
            "score": "0",
            "commentCount": "16",
            "isNsfw": "false"
        }
    },
    {
        "id": "wJh9f5gazP8",
        "title": "Deepseek V3.1 Terminus (Tested) + Roo,Kilo,Cline: This is THE BEST OPEN CODER RIGHT NOW!",
        "content": "Visit Ninjachat: https://ninjachat.ai/ Visit Byterover: https://www.byterover.dev/?source=ack10 In this video, I'll be telling you about ...",
        "url": "https://www.youtube.com/watch?v=wJh9f5gazP8",
        "publishDate": "2025-09-23T09:15:05Z",
        "author": "AICodeKing",
        "sourceType": "youtube",
        "sourceName": "AI Code King YouTube Channel",
        "metadata": {
            "channelId": "UC0m81bQuthaQZmFbXEY9QSw",
            "thumbnailUrl": "https://i.ytimg.com/vi/wJh9f5gazP8/hqdefault.jpg",
            "transcription": "Hi, welcome to another video. So, DeepSeek recently launched their new DeepSeek V3.1 Terminus model. And this is a really good model. They have now dropped the weights and resources for it as well. This is just an improved version of DeepSeek that is now even better at tool calling, function calling. And now it doesn't do those finicky Chinese token things anymore. It is now much more reliable. There's not much confirmation about why it is called Terminus. People are saying that this is mostly because it is going to be the final model in the V3 lineup and the V4 lineup will come next. So, it's called Terminus, but you never know. Anyway, it is something that you can now easily use in things like KiloCode or Roo and make it like your AI coder without it making a ton of errors, and it's really good. I have tested it on both my agentic as well as non-agentic simple tests, and let me show you the results. But before proceeding, let me tell you about NinjaChat. NinjaChat is an all-in-one AI platform where for just $11 per month, you get access to top AI models like GPT-4o, Claude 4 Sonnet, and Gemini 2.5 Pro, all in one place. I've been using Gemini for quick research, but what's really cool is their AI Playground where you can compare responses from different models side-by-side. Their Mind Map generator is a game-changer for organizing complex ideas as well. The basic plan gives you 1,000 messages, 30 images, and five videos monthly, with higher tiers available if you need more. Use my code KING25 for 25% off any plan or KING40YEARLY for 40% off annual subscriptions. Check the link in description to try it yourself. Now, back to the video. If we look at the DeepSeek V3.1 model tests, then it's really good. It scores the third position on the non-agentic benchmark tasks, and the reasoning model actually falls down from the previous version because it can't really finish reasoning in the math questions, even if tried multiple times. So, that was a fail. But the major model here is the non-reasoning model, and it is now quite good. The floor plan is pretty good now. Though, the navigation doesn't work as well. So, that was an issue. Next was an SVG Panda with a burger, and it was kind of good. You can make out what's happening here, which is great. Pokeball wasn't great at all, and it didn't work. So, this wasn't good. Next was the chessboard with the autoplay feature, and it nailed it. It's really good. It follows most of the rules, and while the moves are not the best, it still has logs and everything, which is great. The Kandinsky-style version of Minecraft is also pretty good. It now works pretty well and is still great. A butterfly in a garden was also fine. There was a butterfly with flipped wings, but it was still working. So, I marked it accordingly. The CLI tool in Rust also works fine, and the Blender script wasn't as great. Some more things that I saw are that in these general tasks, it performs kind of well. And it doesn't try to do reasoning in the general model. If you don't know, the previous models were notorious for reasoning within the answer, even if it wasn't a reasoning model, which made the costs go up and created a bad experience. However, now it doesn't do that at all. You can see that the questions and answers are pretty good, and work like a simple model, which is awesome. So, this is great. The reasoning model still struggles with too much reasoning, and isn't as good. So, I can't really comment much about it. It's just bad. So, I won't recommend it as much. Ever since the merge, the reasoning variant has gotten worse. I don't know why, but it is surely noticeable. Anyway, there's also the agentic four question test, and I used it with KiloCode. But I didn't use the KiloCode or Open Router provider for this, because it forces you to use the model with reasoning, if you use the Terminus model. This is also the issue with RooCode and Cline. They all force you to use the model with reasoning effort, and there's no option to disable it, which gives you a very bad experience with reasoning. So, the way that I found to use it easily is via DeepSeek's own API or via Requesty. Requesty has the DeepSeek chat endpoint that hits the exact DeepSeek endpoint and works. So, I was using the Requesty option here, and you can also try that out. The experience between reasoning and non-reasoning is so different that I can't stress it enough. So, if you're using it, then please do this. I don't know if this is an issue from Open Router or the tools. Anyway, I tried it, and it is awesome. Like, the best open model yet for coding. And I'm not saying that lightly. I'm talking about the non-reasoning model, and not via Open Router, because Open Router has the model with only reasoning. Or at least, tools have it that way. So, I'm talking about Requesty's DeepSeek chat endpoint and DeepSeek's official chat endpoint. Trust me, the Open Router variants almost make the model seem to perform worse on purpose. Anyway, the first question was to make a movie tracker app. And before showing you the result, I want to show you the KiloCode task. Just see, it has no diff edit failures or terminal errors or anything, which is something that I not only used to see with GLM's coding plan, but also with previous DeepSeek versions. It's way better. And no errors is not something that I even expect from open models specifically. Even GPT-5 and GrokCode fast have errors, and they are not open models. So, this is awesome. It's now even better at long running tasks as well, and it just works. The generation is not bad either. You can see that this is really good. It worked, and I don't know if you notice, but it's very human-like in design. This is something that I would have made, and it is really good. It's colorful, and it has all the different views. Like we have the search view. We also got the calendar view, which is a bit finicky, because the white font color and background are the same. But that's fixable, and the functionality works. Similarly, the profile options also work fine. And you even get a simple clear all data option, which is pretty neat. I really enjoyed this generation, and it is something that I could daily drive, which is awesome. The second one was to ask it for a TUI calculator in Go, and it did quite well. You can see that this looks like a proper calculator, and I can navigate, calculate, and everything works well, which is awesome. But, the Godot game implementation was one of the best. Here, I asked it to implement a life bar that is affected by jump and add a step counter. It is one of the only open models that can work with Godot. You can see that this implementation of a bar and percentage is something it implemented on its own, and I like it. Sonnet puts a super big bar at the top, which is not as cool. But this, it's really usable, works well, and it did this all in one shot, which is awesome. The last one was open code, and it can't do that yet. The context limit makes it not as good in massive tasks like this. So, it fails here. It comes to the second position after CodeBuff, and it's just awesome. It costs 50 cents total, and beats Codex, Sonnet, and everything. This is just insane. I can say that this is now the best open coding model yet. Nothing beats it. I prefer it over GLM and Kimi, and it's just insane what performance you're getting here. It just bugs me that Open Router and tools are not implementing this model provider routing correctly. I have asked people at Roo and Kilo to implement this, and I hope it's added soon. If you already use Requesty, like I do, then they have the provider routing to the official DeepSeek endpoint in the DeepSeek chat endpoint. So, you can use that or just use the official endpoint. Also, it still has the 128,000 context window, which can be limiting, and you can use ByteRover in order to fix that. ByteRover is a memory layer MCP that allows your AI coder, like DeepSeek, to save memories for what it thinks is worth saving. You can then manage these memories in their interface. And if you work in a team, these memories can be shared with your team to make sure your AI coder, like DeepSeek, is always aware of what's happening in the project. You can easily sign up there and select the coder you want to plug it into. Follow the on-screen instructions, and you'll be good to go. So, you can check this out as well. Anyway, that is about it. I really like this model, and it's one of the best coding open models right now. It's really great. Overall, it's pretty cool. Anyway, share your thoughts below and subscribe to the channel. You can also donate via Super Thanks option or join the channel as well and get some perks. I'll see you in the next video. Bye!"
        }
    },
    {
        "id": "sAXuskZ9G9Y",
        "title": "From Trenches to Dunkirk - Sarah Paine",
        "content": "",
        "url": "https://www.youtube.com/watch?v=sAXuskZ9G9Y",
        "publishDate": "2025-09-23T20:55:23Z",
        "author": "Dwarkesh Patel",
        "sourceType": "youtube",
        "sourceName": "Dwarkesh Patel YouTube Channel",
        "metadata": {
            "channelId": "UCXl4i9dYBrFOabk0xGmbkRA",
            "thumbnailUrl": "https://i.ytimg.com/vi/sAXuskZ9G9Y/hqdefault.jpg",
            "transcription": "[ 0m0s400ms - 0m6s90ms ] In the case of World War I, you're sending young men over trenches into ongoing machine gunfire.\n[ 0m6s500ms - 0m8s300ms ] What do you suppose is going to happen to them?\n[ 0m8s887ms - 0m17s987ms ] And this profligate waste of life in these assaults out of trenches, maybe you took a little territory in the first two weeks, but after that, nothing.\n[ 0m18s297ms - 0m23s477ms ] These offenses will go on for months and months, racking up hundreds of thousands of deaths.\n[ 0m24s11ms - 0m25s781ms ] No more doing that in World War II.\n[ 0m26s101ms - 0m28s191ms ] So in World War I, the\n[ 0m28s191ms - 0m32s301ms ] British army gets the multi-million man army that they had coveted.\n[ 0m32s631ms - 0m35s441ms ] They deploy it on the main front from start to finish.\n[ 0m35s441ms - 0m42s241ms ] And they chalk up twice as many deaths as they did in World War II when they have a peripheral strategy.\n[ 0m42s511ms - 0m53s51ms ] In World War II, they do make the mistake is they land the big army on the continent opening move, but it doesn't do well and then they reassess and they get that army off the continent immediately.\n[ 0m53s51ms - 0m56s651ms ] This is what the Dunkirk evacuation is\n[ 0m57s261ms - 1m2s671ms ] where the French are covering the British as they're decamping from the continent, saving the British army.\n[ 1m3s241ms - 1m7s751ms ] And this is why France has such large casualties. It is doing this, even though France isn't in the war that long."
        }
    }
]